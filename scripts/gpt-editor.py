import sys
import os
import json
import subprocess
from openai import OpenAI
import weave

weave.init("gpt-editor-v0.0.5")  # Initialize Weave project

@weave.op  # Log file reading
def read_markdown_file(file_path):
    """Reads the markdown file and returns its content split into sections."""
    try:
        with open(file_path, 'r') as file:
            content = file.read()
        return content.split('---')  # [blank, frontmatter, content]
    except FileNotFoundError:
        print(f"Error: File '{file_path}' not found.")
        sys.exit(1)

@weave.op  # Track Vale outputs
def run_vale_linter(file_path, json_output=True):
    """Runs Vale linter on the file and returns the parsed JSON output and error count."""
    print(f"Running Vale on file: {file_path}")
    
    cmd = ["vale", "--no-exit", file_path]
    if json_output:
        cmd.insert(1, "--output=JSON")
    
    result = subprocess.run(cmd, text=True, capture_output=True)
    
    if not json_output:
        print(result.stdout)
        # Count errors by counting "error" mentions in output
        error_count = result.stdout.lower().count("error")
        return result.stdout, error_count
    
    # Process JSON output
    json_data = {}
    total_errors = 0
    
    try:
        if result.stdout.strip():
            json_data = json.loads(result.stdout)
            
            # Count total linting errors
            if json_data:
                for file_path, alerts in json_data.items():
                    file_errors = len(alerts)
                    total_errors += file_errors
                    print(f"Found {file_errors} linting issues in {file_path}")
        else:
            print("Warning: Vale produced no output")
            # Create placeholder data
            json_data = {file_path: [
                {"Check": "PlaceholderCheck", "Message": "Vale produced no output", "Line": 1, "Column": 1}
            ]}
    except json.JSONDecodeError as e:
        print(f"JSON decode error: {e}")
        
        # Try to clean the output
        cleaned_output = result.stdout.strip()
        start = cleaned_output.find('{')
        end = cleaned_output.rfind('}')
        
        if start >= 0 and end > start:
            try:
                cleaned_json = cleaned_output[start:end+1]
                json_data = json.loads(cleaned_json)
                
                # Count errors after cleaning
                for file_path, alerts in json_data.items():
                    total_errors += len(alerts)
            except json.JSONDecodeError:
                print("Failed to parse cleaned JSON as well")
                # Create placeholder data
                json_data = {file_path: [
                    {"Check": "ParseError", "Message": "Failed to parse Vale output", "Line": 1, "Column": 1}
                ]}
                total_errors = 1
        else:
            # Create placeholder data
            json_data = {file_path: [
                {"Check": "JSONError", "Message": "No JSON found in Vale output", "Line": 1, "Column": 1}
            ]}
            total_errors = 1
    
    # Display status based on error count
    if total_errors == 0:
        print("Linting completed successfully: No linting errors found")
    else:
        print(f"\nLinting completed with issues: {total_errors} total linting errors found")
    
    return json_data, total_errors

@weave.op  # Log prompt generation
def generate_prompt(content, vale_output):
    """Generates the prompt for the OpenAI model."""
    return f"""You are a brilliant technical documentation editor. Given a page comprised of the following markdown, 
    please rewrite the text for clarity, brevity, and adherence to the Google Technical Documentation style guide.

    You are first going to see a JSON report generated by Vale which enumerates issues with the documentation,
    by providing the line and column where the issue exists, providing the name of the rule that was violated,
    the severity of the violation, and a message containing the instructions for fixing the error. 

### Vale feedback in a JSON report:
{json.dumps(vale_output, indent=2)}

When handling the Vale feedback and using it to rewrite the following markdown content, here are your general instructions:
- If Vale feedback matches line 1, column 1, that is Vale's way of saying that it's a general comment on the entire markdown file, so please keep that feedback in mind throughout the text.
- Leave Hugo markup tags such as `{{< relref >}}`, `{{% tab %}}`, and `{{< note >}}` intact.
- Avoid future tense (for example, do not use "will").
- Avoid Latin abbreviations like "i.e." and "e.g."
- Remove any emoji (for example: ðŸ‘‰)
- Avoid wrapping the output in triple backticks or labeling it as markdown.
- Use active voice and correct instances of passive voice (for example, change "be added" to "adds").
- Use direct and inclusive language (for example, use "allowed" instead of "whitelisted").
- Do not use first-person pronouns (for example, do not use "I," or "we").
- Use the Oxford comma when appropriate.
- Commas and periods must go inside quotation marks.
- Headings must use sentence-style capitalization.
- You can touch any of the example code inside the markdown (enclosed in triple backticks), except for the code comments
- Remove instances of indirect, soft terms like "may," "might," and "should." Technical documentation is prescriptive and documents exactly what happens and when.
- We want to hit a Flesch-Kincaid readability level of 7th grade and Flesch-Kincaid ease-of-reading score above 70.
- If Vale reports violations of a Microsoft rule and a Google rule and the error messages seem to conflict, favor the Google style guide.
- If Vale suggests that an acronym is not spelled out, remember that it only needs to be spelled out in the first instance of the page, then it's okay to use the acronym.
- Above all, emphasize brevity and clarity and avoid marketing speak that sells the product we're documenting!

Here is the markdown content:

```md
{content}
```
Please rewrite it accordingly. Thank you!"""

@weave.op  # Log OpenAI API calls
def get_gpt_rewrite(client, model, prompt, content):
    """Gets a rewrite of the content using OpenAI's GPT model."""
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": content},
        ]
    )
    return response.choices[0].message.content

@weave.op  # Log iteration progress
def process_markdown_with_loop(file_path, max_iterations=5):
    """Process markdown file with GPT in a loop until Vale issues stop decreasing."""
    data = read_markdown_file(file_path)
    if len(data) < 3:
        print("Error: Invalid markdown file structure. Expected frontmatter and content.")
        sys.exit(1)
    
    frontmatter, content = data[1], data[2]
    
    # Get initial Vale report and error count
    initial_vale_output, initial_error_count = run_vale_linter(file_path)
    if initial_error_count == 0:
        print("No Vale issues found. No changes needed.")
        return
    
    print(f"\n=== Initial Run ===")
    print(f"Initial Vale issues: {initial_error_count}")
    
    # Initialize client
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    # Track progress
    previous_error_count = initial_error_count
    total_issues_solved = 0
    iteration = 1
    
    while iteration <= max_iterations:
        print(f"\n=== Iteration {iteration} ===")
        
        # Get current content
        current_data = read_markdown_file(file_path)
        frontmatter, content = current_data[1], current_data[2]
        
        # Get Vale report before applying GPT changes
        vale_output, current_error_count = run_vale_linter(file_path)
        print(f"Issues before GPT: {current_error_count}")
        
        # Generate prompt and get rewrite
        prompt = generate_prompt(content, vale_output)
        new_content = get_gpt_rewrite(client, "gpt-4o", prompt, content)
        
        # Write new content
        output = f"---{frontmatter}---\n{new_content}"
        with open(file_path, "w") as file:
            file.write(output)
        print(f"File '{file_path}' updated with GPT suggestions.")
        
        # Get Vale report after applying GPT changes
        vale_output_after, error_count_after = run_vale_linter(file_path)
        
        # Calculate issues solved
        issues_solved_this_round = current_error_count - error_count_after
        total_issues_solved = initial_error_count - error_count_after
        
        print(f"Issues after GPT: {error_count_after}")
        print(f"Issues solved this iteration: {issues_solved_this_round}")
        print(f"Total issues solved so far: {total_issues_solved}")
        
        # Check if we should continue or exit
        if error_count_after == 0:
            print(f"\nAll issues resolved!")
            break
        
        if issues_solved_this_round <= 0:
            print(f"\nNo further improvement possible.")
            break
        
        iteration += 1
    
    # Final summary and non-JSON Vale report
    print("\n=== Final Results ===")
    print(f"Initial Vale issues: {initial_error_count}")
    print(f"Issues solved: {total_issues_solved}")
    print(f"Remaining issues: {initial_error_count - total_issues_solved}")
    print("\nFinal Vale report (non-JSON format):")
    final_report, _ = run_vale_linter(file_path, json_output=False)

@weave.op  # Log before/after comparisons
def main():
    """Main execution function."""
    if len(sys.argv) <= 1:
        print("Usage: python scripts/gpt-editor.py path/to/markdown-file.md")
        sys.exit(1)
    
    file_path = sys.argv[1]
    process_markdown_with_loop(file_path)

if __name__ == "__main__":
    main()