---
title: "How can I use wandb with multiprocessing, e.g. distributed training?"
sidebarTitle: "How can I use wandb with multiprocessing, e.g. distributed training?"
---
{/*
Built with DocEngine. Do not edit this MDX file directly.
Template: /sites/wandb-docs/templates/support_article.mdx.j2
*/}

If a training program uses multiple processes, structure the program to avoid making wandb method calls from processes without `wandb.init()`. 

Manage multiprocess training using these approaches:

1. Call `wandb.init` in all processes and use the [group](/models/runs/grouping) keyword argument to create a shared group. Each process will have its own wandb run, and the UI will group the training processes together.
2. Call `wandb.init` from only one process and pass data to log through [multiprocessing queues](https://docs.python.org/3/library/multiprocessing.html#exchanging-objects-between-processes).

<Note>
Refer to the [Distributed Training Guide](/models/track/log/distributed-training) for detailed explanations of these approaches, including code examples with Torch DDP.
</Note>

---

<Badge stroke shape="pill" color="orange" size="md">[Experiments](/support/models/tags/experiments)</Badge>