---
title: Weave ã‚’ä½¿ã£ãŸ Audio
description: W&B Weave ã‚’ä½¿ç”¨ã—ã¦ Weave ã§ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚’æ‰±ã†æ–¹æ³•ã‚’å­¦ã¶
---

<Note>
ã“ã‚Œã¯ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã™ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã™ã‚‹ã‹ã€ä»¥ä¸‹ã®ãƒªãƒ³ã‚¯ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š
- [Google Colab ã§é–‹ã](https://colab.research.google.com/github/wandb/docs/blob/main/weave/cookbooks/source/audio_with_weave.ipynb)
- [GitHub ã§ã‚½ãƒ¼ã‚¹ã‚’è¡¨ç¤º](https://github.com/wandb/docs/blob/main/weave/cookbooks/source/audio_with_weave.ipynb)
</Note>

## 

# Audio ãƒ‡ãƒ¼ã‚¿ã§ Weave ã‚’ä½¿ç”¨ã™ã‚‹æ–¹æ³•ï¼šOpenAI ã®ä¾‹

ã“ã®ãƒ‡ãƒ¢ã§ã¯ã€OpenAI ã® chat completions API ã¨ GPT 4o Audio Preview ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹éŸ³å£°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆã—ã€ãã‚Œã‚‰ã‚’ Weave ã§è¿½è·¡ã—ã¾ã™ã€‚

<img src="https://i.imgur.com/OUfsZ2x.png"></img>

é«˜åº¦ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã¨ã—ã¦ã€OpenAI Realtime API ã‚’æ´»ç”¨ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§éŸ³å£°ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã—ã¾ã™ã€‚å‹•ç”»ãƒ‡ãƒ¢ã‚’è¦‹ã‚‹ã«ã¯ã€æ¬¡ã®ã‚µãƒ ãƒã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã‹ã€[ã“ã¡ã‚‰](https://www.youtube.com/watch?v=lnnd73xDElw)ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

[![Everything Is AWESOME](https://img.youtube.com/vi/lnnd73xDElw/0.jpg)](https://www.youtube.com/watch?v=lnnd73xDElw "Everything Is AWESOME")

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

ã¾ãšã€OpenAI (`openai`) ã¨ Weave (`weave`) ã®ä¾å­˜é–¢ä¿‚ã€ãŠã‚ˆã³ API ã‚­ãƒ¼ç®¡ç†ç”¨ã®ä¾å­˜é–¢ä¿‚ `set-env` ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

```python lines
%%capture
!pip install openai
!pip install weave
!pip install set-env-colab-kaggle-dotenv -q # ç’°å¢ƒå¤‰æ•°ç”¨
python
%%capture
# openai ã®ãƒã‚°ã‚’ä¿®æ­£ã™ã‚‹ãŸã‚ã®æš«å®šçš„ãªå›é¿ç­–ï¼š
# TypeError: Client.__init__() got an unexpected keyword argument 'proxies'
# è©³ç´°ã¯ https://community.openai.com/t/error-with-openai-1-56-0-client-init-got-an-unexpected-keyword-argument-proxies/1040332/15 ã‚’å‚ç…§
!pip install "httpx<0.28"
```

æ¬¡ã«ã€OpenAI ã¨ Weave ã«å¿…è¦ãª API ã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚ã“ã“ã§ã¯ã€Google Colab ã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‚­ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã¨äº’æ›æ€§ãŒã‚ã‚Šã€Colab å›ºæœ‰ã® `google.colab.userdata` ã®ä»£æ›¿ã¨ãªã‚‹ `set_env` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯ [ã“ã¡ã‚‰](https://pypi.org/project/set-env-colab-kaggle-dotenv/) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

```python lines
# ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¾ã™ã€‚
from set_env import set_env

_ = set_env("OPENAI_API_KEY")
_ = set_env("WANDB_API_KEY")
```

æœ€å¾Œã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

```python lines
import base64
import os
import time
import wave

import numpy as np
from IPython.display import display
from openai import OpenAI

import weave
```

## éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¨ä¿å­˜ã®ä¾‹

æ¬¡ã«ã€éŸ³å£°ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’æœ‰åŠ¹ã«ã—ãŸ OpenAI ã® completions ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¸ã®ã‚³ãƒ¼ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚ã¾ãšã€OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆã—ã€Weave ã® Projects ã‚’é–‹å§‹ã—ã¾ã™ã€‚

```python lines
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
weave.init("openai-audio-chat")
```

ã“ã“ã§ã€OpenAI ã® completions ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®šç¾©ã—ã€Weave ã®ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ (op) ã‚’è¿½åŠ ã—ã¾ã™ã€‚

é–¢æ•° `prompt_endpont_and_log_trace` ã‚’å®šç¾©ã—ã¾ã™ã€‚ã“ã®é–¢æ•°ã«ã¯ä¸»ã« 3 ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒã‚ã‚Šã¾ã™ï¼š

1. ãƒ†ã‚­ã‚¹ãƒˆã¨éŸ³å£°ã®å…¥å‡ºåŠ›ã«å¯¾å¿œã—ãŸ `GPT 4o Audio Preview` ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€completion ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

   - ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã€ã‚¢ã‚¯ã‚»ãƒ³ãƒˆã‚’å¤‰ãˆãªãŒã‚‰ã‚†ã£ãã‚Šã¨ 13 ã¾ã§æ•°ãˆã‚‹ã‚ˆã†ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‡ºã—ã¾ã™ã€‚
   - completion ã‚’ "stream" ã«è¨­å®šã—ã¾ã™ã€‚

2. ã‚¹ãƒˆãƒªãƒ¼ãƒ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«æ›¸ãè¾¼ã¾ã‚Œã‚‹æ–°ã—ã„å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ãã¾ã™ã€‚

3. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿”ã—ã€Weave ãŒãƒˆãƒ¬ãƒ¼ã‚¹å†…ã«éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```python lines
SAMPLE_RATE = 22050

@weave.op()
def prompt_endpoint_and_log_trace(system_prompt=None, user_prompt=None):
    if not system_prompt:
        system_prompt = "You're the fastest counter in the world"
    if not user_prompt:
        user_prompt = "Count to 13 super super slow, enunciate each number with a dramatic flair, changing up accents as you go along. British, French, German, Spanish, etc."
    # éŸ³å£°ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’æŒ‡å®šã—ã¦ OpenAI API ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    completion = client.chat.completions.create(
        model="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "fable", "format": "pcm16"},
        stream=True,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )

    # æ›¸ãè¾¼ã¿ç”¨ã« wave ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
    with wave.open("./output.wav", "wb") as wav_file:
        wav_file.setnchannels(1)  # ãƒ¢ãƒãƒ©ãƒ«
        wav_file.setsampwidth(2)  # 16-bit
        wav_file.setframerate(SAMPLE_RATE)  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ (å¿…è¦ã«å¿œã˜ã¦èª¿æ•´)

        # API ã‹ã‚‰ã‚¹ãƒˆãƒªãƒ¼ãƒ ã•ã‚Œã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’æ›¸ãè¾¼ã‚€
        for chunk in completion:
            if (
                hasattr(chunk, "choices")
                and chunk.choices is not None
                and len(chunk.choices) > 0
                and hasattr(chunk.choices[0].delta, "audio")
                and chunk.choices[0].delta.audio.get("data") is not None
            ):
                # base64 éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                audio_data = base64.b64decode(chunk.choices[0].delta.audio.get("data"))

                # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ wave ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€
                wav_file.writeframes(audio_data)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ Weave op ã«è¿”ã™
    return wave.open("output.wav", "rb")
```

## ãƒ†ã‚¹ãƒˆ

ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ãŠã‚ˆã³å‡ºåŠ›éŸ³å£°ãŒ Weave ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚
ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ãŸå¾Œã€"ğŸ©" çµµæ–‡å­—ã®éš£ã«ã‚ã‚‹ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```python lines
from IPython.display import Audio

# éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’æ›¸ãè¾¼ã‚€é–¢æ•°ã‚’å‘¼ã³å‡ºã™
prompt_endpoint_and_log_trace(
    system_prompt="You're the fastest counter in the world",
    user_prompt="Count to 13 super super slow, enunciate each number with a dramatic flair, changing up accents as you go along. British, French, German, Spanish, etc.",
)

# æ›´æ–°ã•ã‚ŒãŸéŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¡¨ç¤º
display(Audio("output.wav", rate=SAMPLE_RATE, autoplay=True))
```

# é«˜åº¦ãªä½¿ã„æ–¹ï¼šWeave ã‚’ä½¿ã£ãŸ Realtime Audio API

<img src="https://i.imgur.com/ZiW3IVu.png"/>
<details>
<summary> (é«˜åº¦ãªè¨­å®š) Weave ã‚’ä½¿ã£ãŸ Realtime Audio API </summary>
OpenAI ã® realtime API ã¯ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®éŸ³å£°ãŠã‚ˆã³ãƒ†ã‚­ã‚¹ãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ã€éå¸¸ã«æ©Ÿèƒ½çš„ã§ä¿¡é ¼æ€§ã®é«˜ã„ä¼šè©±å‹ API ã§ã™ã€‚

æ³¨æ„ç‚¹ï¼š

- [ãƒã‚¤ã‚¯ã®è¨­å®š](#microphone-configuration) ã®ã‚»ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
- Google Colab ã®å®Ÿè¡Œç’°å¢ƒã®åˆ¶é™ã«ã‚ˆã‚Šã€**ã“ã‚Œã¯ãƒ›ã‚¹ãƒˆãƒã‚·ãƒ³ä¸Šã§ Jupyter Notebook ã¨ã—ã¦å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™**ã€‚ãƒ–ãƒ©ã‚¦ã‚¶å†…ã§ã¯å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚
  - MacOS ã®å ´åˆã€Pyaudio ã‚’å‹•ä½œã•ã›ã‚‹ãŸã‚ã« Brew çµŒç”±ã§ `portaudio` ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆ[ã“ã¡ã‚‰](https://formulae.brew.sh/formula/portaudio)ã‚’å‚ç…§ï¼‰ã€‚
- OpenAI ã® Python SDK ã¯ã€ã¾ã  Realtime API ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚å¯èª­æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã€Pydantic ã§å®Œå…¨ãª OAI Realtime API ã‚¹ã‚­ãƒ¼ãƒã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚å…¬å¼ã‚µãƒãƒ¼ãƒˆãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸå¾Œã¯éæ¨å¥¨ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
- `enable_audio_playback` ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã¨ã€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŒå‡ºåŠ›ã—ãŸéŸ³å£°ãŒå†ç”Ÿã•ã‚Œã¾ã™ã€‚ã‚¨ã‚³ãƒ¼æ¤œå‡ºã«ã¯éå¸¸ã«è¤‡é›‘ãªå®Ÿè£…ãŒå¿…è¦ãªãŸã‚ã€**ã“ã‚Œã‚’æœ‰åŠ¹ã«ã™ã‚‹å ´åˆã¯ãƒ˜ãƒƒãƒ‰ãƒ•ã‚©ãƒ³ãŒå¿…é ˆã§ã™**ã€‚

## æ§‹æˆè¦ä»¶ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```python lines
%%capture
!pip install numpy==2.0
!pip install weave
!pip install pyaudio # mac ã§ã¯ã€å…ˆã« `brew install portaudio` ã§ portaudio ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™
!pip install websocket-client
!pip install set-env-colab-kaggle-dotenv -q # ç’°å¢ƒå¤‰æ•°ç”¨
!pip install resampy
python
import io
import json
import os
import threading
from typing import Optional

import pyaudio
import resampy
import websocket
from set_env import set_env

import weave
python
# ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¾ã™ã€‚
# ä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯ https://pypi.org/project/set-env-colab-kaggle-dotenv/ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
_ = set_env("OPENAI_API_KEY")
_ = set_env("WANDB_API_KEY")
```

## ãƒã‚¤ã‚¯ã®è¨­å®š

ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€åˆ©ç”¨å¯èƒ½ãªã™ã¹ã¦ã®ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒã‚¤ã‚¹ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚æ¬¡ã«ã€ãƒªã‚¹ãƒˆã•ã‚ŒãŸãƒ‡ãƒã‚¤ã‚¹ã«åŸºã¥ã„ã¦ `INPUT_DEVICE_INDEX` ã¨ `OUTPUT_DEVICE_INDEX` ã‚’å…¥åŠ›ã—ã¾ã™ã€‚å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ã¯å°‘ãªãã¨ã‚‚ 1 ã¤ã®å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«ã‚’æŒã¡ã€å‡ºåŠ›ãƒ‡ãƒã‚¤ã‚¹ã¯å°‘ãªãã¨ã‚‚ 1 ã¤ã®å‡ºåŠ›ãƒãƒ£ãƒ³ãƒãƒ«ã‚’æŒã¡ã¾ã™ã€‚

```python lines
# æ¬¡ã®ã‚»ãƒ«ã‚’è¨­å®šã§ãã‚‹ã‚ˆã†ã«ã€pyaudio ã‹ã‚‰ãƒ‡ãƒã‚¤ã‚¹ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã™
p = pyaudio.PyAudio()
devices_data = {i: p.get_device_info_by_index(i) for i in range(p.get_device_count())}
for i, device in devices_data.items():
    print(
        f"Found device @{i}: {device['name']} with sample rate: {device['defaultSampleRate']} and input channels: {device['maxInputChannels']} and output channels: {device['maxOutputChannels']}"
    )
python
INPUT_DEVICE_INDEX = 3  # @param                                                 # ä¸Šè¨˜ã®ãƒ‡ãƒã‚¤ã‚¹ãƒªã‚¹ãƒˆã«åŸºã¥ã„ã¦é¸æŠã—ã¦ãã ã•ã„ã€‚å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«ãŒ 0 ã‚ˆã‚Šå¤§ãã„ãƒ‡ãƒã‚¤ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚
OUTPUT_DEVICE_INDEX = 12  # @param                                                # ä¸Šè¨˜ã®ãƒ‡ãƒã‚¤ã‚¹ãƒªã‚¹ãƒˆã«åŸºã¥ã„ã¦é¸æŠã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ãƒãƒ£ãƒ³ãƒãƒ«ãŒ 0 ã‚ˆã‚Šå¤§ãã„ãƒ‡ãƒã‚¤ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚
enable_audio_playback = True  # @param {type:"boolean"}                           # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®éŸ³å£°å†ç”Ÿã‚’ã‚ªãƒ³ã«ã—ã¾ã™ã€‚ãƒ˜ãƒƒãƒ‰ãƒ•ã‚©ãƒ³ãŒå¿…è¦ã§ã™ã€‚

# éŸ³å£°éŒ²éŸ³ã¨ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
INPUT_DEVICE_CHANNELS = devices_data[INPUT_DEVICE_INDEX][
    "maxInputChannels"
]  # ä¸Šè¨˜ã®ãƒ‡ãƒã‚¤ã‚¹ãƒªã‚¹ãƒˆã‚ˆã‚Š
SAMPLE_RATE = int(
    devices_data[INPUT_DEVICE_INDEX]["defaultSampleRate"]
)  # ä¸Šè¨˜ã®ãƒ‡ãƒã‚¤ã‚¹ãƒªã‚¹ãƒˆã‚ˆã‚Š
CHUNK = int(SAMPLE_RATE / 10)  # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚ãŸã‚Šã®ã‚µãƒ³ãƒ—ãƒ«æ•°
SAMPLE_WIDTH = p.get_sample_size(pyaudio.paInt16)  # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚ãŸã‚Šã®ã‚µãƒ³ãƒ—ãƒ«å¹…
CHUNK_DURATION = 0.3  # OAI API ã«é€ä¿¡ã•ã‚Œã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®éŸ³å£°ç§’æ•°
OAI_SAMPLE_RATE = (
    24000  # OAI ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã¯ 24kHz ã§ã™ã€‚ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®éŸ³å£°ã‚’å†ç”Ÿã¾ãŸã¯ä¿å­˜ã™ã‚‹ãŸã‚ã«ã“ã‚ŒãŒå¿…è¦ã§ã™ã€‚
)
OUTPUT_DEVICE_CHANNELS = 1  # ãƒ¢ãƒãƒ©ãƒ«å‡ºåŠ›ã®å ´åˆã¯ 1 ã«è¨­å®š
```

## OpenAI Realtime API ã‚¹ã‚­ãƒ¼ãƒã®å®Ÿè£…

OpenAI Python SDK ã¯ã¾ã  Realtime API ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚å¯èª­æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã€Pydantic ã§å®Œå…¨ãª OAI Realtime API ã‚¹ã‚­ãƒ¼ãƒã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚å…¬å¼ã‚µãƒãƒ¼ãƒˆãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸå¾Œã¯éæ¨å¥¨ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

<details>
<summary> OpenAI Realtime API ç”¨ã® Pydantic ã‚¹ã‚­ãƒ¼ãƒ (OpenAI ã® SDK ã«ã¯ Realtime API ã‚µãƒãƒ¼ãƒˆãŒæ¬ ã‘ã¦ã„ã¾ã™) </summary>

```python lines
from enum import Enum
from typing import Any, Literal, Union

from pydantic import BaseModel, Field, ValidationError

class BaseEvent(BaseModel):
    type: Union["ClientEventTypes", "ServerEventTypes"]
    event_id: Optional[str] = None  # ã™ã¹ã¦ã®ã‚¤ãƒ™ãƒ³ãƒˆã«å¯¾ã—ã¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨ã—ã¦ event_id ã‚’è¿½åŠ 

    # def model_dump_json(self, *args, **kwargs):
    #     # None ä»¥å¤–ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ã‚’å«ã‚ã‚‹
    #     return super().model_dump_json(*args, exclude_none=True, **kwargs)

class ChatMessage(BaseModel):
    role: Literal["user", "assistant"]
    content: str
    timestamp: float

""" CLIENT EVENTS """

class ClientEventTypes(str, Enum):
    SESSION_UPDATE = "session.update"
    CONVERSATION_ITEM_CREATE = "conversation.item.create"
    CONVERSATION_ITEM_TRUNCATE = "conversation.item.truncate"
    CONVERSATION_ITEM_DELETE = "conversation.item.delete"
    RESPONSE_CREATE = "response.create"
    RESPONSE_CANCEL = "response.cancel"
    INPUT_AUDIO_BUFFER_APPEND = "input_audio_buffer.append"
    INPUT_AUDIO_BUFFER_COMMIT = "input_audio_buffer.commit"
    INPUT_AUDIO_BUFFER_CLEAR = "input_audio_buffer.clear"
    ERROR = "error"

#### Session Update
class TurnDetection(BaseModel):
    type: Literal["server_vad"]
    threshold: float = Field(..., ge=0.0, le=1.0)
    prefix_padding_ms: int
    silence_duration_ms: int

class InputAudioTranscription(BaseModel):
    model: Optional[str] = None

class ToolParameterProperty(BaseModel):
    type: str

class ToolParameter(BaseModel):
    type: str
    properties: dict[str, ToolParameterProperty]
    required: list[str]

class Tool(BaseModel):
    type: Literal["function", "code_interpreter", "file_search"]
    name: Optional[str] = None
    description: Optional[str] = None
    parameters: Optional[ToolParameter] = None

class Session(BaseModel):
    modalities: Optional[list[str]] = None
    instructions: Optional[str] = None
    voice: Optional[str] = None
    input_audio_format: Optional[str] = None
    output_audio_format: Optional[str] = None
    input_audio_transcription: Optional[InputAudioTranscription] = None
    turn_detection: Optional[TurnDetection] = None
    tools: Optional[list[Tool]] = None
    tool_choice: Optional[str] = None
    temperature: Optional[float] = None
    max_output_tokens: Optional[int] = None

class SessionUpdate(BaseEvent):
    type: Literal[ClientEventTypes.SESSION_UPDATE] = ClientEventTypes.SESSION_UPDATE
    session: Session

#### Audio Buffers
class InputAudioBufferAppend(BaseEvent):
    type: Literal[ClientEventTypes.INPUT_AUDIO_BUFFER_APPEND] = (
        ClientEventTypes.INPUT_AUDIO_BUFFER_APPEND
    )
    audio: str

class InputAudioBufferCommit(BaseEvent):
    type: Literal[ClientEventTypes.INPUT_AUDIO_BUFFER_COMMIT] = (
        ClientEventTypes.INPUT_AUDIO_BUFFER_COMMIT
    )

class InputAudioBufferClear(BaseEvent):
    type: Literal[ClientEventTypes.INPUT_AUDIO_BUFFER_CLEAR] = (
        ClientEventTypes.INPUT_AUDIO_BUFFER_CLEAR
    )

#### Messages
class MessageContent(BaseModel):
    type: Literal["input_audio"]
    audio: str

class ConversationItemContent(BaseModel):
    type: Literal["input_text", "input_audio", "text", "audio"]
    text: Optional[str] = None
    audio: Optional[str] = None
    transcript: Optional[str] = None

class FunctionCallContent(BaseModel):
    call_id: str
    name: str
    arguments: str

class FunctionCallOutputContent(BaseModel):
    output: str

class ConversationItem(BaseModel):
    id: Optional[str] = None
    type: Literal["message", "function_call", "function_call_output"]
    status: Optional[Literal["completed", "in_progress", "incomplete"]] = None
    role: Literal["user", "assistant", "system"]
    content: list[
        Union[ConversationItemContent, FunctionCallContent, FunctionCallOutputContent]
    ]
    call_id: Optional[str] = None
    name: Optional[str] = None
    arguments: Optional[str] = None
    output: Optional[str] = None

class ConversationItemCreate(BaseEvent):
    type: Literal[ClientEventTypes.CONVERSATION_ITEM_CREATE] = (
        ClientEventTypes.CONVERSATION_ITEM_CREATE
    )
    item: ConversationItem

class ConversationItemTruncate(BaseEvent):
    type: Literal[ClientEventTypes.CONVERSATION_ITEM_TRUNCATE] = (
        ClientEventTypes.CONVERSATION_ITEM_TRUNCATE
    )
    item_id: str
    content_index: int
    audio_end_ms: int

class ConversationItemDelete(BaseEvent):
    type: Literal[ClientEventTypes.CONVERSATION_ITEM_DELETE] = (
        ClientEventTypes.CONVERSATION_ITEM_DELETE
    )
    item_id: str

#### Responses
class ResponseCreate(BaseEvent):
    type: Literal[ClientEventTypes.RESPONSE_CREATE] = ClientEventTypes.RESPONSE_CREATE

class ResponseCancel(BaseEvent):
    type: Literal[ClientEventTypes.RESPONSE_CANCEL] = ClientEventTypes.RESPONSE_CANCEL

# Event union ã‚’æ›´æ–°ã—ã¦ã™ã¹ã¦ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã‚’å«ã‚ã‚‹
ClientEvent = Union[
    SessionUpdate,
    InputAudioBufferAppend,
    InputAudioBufferCommit,
    InputAudioBufferClear,
    ConversationItemCreate,
    ConversationItemTruncate,
    ConversationItemDelete,
    ResponseCreate,
    ResponseCancel,
]

""" SERVER EVENTS """

class ServerEventTypes(str, Enum):
    ERROR = "error"
    RESPONSE_AUDIO_TRANSCRIPT_DONE = "response.audio_transcript.done"
    RESPONSE_AUDIO_TRANSCRIPT_DELTA = "response.audio_transcript.delta"
    RESPONSE_AUDIO_DELTA = "response.audio.delta"
    SESSION_CREATED = "session.created"
    SESSION_UPDATED = "session.updated"
    CONVERSATION_CREATED = "conversation.created"
    INPUT_AUDIO_BUFFER_COMMITTED = "input_audio_buffer.committed"
    INPUT_AUDIO_BUFFER_CLEARED = "input_audio_buffer.cleared"
    INPUT_AUDIO_BUFFER_SPEECH_STARTED = "input_audio_buffer.speech_started"
    INPUT_AUDIO_BUFFER_SPEECH_STOPPED = "input_audio_buffer.speech_stopped"
    CONVERSATION_ITEM_CREATED = "conversation.item.created"
    CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED = (
        "conversation.item.input_audio_transcription.completed"
    )
    CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED = (
        "conversation.item.input_audio_transcription.failed"
    )
    CONVERSATION_ITEM_TRUNCATED = "conversation.item.truncated"
    CONVERSATION_ITEM_DELETED = "conversation.item.deleted"
    RESPONSE_CREATED = "response.created"
    RESPONSE_DONE = "response.done"
    RESPONSE_OUTPUT_ITEM_ADDED = "response.output_item.added"
    RESPONSE_OUTPUT_ITEM_DONE = "response.output_item.done"
    RESPONSE_CONTENT_PART_ADDED = "response.content_part.added"
    RESPONSE_CONTENT_PART_DONE = "response.content_part.done"
    RESPONSE_TEXT_DELTA = "response.text.delta"
    RESPONSE_TEXT_DONE = "response.text.done"
    RESPONSE_AUDIO_DONE = "response.audio.done"
    RESPONSE_FUNCTION_CALL_ARGUMENTS_DELTA = "response.function_call_arguments.delta"
    RESPONSE_FUNCTION_CALL_ARGUMENTS_DONE = "response.function_call_arguments.done"
    RATE_LIMITS_UPDATED = "rate_limits.updated"

#### Errors
class ErrorDetails(BaseModel):
    type: Optional[str] = None
    code: Optional[str] = None
    message: Optional[str] = None
    param: Optional[str] = None

class ErrorEvent(BaseEvent):
    type: Literal[ServerEventTypes.ERROR] = ServerEventTypes.ERROR
    error: ErrorDetails

#### Session
class SessionCreated(BaseEvent):
    type: Literal[ServerEventTypes.SESSION_CREATED] = ServerEventTypes.SESSION_CREATED
    session: Session

class SessionUpdated(BaseEvent):
    type: Literal[ServerEventTypes.SESSION_UPDATED] = ServerEventTypes.SESSION_UPDATED
    session: Session

#### Conversation
class Conversation(BaseModel):
    id: str
    object: Literal["realtime.conversation"]

class ConversationCreated(BaseEvent):
    type: Literal[ServerEventTypes.CONVERSATION_CREATED] = (
        ServerEventTypes.CONVERSATION_CREATED
    )
    conversation: Conversation

class ConversationItemCreated(BaseEvent):
    type: Literal[ServerEventTypes.CONVERSATION_ITEM_CREATED] = (
        ServerEventTypes.CONVERSATION_ITEM_CREATED
    )
    previous_item_id: Optional[str] = None
    item: ConversationItem

class ConversationItemInputAudioTranscriptionCompleted(BaseEvent):
    type: Literal[
        ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED
    ] = ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED
    item_id: str
    content_index: int
    transcript: str

class ConversationItemInputAudioTranscriptionFailed(BaseEvent):
    type: Literal[
        ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED
    ] = ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED
    item_id: str
    content_index: int
    error: dict[str, Any]

class ConversationItemTruncated(BaseEvent):
    type: Literal[ServerEventTypes.CONVERSATION_ITEM_TRUNCATED] = (
        ServerEventTypes.CONVERSATION_ITEM_TRUNCATED
    )
    item_id: str
    content_index: int
    audio_end_ms: int

class ConversationItemDeleted(BaseEvent):
    type: Literal[ServerEventTypes.CONVERSATION_ITEM_DELETED] = (
        ServerEventTypes.CONVERSATION_ITEM_DELETED
    )
    item_id: str

#### Response
class ResponseUsage(BaseModel):
    total_tokens: int
    input_tokens: int
    output_tokens: int
    input_token_details: Optional[dict[str, int]] = None
    output_token_details: Optional[dict[str, int]] = None

class ResponseOutput(BaseModel):
    id: str
    object: Literal["realtime.item"]
    type: str
    status: str
    role: str
    content: list[dict[str, Any]]

class ResponseContentPart(BaseModel):
    type: str
    text: Optional[str] = None

class ResponseOutputItemContent(BaseModel):
    type: str
    text: Optional[str] = None

class ResponseStatusDetails(BaseModel):
    type: str
    reason: str

class ResponseOutputItem(BaseModel):
    id: str
    object: Literal["realtime.item"]
    type: str
    status: str
    role: str
    content: list[ResponseOutputItemContent]

class Response(BaseModel):
    id: str
    object: Literal["realtime.response"]
    status: str
    status_details: Optional[ResponseStatusDetails] = None
    output: list[ResponseOutput]
    usage: Optional[ResponseUsage]

class ResponseCreated(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_CREATED] = ServerEventTypes.RESPONSE_CREATED
    response: Response

class ResponseDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_DONE] = ServerEventTypes.RESPONSE_DONE
    response: Response

class ResponseOutputItemAdded(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_OUTPUT_ITEM_ADDED] = (
        ServerEventTypes.RESPONSE_OUTPUT_ITEM_ADDED
    )
    response_id: str
    output_index: int
    item: ResponseOutputItem

class ResponseOutputItemDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_OUTPUT_ITEM_DONE] = (
        ServerEventTypes.RESPONSE_OUTPUT_ITEM_DONE
    )
    response_id: str
    output_index: int
    item: ResponseOutputItem

class ResponseContentPartAdded(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_CONTENT_PART_ADDED] = (
        ServerEventTypes.RESPONSE_CONTENT_PART_ADDED
    )
    response_id: str
    item_id: str
    output_index: int
    content_index: int
    part: ResponseContentPart

class ResponseContentPartDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_CONTENT_PART_DONE] = (
        ServerEventTypes.RESPONSE_CONTENT_PART_DONE
    )
    response_id: str
    item_id: str
    output_index: int
    content_index: int
    part: ResponseContentPart

#### Response Text
class ResponseTextDelta(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_TEXT_DELTA] = (
        ServerEventTypes.RESPONSE_TEXT_DELTA
    )
    response_id: str
    item_id: str
    output_index: int
    content_index: int
    delta: str

class ResponseTextDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_TEXT_DONE] = (
        ServerEventTypes.RESPONSE_TEXT_DONE
    )
    response_id: str
    item_id: str
    output_index: int
    content_index: int
    text: str

#### Response Audio
class ResponseAudioTranscriptDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE] = (
        ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE
    )
    transcript: str

class ResponseAudioTranscriptDelta(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DELTA] = (
        ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DELTA
    )
    delta: str

class ResponseAudioDelta(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_AUDIO_DELTA] = (
        ServerEventTypes.RESPONSE_AUDIO_DELTA
    )
    response_id: str
    item_id: str
    delta: str

class ResponseAudioDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_AUDIO_DONE] = (
        ServerEventTypes.RESPONSE_AUDIO_DONE
    )
    response_id: str
    item_id: str
    output_index: int
    content_index: int

class InputAudioBufferCommitted(BaseEvent):
    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_COMMITTED] = (
        ServerEventTypes.INPUT_AUDIO_BUFFER_COMMITTED
    )
    previous_item_id: Optional[str] = None
    item_id: Optional[str] = None
    event_id: Optional[str] = None

class InputAudioBufferCleared(BaseEvent):
    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_CLEARED] = (
        ServerEventTypes.INPUT_AUDIO_BUFFER_CLEARED
    )

class InputAudioBufferSpeechStarted(BaseEvent):
    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STARTED] = (
        ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STARTED
    )
    audio_start_ms: int
    item_id: str

class InputAudioBufferSpeechStopped(BaseEvent):
    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STOPPED] = (
        ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STOPPED
    )
    audio_end_ms: int
    item_id: str

#### Function Calls
class ResponseFunctionCallArgumentsDelta(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DELTA] = (
        ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DELTA
    )
    response_id: str
    item_id: str
    output_index: int
    call_id: str
    delta: str

class ResponseFunctionCallArgumentsDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DONE] = (
        ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DONE
    )
    response_id: str
    item_id: str
    output_index: int
    call_id: str
    arguments: str

#### Rate Limits
class RateLimit(BaseModel):
    name: str
    limit: int
    remaining: int
    reset_seconds: float

class RateLimitsUpdated(BaseEvent):
    type: Literal[ServerEventTypes.RATE_LIMITS_UPDATED] = (
        ServerEventTypes.RATE_LIMITS_UPDATED
    )
    rate_limits: list[RateLimit]

ServerEvent = Union[
    ErrorEvent,
    ConversationCreated,
    ResponseAudioTranscriptDone,
    ResponseAudioTranscriptDelta,
    ResponseAudioDelta,
    ResponseCreated,
    ResponseDone,
    ResponseOutputItemAdded,
    ResponseOutputItemDone,
    ResponseContentPartAdded,
    ResponseContentPartDone,
    ResponseTextDelta,
    ResponseTextDone,
    ResponseAudioDone,
    ConversationItemInputAudioTranscriptionCompleted,
    SessionCreated,
    SessionUpdated,
    InputAudioBufferCleared,
    InputAudioBufferSpeechStarted,
    InputAudioBufferSpeechStopped,
    ConversationItemCreated,
    ConversationItemInputAudioTranscriptionFailed,
    ConversationItemTruncated,
    ConversationItemDeleted,
    RateLimitsUpdated,
]

EVENT_TYPE_TO_MODEL = {
    ServerEventTypes.ERROR: ErrorEvent,
    ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE: ResponseAudioTranscriptDone,
    ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DELTA: ResponseAudioTranscriptDelta,
    ServerEventTypes.RESPONSE_AUDIO_DELTA: ResponseAudioDelta,
    ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED: ConversationItemInputAudioTranscriptionCompleted,
    ServerEventTypes.SESSION_CREATED: SessionCreated,
    ServerEventTypes.SESSION_UPDATED: SessionUpdated,
    ServerEventTypes.CONVERSATION_CREATED: ConversationCreated,
    ServerEventTypes.INPUT_AUDIO_BUFFER_COMMITTED: InputAudioBufferCommitted,
    ServerEventTypes.INPUT_AUDIO_BUFFER_CLEARED: InputAudioBufferCleared,
    ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STARTED: InputAudioBufferSpeechStarted,
    ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STOPPED: InputAudioBufferSpeechStopped,
    ServerEventTypes.CONVERSATION_ITEM_CREATED: ConversationItemCreated,
    ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED: ConversationItemInputAudioTranscriptionFailed,
    ServerEventTypes.CONVERSATION_ITEM_TRUNCATED: ConversationItemTruncated,
    ServerEventTypes.CONVERSATION_ITEM_DELETED: ConversationItemDeleted,
    ServerEventTypes.RESPONSE_CREATED: ResponseCreated,
    ServerEventTypes.RESPONSE_DONE: ResponseDone,
    ServerEventTypes.RESPONSE_OUTPUT_ITEM_ADDED: ResponseOutputItemAdded,
    ServerEventTypes.RESPONSE_OUTPUT_ITEM_DONE: ResponseOutputItemDone,
    ServerEventTypes.RESPONSE_CONTENT_PART_ADDED: ResponseContentPartAdded,
    ServerEventTypes.RESPONSE_CONTENT_PART_DONE: ResponseContentPartDone,
    ServerEventTypes.RESPONSE_TEXT_DELTA: ResponseTextDelta,
    ServerEventTypes.RESPONSE_TEXT_DONE: ResponseTextDone,
    ServerEventTypes.RESPONSE_AUDIO_DONE: ResponseAudioDone,
    ServerEventTypes.RATE_LIMITS_UPDATED: RateLimitsUpdated,
}

def parse_server_event(event_data: dict) -> ServerEvent:
    event_type = event_data.get("type")
    if not event_type:
        raise ValueError("Event data is missing 'type' field")

    model_class = EVENT_TYPE_TO_MODEL.get(event_type)
    if not model_class:
        raise ValueError(f"Unknown event type: {event_type}")

    try:
        return model_class(**event_data)
    except ValidationError as e:
        raise ValueError(f"Failed to parse event of type {event_type}: {str(e)}") from e
```

</details>

## éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ©ã‚¤ã‚¿ãƒ¼ (ãƒ‡ã‚£ã‚¹ã‚¯ãŠã‚ˆã³ãƒ¡ãƒ¢ãƒªã¸)

```python lines
class StreamingWavWriter:
    """éŸ³å£°ã®æ•´æ•°ã¾ãŸã¯ãƒã‚¤ãƒˆé…åˆ—ãƒãƒ£ãƒ³ã‚¯ã‚’ WAV ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿ã¾ã™ã€‚"""

    wav_file = None
    buffer = None
    in_memory = False

    def __init__(
        self,
        filename=None,
        channels=INPUT_DEVICE_CHANNELS,
        sample_width=SAMPLE_WIDTH,
        framerate=SAMPLE_RATE,
    ):
        self.in_memory = filename is None
        if self.in_memory:
            self.buffer = io.BytesIO()
            self.wav_file = wave.open(self.buffer, "wb")
        else:
            self.wav_file = wave.open(filename, "wb")

        self.wav_file.setnchannels(channels)
        self.wav_file.setsampwidth(sample_width)
        self.wav_file.setframerate(framerate)

    def append_int16_chunk(self, int16_data):
        if int16_data is not None:
            self.wav_file.writeframes(
                int16_data.tobytes()
                if isinstance(int16_data, np.ndarray)
                else int16_data
            )

    def close(self):
        self.wav_file.close()

    def get_wav_buffer(self):
        assert self.in_memory, "Buffer only available if stream is in memory."
        return self.buffer
```

## ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ãƒ¢ãƒ‡ãƒ«

ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  (RT) éŸ³å£°ãƒ¢ãƒ‡ãƒ«ã¯ã€websocket ã‚’ä½¿ç”¨ã—ã¦ OpenAI ã® Realtime audio API ã«ã‚¤ãƒ™ãƒ³ãƒˆã‚’é€ä¿¡ã—ã¾ã™ã€‚ä»•çµ„ã¿ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

1.  **init:** ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒãƒ•ã‚¡ (å…¥åŠ›éŸ³å£°) ã¨ã‚¹ãƒˆãƒªãƒ¼ãƒ  (ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå†ç”Ÿã‚¹ãƒˆãƒªãƒ¼ãƒ ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼éŸ³å£°ãƒ‡ã‚£ã‚¹ã‚¯ãƒ©ã‚¤ã‚¿ãƒ¼ã‚¹ãƒˆãƒªãƒ¼ãƒ ) ã‚’åˆæœŸåŒ–ã—ã€Realtime API ã¸ã®æ¥ç¶šã‚’é–‹ãã¾ã™ã€‚
2.  **receive_messages_thread**: ã‚¹ãƒ¬ãƒƒãƒ‰ãŒ API ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡ã‚’å‡¦ç†ã—ã¾ã™ã€‚ä¸»ã« 4 ã¤ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ãŒå‡¦ç†ã•ã‚Œã¾ã™ï¼š - RESPONSE_AUDIO_TRANSCRIPT_DONE:

            ã‚µãƒ¼ãƒãƒ¼ãŒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒå®Œäº†ã—ãŸã“ã¨ã‚’ç¤ºã—ã€æ›¸ãèµ·ã“ã—ã‚’æä¾›ã—ã¾ã™ã€‚

        - CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED:

            ã‚µãƒ¼ãƒãƒ¼ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³å£°ãŒæ›¸ãèµ·ã“ã•ã‚ŒãŸã“ã¨ã‚’ç¤ºã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼éŸ³å£°ã®æ›¸ãèµ·ã“ã—ã‚’é€ä¿¡ã—ã¾ã™ã€‚ã“ã®æ›¸ãèµ·ã“ã—ã‚’ Weave ã«è¨˜éŒ²ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã«è¡¨ç¤ºã—ã¾ã™ã€‚

        - RESPONSE_AUDIO_DELTA:

            ã‚µãƒ¼ãƒãƒ¼ãŒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹éŸ³å£°ã®æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’é€ä¿¡ã—ã¾ã™ã€‚ã“ã‚Œã‚’ãƒ¬ã‚¹ãƒãƒ³ã‚¹ ID ã‚’ä»‹ã—ã¦é€²è¡Œä¸­ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ ã—ã€å†ç”Ÿç”¨ã®å‡ºåŠ›ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«åŠ ãˆã¾ã™ã€‚

        - RESPONSE_DONE:

            ã‚µãƒ¼ãƒãƒ¼ãŒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å®Œäº†ã‚’ç¤ºã—ã¾ã™ã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸã™ã¹ã¦ã®éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã¨æ›¸ãèµ·ã“ã—ã‚’å–å¾—ã—ã€ã“ã‚Œã‚‰ã‚’ Weave ã«è¨˜éŒ²ã—ã¾ã™ã€‚

    3.**send_audio**: ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ ã—ã€éŸ³å£°ãƒãƒƒãƒ•ã‚¡ãŒç‰¹å®šã®ã‚µã‚¤ã‚ºã«é”ã—ãŸã¨ãã«éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’é€ä¿¡ã—ã¾ã™ã€‚

```python lines
class RTAudioModel(weave.Model):
    """Weave ãƒ­ã‚°ç”¨ã® Whisper ãƒ¦ãƒ¼ã‚¶ãƒ¼æ›¸ãèµ·ã“ã—ã‚’å‚™ãˆãŸã€OpenAI ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  e2e éŸ³å£°ãƒ¢ãƒ‡ãƒ«å¯¾è©±ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã€‚"""

    realtime_model_name: str = "gpt-4o-realtime-preview-2024-10-01"  # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  e2e éŸ³å£°ã®ã¿ã®ãƒ¢ãƒ‡ãƒ«å¯¾è©±

    stop_event: Optional[threading.Event] = threading.Event()  # ãƒ¢ãƒ‡ãƒ«ã‚’åœæ­¢ã•ã›ã‚‹ãŸã‚ã®ã‚¤ãƒ™ãƒ³ãƒˆ
    ws: Optional[websocket.WebSocket] = None  # OpenAI é€šä¿¡ç”¨ã® Websocket

    user_wav_writer: Optional[StreamingWavWriter] = (
        None  # ãƒ¦ãƒ¼ã‚¶ãƒ¼å‡ºåŠ›ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€ãŸã‚ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ 
    )
    input_audio_buffer: Optional[np.ndarray] = None  # ãƒ¦ãƒ¼ã‚¶ãƒ¼éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ç”¨ã®ãƒãƒƒãƒ•ã‚¡
    assistant_outputs: dict[str, StreamingWavWriter] = (
        None  # Weave ã«é€ä¿¡ã™ã‚‹ãŸã‚ã«é›†ç´„ã•ã‚ŒãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå‡ºåŠ›
    )
    playback_stream: Optional[pyaudio.Stream] = (
        None  # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å†ç”Ÿã™ã‚‹ãŸã‚ã®å†ç”Ÿã‚¹ãƒˆãƒªãƒ¼ãƒ 
    )

    def __init__(self):
        super().__init__()
        self.stop_event.clear()
        self.user_wav_writer = StreamingWavWriter(
            filename="user_audio.wav", framerate=SAMPLE_RATE
        )
        self.input_audio_buffer = np.array([], dtype=np.int16)
        self.ws = websocket.WebSocket()
        self.assistant_outputs = {}

        # æœ‰åŠ¹ãªå ´åˆã€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆéŸ³å£°å†ç”Ÿã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’é–‹ã
        if enable_audio_playback:
            self.playback_stream = pyaudio.PyAudio().open(
                format=pyaudio.paInt16,
                channels=OUTPUT_DEVICE_CHANNELS,
                rate=OAI_SAMPLE_RATE,
                output=True,
                output_device_index=OUTPUT_DEVICE_INDEX,
            )

        # Websocket æ¥ç¶š
        try:
            self.ws.connect(
                f"wss://api.openai.com/v1/realtime?model={self.realtime_model_name}",
                header={
                    "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}",
                    "OpenAI-Beta": "realtime=v1",
                },
            )

            # è¨­å®šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
            config_event = SessionUpdate(
                session=Session(
                    modalities=["text", "audio"],  # ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£
                    input_audio_transcription=InputAudioTranscription(
                        model="whisper-1"
                    ),  # æ›¸ãèµ·ã“ã—ç”¨ã® whisper-1
                    turn_detection=TurnDetection(
                        type="server_vad",
                        threshold=0.3,
                        prefix_padding_ms=300,
                        silence_duration_ms=600,
                    ),  # ç„¡éŸ³ã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã®ã‚µãƒ¼ãƒãƒ¼ VAD
                )
            )
            self.ws.send(config_event.model_dump_json(exclude_none=True))
            self.log_ws_message(config_event.model_dump_json(exclude_none=True), "Sent")

            # ãƒªã‚¹ãƒŠãƒ¼ã‚’é–‹å§‹
            websocket_thread = threading.Thread(target=self.receive_messages_thread)
            websocket_thread.daemon = True
            websocket_thread.start()

        except Exception as e:
            print(f"Error connecting to WebSocket: {e}")

    ##### Weave ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ #####
    def handle_assistant_response_audio_delta(self, data: ResponseAudioDelta):
        if data.response_id not in self.assistant_outputs:
            self.assistant_outputs[data.response_id] = StreamingWavWriter(
                framerate=OAI_SAMPLE_RATE
            )

        data_bytes = base64.b64decode(data.delta)
        self.assistant_outputs[data.response_id].append_int16_chunk(data_bytes)

        if enable_audio_playback:
            self.playback_stream.write(data_bytes)

        return {"assistant_audio": data_bytes}

    @weave.op()
    def handle_assistant_response_done(self, data: ResponseDone):
        wave_file_stream = self.assistant_outputs[data.response.id]
        wave_file_stream.close()
        wave_file_stream.buffer.seek(0)
        weave_payload = {
            "assistant_audio": wave.open(wave_file_stream.get_wav_buffer(), "rb"),
            "assistant_transcript": data.response.output[0]
            .content[0]
            .get("transcript", "Transcript Unavailable."),
        }
        return weave_payload

    @weave.op()
    def handle_user_transcription_done(
        self, data: ConversationItemInputAudioTranscriptionCompleted
    ):
        return {"user_transcript": data.transcript}

    ##### ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡æ©Ÿã¨é€ä¿¡æ©Ÿ #####
    def receive_messages_thread(self):
        while not self.stop_event.is_set():
            try:
                data = json.loads(self.ws.recv())
                self.log_ws_message(json.dumps(data, indent=2))

                parsed_event = parse_server_event(data)

                if parsed_event.type == ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE:
                    print("Assistant: ", parsed_event.transcript)
                elif (
                    parsed_event.type
                    == ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED
                ):
                    print("User: ", parsed_event.transcript)
                    self.handle_user_transcription_done(parsed_event)
                elif parsed_event.type == ServerEventTypes.RESPONSE_AUDIO_DELTA:
                    self.handle_assistant_response_audio_delta(parsed_event)
                elif parsed_event.type == ServerEventTypes.RESPONSE_DONE:
                    self.handle_assistant_response_done(parsed_event)
                elif parsed_event.type == ServerEventTypes.ERROR:
                    print(
                        f"\nError from server: {parsed_event.error.model_dump_json(exclude_none=True)}"
                    )
            except websocket.WebSocketConnectionClosedException:
                print("\nWebSocket connection closed")
                break
            except json.JSONDecodeError:
                continue
            except Exception as e:
                print(f"\nError in receive_messages: {e}")
                break

    def send_audio(self, audio_chunk):
        if self.ws and self.ws.connected:
            self.input_audio_buffer = np.append(
                self.input_audio_buffer, np.frombuffer(audio_chunk, dtype=np.int16)
            )
            if len(self.input_audio_buffer) >= SAMPLE_RATE * CHUNK_DURATION:
                try:
                    # éŸ³å£°ã‚’ OAI ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    resampled_audio = (
                        resampy.resample(
                            self.input_audio_buffer, SAMPLE_RATE, OAI_SAMPLE_RATE
                        )
                        if SAMPLE_RATE != OAI_SAMPLE_RATE
                        else self.input_audio_buffer
                    )

                    # éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’ OAI API ã«é€ä¿¡
                    audio_event = InputAudioBufferAppend(
                        audio=base64.b64encode(
                            resampled_audio.astype(np.int16).tobytes()
                        ).decode("utf-8")  # éŸ³å£°é…åˆ—ã‚’ b64 ãƒã‚¤ãƒˆã«å¤‰æ›
                    )
                    self.ws.send(audio_event.model_dump_json(exclude_none=True))
                    self.log_ws_message(
                        audio_event.model_dump_json(exclude_none=True), "Sent"
                    )
                finally:
                    self.user_wav_writer.append_int16_chunk(self.input_audio_buffer)

                    # éŸ³å£°ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
                    self.input_audio_buffer = np.array([], dtype=np.int16)
        else:
            print("Error sending audio: websocket not initialized.")

    ##### ä¸€èˆ¬çš„ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° #####
    def log_ws_message(self, message, direction="Received"):
        with open("websocket_log.txt", "a") as log_file:
            log_file.write(
                f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {direction}: {message}\n"
            )

    def stop(self):
        self.stop_event.set()

        if self.ws:
            self.ws.close()

        self.user_wav_writer.close()
```

## ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ¬ã‚³ãƒ¼ãƒ€ãƒ¼

RTAudio ãƒ¢ãƒ‡ãƒ«ã® `send_audio` ãƒ¡ã‚½ãƒƒãƒ‰ã«ãƒªãƒ³ã‚¯ã•ã‚ŒãŸãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’æŒã¤ pyaudio å…¥åŠ›ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¯ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã«è¿”ã•ã‚Œã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†æ™‚ã«å®‰å…¨ã«çµ‚äº†ã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚

```python lines
# éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚¹ãƒˆãƒªãƒ¼ãƒ 
def record_audio(realtime_model: RTAudioModel) -> pyaudio.Stream:
    """Pyaudio å…¥åŠ›ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¨­å®šã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ RTAudioModel ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"""

    def audio_callback(in_data, frame_count, time_info, status):
        realtime_model.send_audio(in_data)
        return (None, pyaudio.paContinue)

    p = pyaudio.PyAudio()
    stream = p.open(
        format=pyaudio.paInt16,
        channels=INPUT_DEVICE_CHANNELS,
        rate=SAMPLE_RATE,
        input=True,
        input_device_index=INPUT_DEVICE_INDEX,
        frames_per_buffer=CHUNK,
        stream_callback=audio_callback,
    )
    stream.start_stream()

    print("Recording started. Please begin speaking to your personal assistant...")
    return stream
```

## ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ï¼ˆå®Ÿè¡Œã—ã¦ãã ã•ã„ï¼ï¼‰

ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã¯ Weave ãŒçµ±åˆã•ã‚ŒãŸ Realtime Audio Model ã‚’é–‹å§‹ã—ã¾ã™ã€‚æ¬¡ã«ã€éŒ²éŸ³ãŒé–‹å§‹ã•ã‚Œã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å‰²ã‚Šè¾¼ã¿ã‚’å¾…ã¡ã¾ã™ã€‚

```python lines
weave.init(project_name="realtime-oai-audio-testing")

realtime_model = RTAudioModel()

if realtime_model.ws and realtime_model.ws.connected:
    recording_stream: pyaudio.Stream = record_audio(realtime_model)

    try:
        while not realtime_model.stop_event.is_set():
            time.sleep(1)
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print(f"Error in main loop: {e}")
        import traceback

        traceback.print_exc()
    finally:
        print("Exiting...")
        realtime_model.stop()
        if recording_stream and recording_stream.is_active():
            recording_stream.stop_stream()
            recording_stream.close()
else:
    print(
        "WebSocket connection failed. Please check your API key and internet connection."
    )
```

</details>