---
title: Run メトリクスを比較する
description: 複数の run 間でメトリクスを比較する
---

Run Comparer を使用すると、Project 内の Runs 間の違いや共通点を簡単に確認できます。

## Run Comparer パネルの追加

1. ページ右上の **Add panels** ボタンを選択します。
2. **Evaluation** セクションから、**Run comparer** を選択します。

## Run Comparer の使い方
Run Comparer は、Project 内で表示されている最初の 10 個の Runs について、設定やログに記録された メトリクス を 1 列に 1 Run ずつ表示します。

- 比較する Runs を変更するには、左側の Runs リストで検索、フィルタリング、グループ化、またはソートを行います。Run Comparer は自動的に更新されます。
- Run Comparer 上部の検索フィールドを使用して、設定の キー や、Python の バージョン、Run の作成時間などの メタデータ の キー をフィルタリングまたは検索できます。
- 違いを素早く確認し、同一の値を非表示にするには、パネル上部の **Diff only** を切り替えます。
- 列の幅や行の高さを調整するには、パネル上部のフォーマットボタンを使用します。
- 設定や メトリクス の値をコピーするには、値の上にマウスを置き、コピーボタンをクリックします。画面に表示しきれないほど長い値であっても、値全体がコピーされます。

<Note>
デフォルトでは、Run Comparer は [`job_type`](/models/ref/python/functions/init) の値が異なる Runs を区別しません。つまり、Project 内で本来比較対象にならない Runs 同士を比較してしまう可能性があります。例えば、トレーニングの Run と モデルの評価 の Run を比較することができてしまいます。トレーニングの Run には Run の ログ、ハイパーパラメーター、トレーニングの損失 メトリクス、そして モデル 自体が含まれる場合があります。一方で評価の Run は、その モデル を使用して新しい トレーニングデータ に対する モデル のパフォーマンスをチェックします。

Runs Table で Runs のリストを検索、フィルタリング、グループ化、またはソートすると、Run Comparer は自動的に更新され、最初の 10 個の Runs を比較します。`job_type` でリストをフィルタリングやソートするなど、Runs Table 内でフィルタリングや検索を行い、同種の Runs を比較するようにしてください。詳細は [Runs のフィルタリング](/models/runs/filter-runs/) を参照してください。
</Note>