---
title: "W&B Weave"
description: "Track, test, and improve language model apps with W&B Weave"
mode: wide
---

W&B Weave is a powerful observability and evaluation platform that helps you track, evaluate, and improve your LLM application's performance. Weave has the ability to:

* [Trace](/weave/quickstart) your application's LLM calls, capturing inputs, outputs, costs, and latency
* [Evaluate](/weave/guides/core-types/evaluations) and [monitor](/weave/guides/evaluation/guardrails_and_monitors) your application's responses using scorers and LLM judges
* [Log versions](/weave/tutorial-weave_models) of your application's code, prompts, datasets, and other attributes
* [Create leaderboards](/weave/guides/core-types/leaderboards) to track and compare your application's performance over time
* [Integrate Weave into your W&B reinforcement-learning training runs](/weave/guides/tools/weave-in-workspaces) to gain observability into how your models perform during training

Weave works with many [popular frameworks](/weave/guides/integrations) and has both [Python](/weave/reference/python-sdk) and [TypeScript SDKs](/weave/reference/typescript-sdk).

## Get Started

See the following quickstart docs to install and learn how integrate Weave into your code:

* [Track LLM inputs and outputs](/weave/quickstart)
* [Learn Weave with W&B inference](/weave/quickstart-inference)

## Advanced guides

Explore advanced topics:

- **[Integrations](/weave/guides/integrations/)**: Connect Weave with popular language model providers, such as OpenAI and Anthropic.
- **[Cookbooks](/weave/cookbooks/Intro_to_Weave_Hello_Trace)**: See examples of how to use Weave in our interactive notebooks.
- **[W&B AI Academy](https://wandb.ai/site/courses)**: Build advanced retrieval systems, improve language model prompting, and fine-tune models.