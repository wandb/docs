---
title: "W&B Weave"
description: "Track, test, and improve language model apps with W&B Weave"
mode: wide
---

W&B Weave is an observability and evaluation platform that helps you track, evaluate, and improve your LLM application. With Weave, you can:

* [Observe and debug](/weave/quickstart) your LLM application
* [Evaluate](/weave/tutorial-eval) your applicationâ€™s responses using LLM judges and custom scorers


## Get Started

The following docs guide you through the basics of how to use Weave's suite of tools.

<CardGroup cols={3}>
  <Card
    title="Quickstart: Track LLM inputs & outputs"
    icon="chart-line"
    href="/weave/quickstart"
  >
    Start by tracing a basic call to an LLM and reviewing the data in your W&B account.
  </Card>

  <Card
    title="Get started evaluating your app"
    icon="clipboard-check"
    href="/weave/tutorial-eval"
  >
    Learn how to build an evaluation pipeline using Weave scorers to test and track your application's performance.
  </Card>

  <Card
    title="Evaluate a RAG application"
    icon="search"
    href="/weave/tutorial-rag"
  >
    Build and evaluate RAG applications using Weave with LLM judges to measure retrieval quality.
  </Card>
</CardGroup>

## Install Weave

W&B Weave provides Python and TypeScript libraries. To install the Weave library, run the following command:

<Tabs>
  <Tab title="Python">
    ```bash
    pip install weave
    ```
  </Tab>
  <Tab title="TypeScript">
    ```bash
    pnpm install weave
    ```
  </Tab>
</Tabs>

To start using the Weave library, create a [Weights & Biases (W&B) account](https://wandb.ai) and an [API key at User Settings](https://wandb.ai/settings). The API key allows you to authenticate to your W&B account and start sending data to it.
