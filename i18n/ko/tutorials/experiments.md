
# ì‹¤í—˜ ì¶”ì í•˜ê¸°


[**Colab ë…¸íŠ¸ë¶ì—ì„œ ì‹œë„í•´ ë³´ì„¸ìš” â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_&_Biases.ipynb)

ë¹ ë¥¸ ì‹¤í—˜ì€ ê¸°ê³„í•™ìŠµì— ìˆì–´ ê·¼ë³¸ì ì…ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” W&Bë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ì„ ì¶”ì í•˜ê³  ì‹œê°í™”í•˜ì—¬ ë¹ ë¥´ê²Œ ë°˜ë³µí•˜ê³  ê²°ê³¼ë¥¼ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¤© ì‹¤í—˜ì„ ìœ„í•œ ê³µìœ  ëŒ€ì‹œë³´ë“œ

ë‹¨ ëª‡ ì¤„ì˜ ì½”ë“œë¡œ,
ë‹¹ì‹ ì€ í’ë¶€í•˜ê³  ì¸í„°ë™í‹°ë¸Œí•˜ë©° ê³µìœ  ê°€ëŠ¥í•œ ëŒ€ì‹œë³´ë“œë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ [ì—¬ê¸°ì—ì„œ ì§ì ‘ í™•ì¸í•˜ì„¸ìš”](https://wandb.ai/wandb/wandb_example).
![](https://i.imgur.com/Pell4Oo.png)

## ğŸ”’ ë°ì´í„° & ê°œì¸ì •ë³´ ë³´í˜¸

ìš°ë¦¬ëŠ” ë³´ì•ˆì„ ë§¤ìš° ì‹¬ê°í•˜ê²Œ ìƒê°í•˜ë©°, í´ë¼ìš°ë“œ í˜¸ìŠ¤íŒ… ëŒ€ì‹œë³´ë“œëŠ” ì—…ê³„ í‘œì¤€ ëª¨ë²” ì‚¬ë¡€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•”í˜¸í™”í•©ë‹ˆë‹¤. ë§Œì•½ ì—¬ëŸ¬ë¶„ì´ ê¸°ì—… í´ëŸ¬ìŠ¤í„°ë¥¼ ë– ë‚  ìˆ˜ ì—†ëŠ” ë°ì´í„°ì…‹ì„ ë‹¤ë£¨ê³  ìˆë‹¤ë©´, ìš°ë¦¬ëŠ” [ì˜¨í”„ë ˆë¯¸ìŠ¤](https://docs.wandb.com/self-hosted) ì„¤ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ë˜í•œ ëª¨ë“  ë°ì´í„°ë¥¼ ì‰½ê²Œ ë‹¤ìš´ë¡œë“œí•˜ê³  ë‹¤ë¥¸ íˆ´ë¡œ ë‚´ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ â€” ì˜ˆë¥¼ ë“¤ì–´, Jupyter ë…¸íŠ¸ë¶ì—ì„œì˜ ì‚¬ìš©ì ì§€ì • ë¶„ì„ê³¼ ê°™ì€. ì—¬ê¸°ì—ì„œ [ìš°ë¦¬ì˜ APIì— ëŒ€í•´ ë” ì•Œì•„ë³´ì„¸ìš”](https://docs.wandb.com/library/api).

---

## ğŸª„ `wandb` ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ë¡œê·¸ì¸


ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  ë¬´ë£Œ ê³„ì •ì— ë¡œê·¸ì¸í•˜ê¸°ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.




```python
!pip install wandb -qU
```


```python
# W&B ê³„ì •ì— ë¡œê·¸ì¸
import wandb
wandb.login()
```

## ğŸ‘Ÿ ì‹¤í—˜ ì‹¤í–‰í•˜ê¸°
1ï¸âƒ£. **ìƒˆ runì„ ì‹œì‘**í•˜ê³  ì¶”ì í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì „ë‹¬í•˜ê¸°

2ï¸âƒ£. **íŠ¸ë ˆì´ë‹ ë˜ëŠ” í‰ê°€ì—ì„œ ë©”íŠ¸ë¦­ ë¡œê·¸í•˜ê¸°**

3ï¸âƒ£. **ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ ì‹œê°í™”í•˜ê¸°**


```python
import random

# 5ê°œì˜ ì‹œë®¬ë ˆì´ì…˜ëœ ì‹¤í—˜ ì‹¤í–‰
total_runs = 5
for run in range(total_runs):
  # ğŸ 1ï¸âƒ£ ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¶”ì í•  ìƒˆ run ì‹œì‘í•˜ê¸°
  wandb.init(
      # ì´ runì´ ê¸°ë¡ë  í”„ë¡œì íŠ¸ ì„¤ì •
      project="basic-intro", 
      # run ì´ë¦„ì„ ì „ë‹¬í•©ë‹ˆë‹¤ (ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë¬´ì‘ìœ„ë¡œ í• ë‹¹ë©ë‹ˆë‹¤, ì˜ˆ: sunshine-lollypop-10)
      name=f"experiment_{run}", 
      # í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ run ë©”íƒ€ë°ì´í„° ì¶”ì 
      config={
      "learning_rate": 0.02,
      "architecture": "CNN",
      "dataset": "CIFAR-100",
      "epochs": 10,
      })
  
  # ì´ ê°„ë‹¨í•œ ë¸”ë¡ì€ ë©”íŠ¸ë¦­ ë¡œê¹…ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” íŠ¸ë ˆì´ë‹ ë£¨í”„ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤
  epochs = 10
  offset = random.random() / 5
  for epoch in range(2, epochs):
      acc = 1 - 2 ** -epoch - random.random() / epoch - offset
      loss = 2 ** -epoch + random.random() / epoch + offset
      
      # ğŸ 2ï¸âƒ£ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ W&Bë¡œ ë©”íŠ¸ë¦­ ë¡œê·¸í•˜ê¸°
      wandb.log({"acc": acc, "loss": loss})
      
  # runì„ ì™„ë£Œë¡œ í‘œì‹œí•˜ê¸°
  wandb.finish()
```

3ï¸âƒ£ ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ìœ„ì˜ ğŸ‘† wandb ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ ëŒ€í™”í˜• ëŒ€ì‹œë³´ë“œë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ğŸ”¥ ê°„ë‹¨í•œ Pytorch ì‹ ê²½ë§

ğŸ’ª ì´ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ ê°„ë‹¨í•œ MNIST ë¶„ë¥˜ê¸°ë¥¼ íŠ¸ë ˆì´ë‹í•˜ê³ , í”„ë¡œì íŠ¸ í˜ì´ì§€ ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ W&B í”„ë¡œì íŠ¸ì— ê²°ê³¼ê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°ë˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì„¸ìš”.


`wandb`ì—ì„œ ì‹¤í–‰í•˜ëŠ” ëª¨ë“  runì€ ìë™ìœ¼ë¡œ [ë©”íŠ¸ë¦­](https://docs.wandb.ai/ref/app/pages/run-page#charts-tab),
[ì‹œìŠ¤í…œ ì •ë³´](https://docs.wandb.ai/ref/app/pages/run-page#system-tab),
[í•˜ì´í¼íŒŒë¼ë¯¸í„°](https://docs.wandb.ai/ref/app/pages/run-page#overview-tab),
[í„°ë¯¸ë„ ì¶œë ¥](https://docs.wandb.ai/ref/app/pages/run-page#logs-tab)ì„ ë¡œê·¸í•˜ë©°,
ëª¨ë¸ ì…ë ¥ ë° ì¶œë ¥ê³¼ í•¨ê»˜ [ì¸í„°ë™í‹°ë¸Œ í…Œì´ë¸”](https://docs.wandb.ai/guides/tables)ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Dataloader ì„¤ì •í•˜ê¸°

ì´ ì˜ˆì œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ PyTorchë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. Google Colabì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì´ë¯¸ ì‚¬ì „ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 


```python
!pip install torch torchvision
```


```python
import wandb
import math
import random
import torch, torchvision
import torch.nn as nn
import torchvision.transforms as T

device = "cuda:0" if torch.cuda.is_available() else "cpu"

def get_dataloader(is_train, batch_size, slice=5):
    "íŠ¸ë ˆì´ë‹ì„ ìœ„í•œ dataloader ê°€ì ¸ì˜¤ê¸°"
    full_dataset = torchvision.datasets.MNIST(root=".", train=is_train, transform=T.ToTensor(), download=True)
    sub_dataset = torch.utils.data.Subset(full_dataset, indices=range(0, len(full_dataset), slice))
    loader = torch.utils.data.DataLoader(dataset=sub_dataset, 
                                         batch_size=batch_size, 
                                         shuffle=True if is_train else False, 
                                         pin_memory=True, num_workers=2)
    return loader

def get_model(dropout):
    "ê°„ë‹¨í•œ ëª¨ë¸"
    model = nn.Sequential(nn.Flatten(),
                         nn.Linear(28*28, 256),
                         nn.BatchNorm1d(256),
                         nn.ReLU(),
                         nn.Dropout(dropout),
                         nn.Linear(256,10)).to(device)
    return model

def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):
    "ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê³„ì‚°í•˜ê³  wandb.Tableë¡œ ë¡œê·¸í•˜ê¸°"
    model.eval()
    val_loss = 0.
    with torch.inference_mode():
        correct = 0
        for i, (images, labels) in enumerate(valid_dl):
            images, labels = images.to(device), labels.to(device)

            # Forward íŒ¨ìŠ¤ â¡
            outputs = model(images)
            val_loss += loss_func(outputs, labels)*labels.size(0)

            # ì •í™•ë„ ê³„ì‚° ë° ëˆ„ì 
            _, predicted = torch.max(outputs.data, 1)
            correct += (predicted == labels).sum().item()

            # ëŒ€ì‹œë³´ë“œì— í•œ ë°°ì¹˜ì˜ ì´ë¯¸ì§€ ë¡œê·¸í•˜ê¸°, í•­ìƒ ê°™ì€ batch_idx.
            if i==batch_idx && log_images:
                log_image_table(images, predicted, labels, outputs.softmax(dim=1))
    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)

def log_image_table(images, predicted, labels, probs):
    "wandb.Tableë¡œ (img, pred, target, scores) ë¡œê·¸í•˜ê¸°"
    # ğŸ ì´ë¯¸ì§€, ë¼ë²¨, ì˜ˆì¸¡ì„ ë¡œê·¸í•˜ê¸° ìœ„í•œ wandb Table ìƒì„±í•˜ê¸°
    table = wandb.Table(columns=["image", "pred", "target"]+[f"score_{i}" for i in range(10)])
    for img, pred, targ, prob in zip(images.to("cpu"), predicted.to("cpu"), labels.to("cpu"), probs.to("cpu")):
        table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())
    wandb.log({"predictions_table":table}, commit=False)
```

## ëª¨ë¸ íŠ¸ë ˆì´ë‹í•˜ê¸°


```python
# ë‹¤ì–‘í•œ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ì„ ì‹œë„í•˜ë©° 5ê°œì˜ ì‹¤í—˜ ì‹¤í–‰
for _ in range(5):
    # ğŸ wandb run ì´ˆê¸°í™”í•˜ê¸°
    wandb.init(
        project="pytorch-intro",
        config={
            "epochs": 10,
            "batch_size": 128,
            "lr": 1e-3,
            "dropout": random.uniform(0.01, 0.80),
            })
    
    # ì„¤ì • ë³µì‚¬í•˜ê¸°
    config = wandb.config

    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    train_dl = get_dataloader(is_train=True, batch_size=config.batch_size)
    valid_dl = get_dataloader(is_train=False, batch_size=2*config.batch_size)
    n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)
    
    # ê°„ë‹¨í•œ MLP ëª¨ë¸
    model = get_model(config.dropout)

    # ì†ì‹¤ê³¼ ì˜µí‹°ë§ˆì´ì € ë§Œë“¤ê¸°
    loss_func = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)

   # íŠ¸ë ˆì´ë‹
    example_ct = 0
    step_ct = 0
    for epoch in range(config.epochs):
        model.train()
        for step, (images, labels) in enumerate(train_dl):
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            train_loss = loss_func(outputs, labels)
            optimizer.zero_grad()
            train_loss.backward()
            optimizer.step()
            
            example_ct += len(images)
            metrics = {"train/train_loss": train_loss, 
                       "train/epoch": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, 
                       "train/example_ct": example_ct}
            
            if step + 1 < n_steps_per_epoch:
                # ğŸ íŠ¸ë ˆì´ë‹ ë©”íŠ¸ë¦­ì„ wandbì— ë¡œê·¸í•˜ê¸°
                wandb.log(metrics)
                
            step_ct += 1

        val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=(epoch==(config.epochs-1)))

        # ğŸ íŠ¸ë ˆì´ë‹ ë° ê²€ì¦ ë©”íŠ¸ë¦­ì„ wandbì— ë¡œê·¸í•˜ê¸°
        val_metrics = {"val/val_loss": val_loss, 
                       "val/val_accuracy": accuracy}
        wandb.log({**metrics, **val_metrics})
        
        print(f"Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, Accuracy: {accuracy:.2f}")

    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ê°€ ìˆì—ˆë‹¤ë©´, ì´ë ‡ê²Œ ìš”ì•½ ë©”íŠ¸ë¦­ìœ¼ë¡œ ë¡œê·¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    wandb.summary['test_accuracy'] = 0.8

    # ğŸ wandb runì„ ë§ˆë¬´ë¦¬í•˜ê¸°
    wandb.finish()
```

ì´ì œ wandbë¥¼ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í–ˆìŠµë‹ˆë‹¤! ğŸ‘† ìœ„ì˜ wandb ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ ë©”íŠ¸ë¦­ì„ í™•ì¸í•˜ì„¸ìš”.

# ğŸ”” W&B ì•Œë¦¼ ì‹œë„í•˜ê¸°

**[W&B ì•Œë¦¼](https://docs.wandb.ai/guides/track/alert)** ì€ Python ì½”ë“œì—ì„œ íŠ¸ë¦¬ê±°ëœ ì•Œë¦¼ì„ Slackì´ë‚˜ ì´ë©”ì¼ë¡œ ë³´ë‚¼ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ì½”ë“œì—ì„œ Slackì´ë‚˜ ì´ë©”ì¼ ì•Œë¦¼ì„ ë³´ë‚´ê³  ì‹¶ì€ ì²« ë²ˆì§¸ ì‹œë„ì— ë”°ë¼ì•¼ í•  2ë‹¨ê³„ê°€ ìˆìŠµë‹ˆë‹¤:

1) W&B [ì‚¬ìš©ì ì„¤ì •](https://wandb.ai/settings)ì—ì„œ ì•Œë¦¼ì„ ì¼œê¸°

2) ì½”ë“œì— `wandb.alert()` ì¶”ê°€í•˜ê¸°:

```python
wandb.alert(
    title="ì •í™•ë„ ë‚®ìŒ", 
    text=f"ì •í™•ë„ê°€ í—ˆìš© ê°€ëŠ¥í•œ ì„ê³„ê°’ ì•„ë˜ì…ë‹ˆë‹¤"
)
```

**[W&B ì•Œë¦¼](https://docs.wandb.ai/guides/track/alert)** ì— ëŒ€í•œ ì „ì²´ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ì•„ë˜ì˜ ìµœì†Œ ì˜ˆì œë¥¼ í™•ì¸í•˜ì—¬ `wandb.alert` ì‚¬ìš© ë°©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.


```python
# wandb run ì‹œì‘í•˜ê¸°
wandb.init(project="pytorch-intro")

# ëª¨ë¸ íŠ¸ë ˆì´ë‹ ë£¨í”„ ì‹œë®¬ë ˆì´ì…˜
acc_threshold = 0.3
for training_step in range(1000):

    # ì •í™•ë„ë¥¼ ìœ„í•œ ë¬´ì‘ìœ„ ìˆ«ì ìƒì„±
    accuracy = round(random.random() + random.random(), 3)
    print(f'ì •í™•ë„ëŠ”: {accuracy}, {acc_threshold}')
    
    # ğŸ wandbì— ì •í™•ë„ ë¡œê·¸í•˜ê¸°
    wandb.log({"Accuracy": accuracy})

    # ğŸ”” ì •í™•ë„ê°€ ì„ê³„ê°’ ì´í•˜ì¸ ê²½ìš°, W&B ì•Œë¦¼ì„ ë°œìƒì‹œí‚¤ê³  runì„ ë©ˆì¶”ê¸°
    if accuracy <= acc_threshold:
        # ğŸ wandb ì•Œë¦¼ ë³´ë‚´ê¸°
        wandb.alert(
            title='ì •í™•ë„ ë‚®ìŒ',
            text=f'{training_step} ë‹¨ê³„ì—ì„œ ì •í™•ë„ {accuracy}ê°€ í—ˆìš© ê°€ëŠ¥í•œ ì„ê³„ê°’, {acc_threshold} ì•„ë˜ì…ë‹ˆë‹¤',
        )
        print('ì•Œë¦¼ì´ íŠ¸ë¦¬ê±°ë˜ì—ˆìŠµë‹ˆë‹¤')
        break

# runì´ ì™„ë£Œë˜ì—ˆë‹¤ê³  í‘œì‹œí•˜ê¸° (Jupyter ë…¸íŠ¸ë¶ì—ì„œ ìœ ìš©)
wandb.finish()
```

# ë‹¤ìŒì€ ë¬´ì—‡ì¸ê°€ìš”?
ë‹¤ìŒ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” W&B í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì˜ˆì¸¡ì„ ë³´ê³  ë¶„ì„í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤:

## ğŸ‘‰ [ëª¨ë¸ ì˜ˆì¸¡ ë³´ê¸° & ë¶„ì„í•˜ê¸°](tables)