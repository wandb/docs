
# PyTorch
[Weights & Biases](https://wandb.com)λ¥Ό μ‚¬μ©ν•μ—¬ λ¨Έμ‹  λ¬λ‹ μ‹¤ν— μ¶”μ , λ°μ΄ν„°μ„ΈνΈ λ²„μ „ κ΄€λ¦¬ λ° ν”„λ΅μ νΈ ν‘μ—…μ„ μν–‰ν•μ„Έμ”.

[**Colab λ…ΈνΈλ¶μ—μ„ μ‹λ„ν•΄ λ³΄μ„Έμ” β†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb)

<div><img /></div>

<img src="https://wandb.me/mini-diagram" width="650" alt="Weights & Biases" />

<div><img /></div>

## μ΄ λ…ΈνΈλ¶μ΄ λ‹¤λ£¨λ” λ‚΄μ©:

PyTorch μ½”λ“μ™€ Weights & Biasesλ¥Ό ν†µν•©ν•μ—¬ νμ΄ν”„λΌμΈμ— μ‹¤ν— μ¶”μ μ„ μ¶”κ°€ν•λ” λ°©λ²•μ„ λ³΄μ—¬μ¤λ‹λ‹¤.

## κ²°κ³Όμ μΌλ΅ λ‚μ¨ μΈν„°λ™ν‹°λΈν• W&B λ€μ‹λ³΄λ“λ” λ‹¤μκ³Ό κ°™μµλ‹λ‹¤:
![](https://i.imgur.com/z8TK2Et.png)

## μμ‚¬μ½”λ“μ—μ„ μ°λ¦¬κ°€ ν•  μΌμ€:
```python
# λΌμ΄λΈλ¬λ¦¬λ¥Ό importν•©λ‹λ‹¤
import wandb

# μƒ μ‹¤ν—μ„ μ‹μ‘ν•©λ‹λ‹¤
wandb.init(project="new-sota-model")

# configλ΅ ν•μ΄νΌνλΌλ―Έν„°μ μ‚¬μ „μ„ μΊ΅μ²ν•©λ‹λ‹¤
wandb.config = {"learning_rate": 0.001, "epochs": 100, "batch_size": 128}

# λ¨λΈκ³Ό λ°μ΄ν„°λ¥Ό μ„¤μ •ν•©λ‹λ‹¤
model, dataloader = get_model(), get_data()

# μ„ νƒμ‚¬ν•­: κ·Έλ μ΄λ””μ–ΈνΈλ¥Ό μ¶”μ ν•©λ‹λ‹¤
wandb.watch(model)

for batch in dataloader:
  metrics = model.training_step()
  # ν•™μµ λ£¨ν”„ λ‚΄μ—μ„ λ©”νΈλ¦­μ„ λ΅κ·Έν•μ—¬ λ¨λΈ μ„±λ¥μ„ μ‹κ°ν™”ν•©λ‹λ‹¤
  wandb.log(metrics)

# μ„ νƒμ‚¬ν•­: λ§μ§€λ§‰μ— λ¨λΈμ„ μ €μ¥ν•©λ‹λ‹¤
model.to_onnx()
wandb.save("model.onnx")
```

## [λΉ„λ””μ¤ νν† λ¦¬μ–Όμ„ λ”°λΌν•μ„Έμ”](http://wandb.me/pytorch-video)!
**μ°Έκ³ **: _Step_μΌλ΅ μ‹μ‘ν•λ” μ„Ήμ…λ“¤μ€ κΈ°μ΅΄ νμ΄ν”„λΌμΈμ— W&Bλ¥Ό ν†µν•©ν•λ” λ° ν•„μ”ν• λ¨λ“  κ²ƒμ…λ‹λ‹¤. λ‚λ¨Έμ§€λ” λ°μ΄ν„°λ¥Ό λ΅λ“ν•κ³  λ¨λΈμ„ μ •μν•λ” κ²ƒμ…λ‹λ‹¤.

# π€ μ„¤μΉ, Import, κ·Έλ¦¬κ³  λ΅κ·ΈμΈ


```python
import os
import random

import numpy as np
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from tqdm.auto import tqdm

# κ²°μ •μ μΈ λ™μ‘μ„ λ³΄μ¥ν•©λ‹λ‹¤
torch.backends.cudnn.deterministic = True
random.seed(hash("setting random seeds") % 2**32 - 1)
np.random.seed(hash("improves reproducibility") % 2**32 - 1)
torch.manual_seed(hash("by removing stochasticity") % 2**32 - 1)
torch.cuda.manual_seed_all(hash("so runs are repeatable") % 2**32 - 1)

# μ¥μΉ κµ¬μ„±
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# MNIST λ―Έλ¬ λ©λ΅μ—μ„ λλ¦° λ―Έλ¬λ¥Ό μ κ±°ν•©λ‹λ‹¤
torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors
                                      if not mirror.startswith("http://yann.lecun.com")]
```

### 0οΈβƒ£ λ‹¨κ³„ 0: W&B μ„¤μΉ

μ‹μ‘ν•λ ¤λ©΄ λΌμ΄λΈλ¬λ¦¬λ¥Ό λ°›μ•„μ•Ό ν•©λ‹λ‹¤.
`wandb`λ” `pip`μ„ μ‚¬μ©ν•μ—¬ μ‰½κ² μ„¤μΉν•  μ μμµλ‹λ‹¤.


```python
!pip install wandb onnx -Uq
```

### 1οΈβƒ£ λ‹¨κ³„ 1: W&B Import λ° λ΅κ·ΈμΈ

μ°λ¦¬μ μ›Ή μ„λΉ„μ¤μ— λ°μ΄ν„°λ¥Ό λ΅κ·Έν•κΈ° μ„ν•΄μ„λ”,
λ΅κ·ΈμΈν•΄μ•Ό ν•©λ‹λ‹¤.

W&Bλ¥Ό μ²μ μ‚¬μ©ν•λ” κ²½μ°,
λ“±μ¥ν•λ” λ§ν¬μ—μ„ λ¬΄λ£ κ³„μ •μ— κ°€μ…ν•΄μ•Ό ν•©λ‹λ‹¤.


```
import wandb

wandb.login()
```

# π‘©β€π”¬ μ‹¤ν— λ° νμ΄ν”„λΌμΈ μ •μ

## 2οΈβƒ£ λ‹¨κ³„ 2: `wandb.init`μΌλ΅ λ©”νƒ€λ°μ΄ν„° λ° ν•μ΄νΌνλΌλ―Έν„° μ¶”μ 

ν”„λ΅κ·Έλλ°μ μΌλ΅, μ°λ¦¬κ°€ ν•λ” μ²« λ²μ§Έ μΌμ€ μ°λ¦¬μ μ‹¤ν—μ„ μ •μν•λ” κ²ƒμ…λ‹λ‹¤:
ν•μ΄νΌνλΌλ―Έν„°λ” λ¬΄μ—‡μΈκ°€μ”? μ΄ μ‹¤ν–‰κ³Ό κ΄€λ ¨λ λ©”νƒ€λ°μ΄ν„°λ” λ¬΄μ—‡μΈκ°€μ”?

μ΄ μ •λ³΄λ¥Ό `config` μ‚¬μ „μ— μ €μ¥ν•κ³  ν•„μ”ν•  λ•λ§λ‹¤ μ•΅μ„Έμ¤ν•λ” κ²ƒμ΄ μΌλ°μ μΈ μ›ν¬ν”λ΅μ…λ‹λ‹¤.

μ΄ μμ μ—μ„λ” λ‡ κ°€μ§€ ν•μ΄νΌνλΌλ―Έν„°λ§ λ³€κ²½ν•κ³  λ‚λ¨Έμ§€λ” ν•λ“μ½”λ”©ν•©λ‹λ‹¤.
ν•μ§€λ§ λ¨λΈμ μ–΄λ–¤ λ¶€λ¶„λ„ `config`μ μΌλ¶€κ°€ λ  μ μμµλ‹λ‹¤!

μ°λ¦¬λ” λν• λ‡ κ°€μ§€ λ©”νƒ€λ°μ΄ν„°λ¥Ό ν¬ν•¨ν•©λ‹λ‹¤: μ°λ¦¬λ” MNIST λ°μ΄ν„°μ„ΈνΈμ™€ μ»¨λ³Όλ£¨μ…
μ•„ν‚¤ν…μ²λ¥Ό μ‚¬μ©ν•©λ‹λ‹¤. λ‚μ¤‘μ— κ°™μ€ ν”„λ΅μ νΈμ—μ„ CIFARμ—μ„ μ™„μ „ μ—°κ²° μ•„ν‚¤ν…μ²λ΅ μ‘μ—…ν•λ‹¤λ©΄,
μ΄κ²ƒμ€ μ°λ¦¬κ°€ μ‹¤ν–‰μ„ λ¶„λ¦¬ν•λ” λ° λ„μ›€μ΄ λ  κ²ƒμ…λ‹λ‹¤.


```python
config = dict(
    epochs=5,
    classes=10,
    kernels=[16, 32],
    batch_size=128,
    learning_rate=0.005,
    dataset="MNIST",
    architecture="CNN")
```

μ΄μ  μ „μ²΄ νμ΄ν”„λΌμΈμ„ μ •μν•κ² μµλ‹λ‹¤,
μ΄λ” λ¨λΈ ν•™μµμ— κ½¤ μ „ν•μ μ…λ‹λ‹¤:

1. μ°λ¦¬λ” λ¨Όμ € λ¨λΈμ„ `λ§λ“¤κ³ `, κ΄€λ ¨ λ°μ΄ν„°μ™€ μµν‹°λ§μ΄μ €λ„ λ§λ“¤κ³ ,
2. κ·Έμ— λ”°λΌ λ¨λΈμ„ `ν•™μµ`ν•κ³  λ§μ§€λ§‰μΌλ΅
3. ν•™μµμ΄ μ–΄λ–»κ² μ§„ν–‰λμ—λ”μ§€ λ³΄κΈ° μ„ν•΄ `ν…μ¤νΈ`ν•©λ‹λ‹¤.

μ•„λμ—μ„ μ΄ ν•¨μλ“¤μ„ κµ¬ν„ν•κ² μµλ‹λ‹¤.


```python
def model_pipeline(hyperparameters):

    # wandbλ¥Ό μ‹μ‘ν•λΌκ³  μ•λ¦½λ‹λ‹¤
    with wandb.init(project="pytorch-demo", config=hyperparameters):
      # λ¨λ“  HPλ¥Ό wandb.configλ¥Ό ν†µν•΄ μ•΅μ„Έμ¤ν•λ―€λ΅ λ΅κΉ…μ΄ μ‹¤ν–‰κ³Ό μΌμΉν•©λ‹λ‹¤!
      config = wandb.config

      # λ¨λΈ, λ°μ΄ν„° λ° μµμ ν™” λ¬Έμ λ¥Ό λ§λ“­λ‹λ‹¤
      model, train_loader, test_loader, criterion, optimizer = make(config)
      print(model)

      # μ΄λ¥Ό μ‚¬μ©ν•μ—¬ λ¨λΈμ„ ν•™μµν•©λ‹λ‹¤
      train(model, train_loader, criterion, optimizer, config)

      # μµμΆ… μ„±λ¥μ„ ν…μ¤νΈν•©λ‹λ‹¤
      test(model, test_loader)

    return model
```

μ—¬κΈ°μ—μ„ ν‘μ¤€ νμ΄ν”„λΌμΈκ³Όμ μ μΌν• μ°¨μ΄μ μ€
λ¨λ‘ `wandb.init`μ μ»¨ν…μ¤νΈ λ‚΄μ—μ„ λ°μƒν•λ‹¤λ” κ²ƒμ…λ‹λ‹¤.
μ΄ ν•¨μλ¥Ό νΈμ¶ν•λ©΄ μ½”λ“μ™€ μ„λ²„ κ°„μ ν†µμ‹  λΌμΈμ΄ μ„¤μ •λ©λ‹λ‹¤.

`config` μ‚¬μ „μ„ `wandb.init`μ— μ „λ‹¬ν•λ©΄
κ·Έ μ •λ³΄κ°€ μ¦‰μ‹ μ°λ¦¬μ—κ² λ΅κ·Έλλ―€λ΅,
μ‹¤ν—μ— μ‚¬μ©ν• ν•μ΄νΌνλΌλ―Έν„° κ°’μ΄ ν•­μƒ λ¬΄μ—‡μΈμ§€ μ• μ μμµλ‹λ‹¤.

λ¨λΈμ—μ„ μ„ νƒν•κ³  λ΅κ·Έν• κ°’μ΄ ν•­μƒ μ‚¬μ©λλ” κ°’μ΄ λλ„λ΅ ν•κΈ° μ„ν•΄,
`wandb.config` μ‚¬λ³Έμ„ μ‚¬μ©ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤.
μ•„λ `make`μ μ •μλ¥Ό ν™•μΈν•λ©΄ λ‡ κ°€μ§€ μμ‹λ¥Ό λ³Ό μ μμµλ‹λ‹¤.

> *μ‚¬μ΄λ“ λ…ΈνΈ*: μ°λ¦¬λ” μ½”λ“κ°€ λ³„λ„μ ν”„λ΅μ„Έμ¤μ—μ„ μ‹¤ν–‰λλ„λ΅ μ£Όμλ¥Ό κΈ°μΈμ…λ‹λ‹¤,
κ·Έλμ„ μ°λ¦¬ μ½μ— λ¬Έμ κ°€ μμ–΄λ„
(μ: κ±°λ€ν• ν•΄μ–‘ λ¬μ¤ν„°κ°€ μ°λ¦¬ λ°μ΄ν„° μ„Όν„°λ¥Ό κ³µκ²©)
λ‹Ήμ‹ μ μ½”λ“κ°€ μ¶©λν•μ§€ μ•μµλ‹λ‹¤.
λ¬Έμ κ°€ ν•΄κ²°λλ©΄ (μ: ν¬λΌμΌ„μ΄ κΉμ€ λ°”λ‹¤λ΅ λμ•„κ°)
`wandb sync`λ΅ λ°μ΄ν„°λ¥Ό λ΅κ·Έν•  μ μμµλ‹λ‹¤.


```python
def make(config):
    # λ°μ΄ν„°λ¥Ό λ§λ“­λ‹λ‹¤
    train, test = get_data(train=True), get_data(train=False)
    train_loader = make_loader(train, batch_size=config.batch_size)
    test_loader = make_loader(test, batch_size=config.batch_size)

    # λ¨λΈμ„ λ§λ“­λ‹λ‹¤
    model = ConvNet(config.kernels, config.classes).to(device)

    # μ†μ‹¤ λ° μµν‹°λ§μ΄μ €λ¥Ό λ§λ“­λ‹λ‹¤
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(
        model.parameters(), lr=config.learning_rate)
    
    return model, train_loader, test_loader, criterion, optimizer
```

# π“΅ λ°μ΄ν„° λ΅λ”© λ° λ¨λΈ μ •μ

μ΄μ  λ°μ΄ν„°κ°€ μ–΄λ–»κ² λ΅λ“λλ”μ§€ λ° λ¨λΈμ΄ μ–΄λ–¤ λ¨μµμΈμ§€λ¥Ό λ…μ‹ν•΄μ•Ό ν•©λ‹λ‹¤.

μ΄ λ¶€λ¶„μ€ λ§¤μ° μ¤‘μ”ν•μ§€λ§,
`wandb` μ—†μ΄λ„ λ‘κ°™μ„ κ²ƒμ΄λ―€λ΅,
μ°λ¦¬λ” μ΄μ— λ€ν•΄ ν¬κ² μ–ΈκΈ‰ν•μ§€ μ•κ² μµλ‹λ‹¤.


```python
def get_data(slice=5, train=True):
    μ „μ²΄ λ°μ΄ν„°μ„ΈνΈ = torchvision.datasets.MNIST(root=".",
                                              train=train, 
                                              transform=transforms.ToTensor(),
                                              download=True)
    # [::slice]λ΅ μ¬λΌμ΄μ‹±ν•λ” κ²ƒκ³Ό λ™λ“±ν•©λ‹λ‹¤
    μ„λΈ λ°μ΄ν„°μ„ΈνΈ = torch.utils.data.Subset(
      μ „μ²΄ λ°μ΄ν„°μ„ΈνΈ, indices=range(0, len(μ „μ²΄ λ°μ΄ν„°μ„ΈνΈ), slice))
    
    return μ„λΈ λ°μ΄ν„°μ„ΈνΈ


def make_loader(dataset, batch_size):
    loader = torch.utils.data.DataLoader(dataset=dataset,
                                         batch_size=batch_size, 
                                         shuffle=True,
                                         pin_memory=True, num_workers=2)
    return loader
```

λ¨λΈμ„ μ •μν•λ” κ²ƒμ€ λ³΄ν†µ μ¬λ―Έμλ” λ¶€λ¶„μ…λ‹λ‹¤!

ν•μ§€λ§ `wandb`μ™€ ν•¨κ»λΌλ„ λ³€ν•λ” κ²ƒμ€ μ—†μΌλ―€λ΅,
μ°λ¦¬λ” ν‘μ¤€ ConvNet μ•„ν‚¤ν…μ²λ¥Ό κ³ μν•  κ²ƒμ…λ‹λ‹¤.

μ΄κ²ƒμ„ μμ •ν•κ³  λ‡ κ°€μ§€ μ‹¤ν—μ„ ν•΄λ³΄μ§€ λ§μ‹­μ‹μ¤ --
λ¨λ“  κ²°κ³Όλ” [wandb.ai](https://wandb.ai)μ— λ΅κ·Έλ  κ²ƒμ…λ‹λ‹¤!




```python
# μ „ν†µμ μ΄κ³  μ»¨λ³Όλ£¨μ… μ‹ κ²½λ§

class ConvNet(nn.Module):
    def __init__(self, kernels, classes=10):
        super(ConvNet, self).__init__()
        
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(16, kernels[1], kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.fc = nn.Linear(7 * 7 * kernels[-1], classes)
        
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        return out
```

# π‘ ν•™μµ λ΅μ§ μ •μ

μ°λ¦¬μ `model_pipeline`μ—μ„ κ³„μ†ν•΄μ„ `ν•™μµ`μ„ μ–΄λ–»κ² ν•λ”μ§€ λ…μ‹ν•  μ°¨λ΅€μ…λ‹λ‹¤.

μ—¬κΈ°μ—λ” λ‘ κ°€μ§€ `wandb` ν•¨μκ°€ μ‚¬μ©λ©λ‹λ‹¤: `watch`μ™€ `log`.

### 3οΈβƒ£ λ‹¨κ³„ 3. `wandb.watch`λ΅ κ·Έλ μ΄λ””μ–ΈνΈλ¥Ό μ¶”μ ν•κ³  `wandb.log`λ΅ λ¨λ“  κ²ƒμ„ μ¶”μ ν•©λ‹λ‹¤

`wandb.watch`λ” ν•™μµμ κ° `log_freq` λ‹¨κ³„λ§λ‹¤ λ¨λΈμ κ·Έλ μ΄λ””μ–ΈνΈμ™€ νλΌλ―Έν„°λ¥Ό λ΅κ·Έν•©λ‹λ‹¤.

ν•™μµμ„ μ‹μ‘ν•κΈ° μ „μ— νΈμ¶ν•κΈ°λ§ ν•λ©΄ λ©λ‹λ‹¤.

λ‚λ¨Έμ§€ ν•™μµ μ½”λ“λ” λ™μΌν•κ² μ μ§€λ©λ‹λ‹¤:
μ—ν¬ν¬μ™€ λ°°μΉλ¥Ό λ°λ³µν•λ©°,
μλ°©ν–¥ λ° μ—­λ°©ν–¥ ν¨μ¤λ¥Ό μ‹¤ν–‰ν•κ³ 
`μµν‹°λ§μ΄μ €`λ¥Ό μ μ©ν•©λ‹λ‹¤.


```python
def train(model, loader, criterion, optimizer, config):
    # λ¨λΈμ΄ λ¬΄μ—‡μ„ ν•λ”μ§€ wandbμ— μ•λ¦½λ‹λ‹¤: κ·Έλ μ΄λ””μ–ΈνΈ, κ°€μ¤‘μΉ λ“±!
    wandb.watch(model, criterion, log="all", log_freq=10)

    # ν•™μµμ„ μ‹¤ν–‰ν•κ³  wandbλ΅ μ¶”μ ν•©λ‹λ‹¤
    total_batches = len(loader) * config.epochs
    example_ct = 0  # λ³Έ μμ‹μ μ
    batch_ct = 0
    for epoch in tqdm(range(config.epochs)):
        for _, (images, labels) in enumerate(loader):

            loss = train_batch(images, labels, model, optimizer, criterion)
            example_ct +=  len(images)
            batch_ct += 1

            # 25λ²μ§Έ λ°°μΉλ§λ‹¤ λ©”νΈλ¦­μ„ λ³΄κ³ ν•©λ‹λ‹¤
            if ((batch_ct + 1) % 25) == 0:
                train_log(loss, example_ct, epoch)


def train_batch(images, labels, model, optimizer, criterion):
    images, labels = images.to(device), labels.to(device)
    
    # μλ°©ν–¥ μ „λ‹¬ β΅
    outputs = model(images)
    loss = criterion(outputs, labels)
    
    # μ—­λ°©ν–¥ μ „λ‹¬ β¬…
    optimizer.zero_grad()
    loss.backward()

    # μµν‹°λ§μ΄μ €λ΅ μ¤ν…μ„ μ§„ν–‰ν•©λ‹λ‹¤
    optimizer.step()

    return loss
```

λ΅κ·Έ μ½”λ“μ—μ„μ μ μΌν• μ°¨μ΄μ μ€:
μ΄μ „μ—λ” ν„°λ―Έλ„μ— λ©”νΈλ¦­μ„ λ³΄κ³ ν–μ„ μλ„ μμ§€λ§,
μ΄μ λ” λ™μΌν• μ •λ³΄λ¥Ό `wandb.log`μ— μ „λ‹¬ν•©λ‹λ‹¤.

`wandb.log`λ” λ¬Έμμ—΄μ„ ν‚¤λ΅ κ°€μ§€λ” μ‚¬μ „μ„ κΈ°λ€ν•©λ‹λ‹¤.
μ΄ λ¬Έμμ—΄μ€ λ΅κ·Έλλ” κ°μ²΄λ¥Ό μ‹λ³„ν•λ©°, μ΄ κ°μ²΄λ“¤μ΄ κ°’μΌλ΅ κµ¬μ„±λ©λ‹λ‹¤.
λν• μ„ νƒμ μΌλ΅ ν•™μµμ μ–΄λ–¤ `λ‹¨κ³„`μ— μλ”μ§€ λ΅κ·Έν•  μλ„ μμµλ‹λ‹¤.

> *μ‚¬μ΄λ“ λ…ΈνΈ*: μ €λ” λ¨λΈμ΄ λ³Έ μμ‹μ μλ¥Ό μ‚¬μ©ν•λ” κ²ƒμ„ μ„ νΈν•©λ‹λ‹¤,
μ΄λ” λ°°μΉ ν¬κΈ°λ¥Ό κ±Έμ³ λ” μ‰½κ² λΉ„κµν•  μ μκΈ° λ•λ¬Έμ…λ‹λ‹¤,
ν•μ§€λ§ μ›μ‹ λ‹¨κ³„λ‚ λ°°μΉ μλ¥Ό μ‚¬μ©ν•  μλ„ μμµλ‹λ‹¤. λ” κΈ΄ ν•™μµ μ‹¤ν–‰μ κ²½μ°, `μ—ν¬ν¬`λ³„λ΅ λ΅κ·Έν•λ” κ²ƒλ„ μλ―Έκ°€ μμ„ μ μμµλ‹λ‹¤.


```python
def train_log(loss, example_ct, epoch):
    # λ§λ²•μ΄ μΌμ–΄λ‚λ” κ³³
    wandb.log({"epoch": epoch, "loss": loss}, step=example_ct)
    print(f"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}")
```

# π§ ν…μ¤νΈ λ΅μ§ μ •μ

λ¨λΈμ΄ ν•™μµμ„ λ§μΉ ν›„, μ°λ¦¬λ” κ·Έκ²ƒμ„ ν…μ¤νΈν•κ³  μ‹¶μµλ‹λ‹¤:
μƒμ‚°μ—μ„ μƒλ΅μ΄ λ°μ΄ν„°μ— λ€ν•΄ μ‹¤ν–‰ν•κ±°λ‚,
μΌλ¶€ μμ‘μ—…μΌλ΅ μ„ λ³„λ "μ–΄λ ¤μ΄ μμ‹"μ— μ μ©ν•΄ λ³΄μ„Έμ”.

#### 4οΈβƒ£ μ„ νƒμ  λ‹¨κ³„ 4: `wandb.save` νΈμ¶

μ΄κ²ƒμ€ λν• λ¨λΈμ μ•„ν‚¤ν…μ²μ™€ μµμΆ… νλΌλ―Έν„°λ¥Ό λ””μ¤ν¬μ— μ €μ¥ν•λ” μΆ‹μ€ μ‹κ°„μ…λ‹λ‹¤.
μµλ€ νΈν™μ„±μ„ μ„ν•΄, μ°λ¦¬λ” λ¨λΈμ„
[Open Neural Network Exchange (ONNX) ν¬λ§·](https://onnx.ai/)μΌλ΅ `λ‚΄λ³΄λƒ…λ‹λ‹¤`.

ν•΄λ‹Ή νμΌ μ΄λ¦„μ„ `wandb.save`μ— μ „λ‹¬ν•λ©΄ λ¨λΈ νλΌλ―Έν„°κ°€
W&B μ„λ²„μ— μ €μ¥λ©λ‹λ‹¤: μ–΄λ–¤ `.h5` λλ” `.pb`κ°€ μ–΄λ–¤ ν•™μµ μ‹¤ν–‰κ³Ό λ€μ‘ν•λ”μ§€ λ” μ΄μƒ μ¶”μ ν•μ§€ μ•μ•„λ„ λ©λ‹λ‹¤!

λ¨λΈμ„ μ €μ¥, λ²„μ „ κ΄€λ¦¬ λ° λ°°ν¬ν•κΈ° μ„ν• `wandb`μ λ” κ³ κΈ‰ κΈ°λ¥μ— λ€ν•΄μ„λ”,
[μ•„