
# TensorFlow

[**ì—¬ê¸°ì—ì„œ Colab ë…¸íŠ¸ë¶ìœ¼ë¡œ ì‹œë„í•´ ë³´ì„¸ìš” â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/tensorflow/Simple_TensorFlow_Integration.ipynb)

ê¸°ê³„í•™ìŠµ ì‹¤í—˜ ì¶”ì , ë°ì´í„°ì…‹ ë²„ì „ ê´€ë¦¬ ë° í”„ë¡œì íŠ¸ í˜‘ì—…ì„ ìœ„í•´ Weights & Biasesë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

<div><img /></div>

<img src="http://wandb.me/mini-diagram" width="650" alt="Weights & Biases" />

<div><img /></div>

## ì´ ë…¸íŠ¸ë¶ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©

* ì‹¤í—˜ ì¶”ì ì„ ìœ„í•œ TensorFlow íŒŒì´í”„ë¼ì¸ê³¼ Weights and Biasesì˜ ì‰¬ìš´ í†µí•©.
* `keras.metrics`ë¥¼ ì‚¬ìš©í•œ ë©”íŠ¸ë¦­ ê³„ì‚°
* ì‚¬ìš©ì ì •ì˜ íŠ¸ë ˆì´ë‹ ë£¨í”„ì—ì„œ `wandb.log`ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ë©”íŠ¸ë¦­ ë¡œê¹….

## ì¸í„°ë™í‹°ë¸Œí•œ W&B ëŒ€ì‹œë³´ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:


![ëŒ€ì‹œë³´ë“œ](/images/tutorials/tensorflow/dashboard.png)

**ì°¸ê³ **: _Step_ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì„¹ì…˜ì€ ê¸°ì¡´ ì½”ë“œì— W&Bë¥¼ í†µí•©í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ê²ƒì…ë‹ˆë‹¤. ë‚˜ë¨¸ì§€ëŠ” ë‹¨ì§€ í‘œì¤€ MNIST ì˜ˆì œì¼ ë¿ì…ë‹ˆë‹¤.




```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.datasets import cifar10

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

# ğŸš€ ì„¤ì¹˜, ê°€ì ¸ì˜¤ê¸°, ë¡œê·¸ì¸

## Step 0ï¸âƒ£: W&B ì„¤ì¹˜


```python
%%capture
!pip install wandb
```

## Step 1ï¸âƒ£: W&B ê°€ì ¸ì˜¤ê¸° ë° ë¡œê·¸ì¸


```python
import wandb
from wandb.keras import WandbCallback

wandb.login()
```

> ë¶€ê°€ ì„¤ëª…: W&Bë¥¼ ì²˜ìŒ ì‚¬ìš©í•˜ê±°ë‚˜ ë¡œê·¸ì¸í•˜ì§€ ì•Šì€ ê²½ìš°, `wandb.login()`ì„ ì‹¤í–‰í•œ í›„ ë‚˜íƒ€ë‚˜ëŠ” ë§í¬ê°€ ê°€ì…/ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤. ê°€ì…ì€ í•œ ë²ˆì˜ í´ë¦­ìœ¼ë¡œ ê°„ë‹¨í•©ë‹ˆë‹¤.

# ğŸ‘©â€ğŸ³ ë°ì´í„°ì…‹ ì¤€ë¹„


```python
# íŠ¸ë ˆì´ë‹ ë°ì´í„°ì…‹ ì¤€ë¹„
BATCH_SIZE = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

# tf.dataë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)

val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
val_dataset = val_dataset.batch(BATCH_SIZE)
```

# ğŸ§  ëª¨ë¸ ë° íŠ¸ë ˆì´ë‹ ë£¨í”„ ì •ì˜


```python
def make_model():
    inputs = keras.Input(shape=(784,), name="digits")
    x1 = keras.layers.Dense(64, activation="relu")(inputs)
    x2 = keras.layers.Dense(64, activation="relu")(x1)
    outputs = keras.layers.Dense(10, name="predictions")(x2)

    return keras.Model(inputs=inputs, outputs=outputs)
```


```python
def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)

    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))

    train_acc_metric.update_state(y, logits)

    return loss_value
```


```python
def test_step(x, y, model, loss_fn, val_acc_metric):
    val_logits = model(x, training=False)
    loss_value = loss_fn(y, val_logits)
    val_acc_metric.update_state(y, val_logits)

    return loss_value
```

## Step 2ï¸âƒ£: íŠ¸ë ˆì´ë‹ ë£¨í”„ì— `wandb.log` ì¶”ê°€


```python
def train(train_dataset, val_dataset,  model, optimizer,
          train_acc_metric, val_acc_metric,
          epochs=10,  log_step=200, val_log_step=50):
  
    for epoch in range(epochs):
        print("\nì—í¬í¬ %d ì‹œì‘" % (epoch,))

        train_loss = []   
        val_loss = []

        # ë°ì´í„°ì…‹ì˜ ë°°ì¹˜ë¥¼ ë°˜ë³µ ì²˜ë¦¬
        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
            loss_value = train_step(x_batch_train, y_batch_train, 
                                    model, optimizer, 
                                    loss_fn, train_acc_metric)
            train_loss.append(float(loss_value))

        # ê° ì—í¬í¬ì˜ ëì— ê²€ì¦ ë£¨í”„ ì‹¤í–‰
        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):
            val_loss_value = test_step(x_batch_val, y_batch_val, 
                                       model, loss_fn, 
                                       val_acc_metric)
            val_loss.append(float(val_loss_value))
            
        # ê° ì—í¬í¬ì˜ ëì— ë©”íŠ¸ë¦­ í‘œì‹œ
        train_acc = train_acc_metric.result()
        print("ì—í¬í¬ë³„ íŠ¸ë ˆì´ë‹ ì •í™•ë„: %.4f" % (float(train_acc),))

        val_acc = val_acc_metric.result()
        print("ê²€ì¦ ì •í™•ë„: %.4f" % (float(val_acc),))

        # ê° ì—í¬í¬ì˜ ëì— ë©”íŠ¸ë¦­ ì¬ì„¤ì •
        train_acc_metric.reset_states()
        val_acc_metric.reset_states()

        # â­: wandb.logë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”íŠ¸ë¦­ ë¡œê¹…
        wandb.log({'epochs': epoch,
                   'loss': np.mean(train_loss),
                   'acc': float(train_acc), 
                   'val_loss': np.mean(val_loss),
                   'val_acc':float(val_acc)})
```

# ğŸ‘Ÿ íŠ¸ë ˆì´ë‹ ì‹¤í–‰

## Step 3ï¸âƒ£: ì‹¤í–‰ì„ ì‹œì‘í•˜ê¸° ìœ„í•´ `wandb.init` í˜¸ì¶œ

ì‹¤í—˜ì„ ì‹œì‘í•œë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ì£¼ê¸° ìœ„í•´ ì´ë¥¼ í˜¸ì¶œí•˜ë©´, ê³ ìœ  IDì™€ ëŒ€ì‹œë³´ë“œê°€ ì œê³µë©ë‹ˆë‹¤.

[ì—¬ê¸°ì„œ ê³µì‹ ë¬¸ì„œ í™•ì¸ $\rightarrow$](https://docs.wandb.com/library/init)



```python
# í”„ë¡œì íŠ¸ ì´ë¦„ê³¼ ì„ íƒì ìœ¼ë¡œ ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬ wandb ì´ˆê¸°í™”.
# ì„¤ì • ê°’ë“¤ì„ ë³€ê²½í•´ë³´ê³  wandb ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.
config = {
              "learning_rate": 0.001,
              "epochs": 10,
              "batch_size": 64,
              "log_step": 200,
              "val_log_step": 50,
              "ì•„í‚¤í…ì²˜": "CNN",
              "ë°ì´í„°ì…‹": "CIFAR-10"
           }

run = wandb.init(project='my-tf-integration', config=config)
config = wandb.config

# ëª¨ë¸ ì´ˆê¸°í™”.
model = make_model()

# ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•˜ê¸° ìœ„í•œ ì˜µí‹°ë§ˆì´ì € ì¸ìŠ¤í„´ìŠ¤í™”.
optimizer = keras.optimizers.SGD(learning_rate=config.learning_rate)
# ì†ì‹¤ í•¨ìˆ˜ ì¸ìŠ¤í„´ìŠ¤í™”.
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# ë©”íŠ¸ë¦­ ì¤€ë¹„.
train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

train(train_dataset,
      val_dataset, 
      model,
      optimizer,
      train_acc_metric,
      val_acc_metric,
      epochs=config.epochs, 
      log_step=config.log_step, 
      val_log_step=config.val_log_step)

run.finish()  # Jupyter/Colabì—ì„œ, ì‘ì—…ì´ ëë‚¬ìŒì„ ì•Œë¦½ë‹ˆë‹¤!
```

# ğŸ‘€ ê²°ê³¼ ì‹œê°í™”

ìœ„ì˜ [**ì‹¤í–‰ í˜ì´ì§€**](https://docs.wandb.ai/ref/app/pages/run-page)
ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ ì‹¤ì‹œê°„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.

# ğŸ§¹ ìŠ¤ìœ• 101

í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìë™í™”í•˜ê³  ê°€ëŠ¥í•œ ëª¨ë¸ì˜ ê³µê°„ì„ íƒìƒ‰í•˜ê¸° ìœ„í•´ Weights & Biases ìŠ¤ìœ•ì„ ì‚¬ìš©í•˜ì„¸ìš”.

## [W&B ìŠ¤ìœ•ì„ ì‚¬ìš©í•œ TensorFlowì—ì„œì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í™•ì¸í•˜ê¸° $\rightarrow$](http://wandb.me/tf-sweeps-colab)

### W&B ìŠ¤ìœ• ì‚¬ìš©ì˜ ì´ì 

* **ë¹ ë¥¸ ì„¤ì •**: ëª‡ ì¤„ì˜ ì½”ë“œë¡œ W&B ìŠ¤ìœ•ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **íˆ¬ëª…ì„±**: ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì„ ì¸ìš©í•˜ë©°, [ìš°ë¦¬ì˜ ì½”ë“œëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ì…ë‹ˆë‹¤](https://github.com/wandb/client/tree/master/wandb/sweeps).
* **ê°•ë ¥í•¨**: ìš°ë¦¬ì˜ ìŠ¤ìœ•ì€ ì™„ì „íˆ ë§ì¶¤ ì„¤ì • ë° êµ¬ì„± ê°€ëŠ¥í•©ë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì˜ ë…¸íŠ¸ë¶ì—ì„œ ìŠ¤ìœ•ì„ ì‹œì‘í•˜ëŠ” ê²ƒì´ ì—¬ëŸ¬ ëŒ€ì˜ ê¸°ê³„ì— ê±¸ì³ ìŠ¤ìœ•ì„ ì‹œì‘í•˜ëŠ” ê²ƒë§Œí¼ ì‰½ìŠµë‹ˆë‹¤.


<img src="https://i.imgur.com/6eWHZhg.png" alt="Sweep Result" />

# ğŸ¨ ì˜ˆì‹œ ê°¤ëŸ¬ë¦¬

W&Bì—ì„œ ì¶”ì  ë° ì‹œê°í™”ëœ í”„ë¡œì íŠ¸ì˜ ì˜ˆì‹œë¥¼ ìš°ë¦¬ì˜ ì˜ˆì‹œ ê°¤ëŸ¬ë¦¬, [Fully Connected â†’](https://wandb.me/fc)ì—ì„œ í™•ì¸í•˜ì„¸ìš”.

# ğŸ“ ëª¨ë²” ì‚¬ë¡€
1. **í”„ë¡œì íŠ¸**: ì—¬ëŸ¬ ì‹¤í–‰ì„ í”„ë¡œì íŠ¸ì— ë¡œê·¸í•˜ì—¬ ë¹„êµí•©ë‹ˆë‹¤. `wandb.init(project="project-name")`
2. **ê·¸ë£¹**: ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ ë˜ëŠ” êµì°¨ê²€ì¦ í´ë“œì˜ ê²½ìš°, ê° í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰ìœ¼ë¡œ ë¡œê·¸í•˜ê³  í•¨ê»˜ ê·¸ë£¹í™”í•©ë‹ˆë‹¤. `wandb.init(group='experiment-1')`
3. **íƒœê·¸**: í˜„ì¬ ë² ì´ìŠ¤ë¼ì¸ ë˜ëŠ” í”„ë¡œë•ì…˜ ëª¨ë¸ì„ ì¶”ì í•˜ê¸° ìœ„í•´ íƒœê·¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
4. **ë…¸íŠ¸**: ì‹¤í–‰ ì‚¬ì´ì˜ ë³€ê²½ ì‚¬í•­ì„ ì¶”ì í•˜ê¸° ìœ„í•´ í…Œì´ë¸”ì— ë…¸íŠ¸ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.
5. **ë¦¬í¬íŠ¸**: ì§„í–‰ ìƒí™©ì— ëŒ€í•œ ë¹ ë¥¸ ë…¸íŠ¸ë¥¼ ë™ë£Œì™€ ê³µìœ í•˜ê³  ML í”„ë¡œì íŠ¸ì˜ ëŒ€ì‹œë³´ë“œ ë° ìŠ¤ëƒ…ìƒ·ì„ ë§Œë“­ë‹ˆë‹¤.

## ğŸ¤“ ê³ ê¸‰ ì„¤ì •
1. [í™˜ê²½ ë³€ìˆ˜](https://docs.wandb.com/library/environment-variables): ê´€ë¦¬ í´ëŸ¬ìŠ¤í„°ì—ì„œ íŠ¸ë ˆì´ë‹ì„ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ í™˜ê²½ ë³€ìˆ˜ì— API í‚¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
2. [ì˜¤í”„ë¼ì¸ ëª¨ë“œ](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): `dryrun` ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤í”„ë¼ì¸ìœ¼ë¡œ íŠ¸ë ˆì´ë‹ì„ í•˜ê³  ë‚˜ì¤‘ì— ê²°ê³¼ë¥¼ ë™ê¸°í™”í•©ë‹ˆë‹¤.
3. [ì˜¨í”„ë ˆë¯¸ìŠ¤](https://docs.wandb.com/self-hosted): W&Bë¥¼ ê·€í•˜ì˜ ì¸í”„ë¼ ë‚´ì˜ í”„ë¼ì´ë¹— í´ë¼ìš°ë“œ ë˜ëŠ” ì—ì–´ê°­ ì„œë²„ì— ì„¤ì¹˜í•˜ì„¸ìš”. ìš°ë¦¬ëŠ” í•™ê³„ë¶€í„° ì—”í„°í”„ë¼ì´ì¦ˆ íŒ€ê¹Œì§€ ëª¨ë“  ì‚¬ëŒì„ ìœ„í•œ ë¡œì»¬ ì„¤ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
4. [ì•„í‹°íŒ©íŠ¸](http://wandb.me/artifacts-colab): ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ìŠ¤íŠ¸ë¦¼ë¼ì¸ ë°©ì‹ìœ¼ë¡œ ì¶”ì  ë° ë²„ì „ ê´€ë¦¬í•˜ì—¬ ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•  ë•Œ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë¥¼ ìë™ìœ¼ë¡œ íŒŒì•…í•©ë‹ˆë‹¤.