
# ì˜ˆì¸¡ ì‹œê°í™”í•˜ê¸°

[**ì—¬ê¸°ì—ì„œ Colab ë…¸íŠ¸ë¶ìœ¼ë¡œ ì‹œë„í•´ë³´ì„¸ìš” â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/datasets-predictions/W&B_Tables_Quickstart.ipynb)

ì´ ë¬¸ì„œì—ì„œëŠ” PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ MNIST ë°ì´í„°ë¡œ ëª¨ë¸ íŠ¸ë ˆì´ë‹ ì¤‘ ì˜ˆì¸¡ê°’ì„ ì¶”ì , ì‹œê°í™” ë° ë¹„êµí•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

ë‹¤ìŒì„ ë°°ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
1. ëª¨ë¸ íŠ¸ë ˆì´ë‹ ë˜ëŠ” í‰ê°€ ì¤‘ `wandb.Table()`ì— ë©”íŠ¸ë¦­, ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ ë“±ì„ ë¡œê·¸í•˜ê¸°
2. ì´ëŸ¬í•œ í…Œì´ë¸”ì„ ë³´ê³ , ì •ë ¬í•˜ê³ , í•„í„°ë§í•˜ê³ , ê·¸ë£¹í™”í•˜ê³ , í•©ì¹˜ê³ , ëŒ€í™”ì‹ìœ¼ë¡œ ì¿¼ë¦¬í•˜ê³  íƒìƒ‰í•˜ê¸°
3. íŠ¹ì • ì´ë¯¸ì§€, í•˜ì´í¼íŒŒë¼ë¯¸í„°/ëª¨ë¸ ë²„ì „ ë˜ëŠ” ì‹œê°„ ë‹¨ê³„ì—ì„œ ë™ì ìœ¼ë¡œ ëª¨ë¸ ì˜ˆì¸¡ê°’ ë˜ëŠ” ê²°ê³¼ ë¹„êµí•˜ê¸°

# ì˜ˆì‹œ

## íŠ¹ì • ì´ë¯¸ì§€ì— ëŒ€í•œ ì˜ˆì¸¡ ì ìˆ˜ ë¹„êµí•˜ê¸°

[ì‹¤ì‹œê°„ ì˜ˆì‹œ: íŠ¸ë ˆì´ë‹ 1 ì—í¬í¬ ëŒ€ 5 ì—í¬í¬ í›„ì˜ ì˜ˆì¸¡ ë¹„êµ â†’](https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU#compare-predictions-after-1-vs-5-epochs)
<img src="https://i.imgur.com/NMme6Qj.png" alt="1 ì—í¬í¬ ëŒ€ 5 ì—í¬í¬ì˜ íŠ¸ë ˆì´ë‹"/>
íˆìŠ¤í† ê·¸ë¨ì€ ë‘ ëª¨ë¸ ê°„ì˜ í´ë˜ìŠ¤ë³„ ì ìˆ˜ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. ê° íˆìŠ¤í† ê·¸ë¨ì—ì„œ ìƒë‹¨ì˜ ì´ˆë¡ìƒ‰ ë§‰ëŒ€ëŠ” ëª¨ë¸ "CNN-2, 1 ì—í¬í¬"(id 0)ë¥¼ ë‚˜íƒ€ë‚´ë©°, 1 ì—í¬í¬ë§Œ íŠ¸ë ˆì´ë‹ë˜ì—ˆìŠµë‹ˆë‹¤. í•˜ë‹¨ì˜ ë³´ë¼ìƒ‰ ë§‰ëŒ€ëŠ” ëª¨ë¸ "CNN-2, 5 ì—í¬í¬"(id 1)ë¥¼ ë‚˜íƒ€ë‚´ë©°, 5 ì—í¬í¬ ë™ì•ˆ íŠ¸ë ˆì´ë‹ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ëŠ” ëª¨ë¸ì´ ë™ì˜í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì— í•„í„°ë§ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì²« ë²ˆì§¸ í–‰ì—ì„œ "4"ëŠ” 1 ì—í¬í¬ í›„ ëª¨ë“  ê°€ëŠ¥í•œ ìˆ«ìì— ëŒ€í•´ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì§€ë§Œ, 5 ì—í¬í¬ í›„ì—ëŠ” ì˜¬ë°”ë¥¸ ë¼ë²¨ì— ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ê³  ë‚˜ë¨¸ì§€ì—ëŠ” ë§¤ìš° ë‚®ì€ ì ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤.

## ì‹œê°„ì— ë”°ë¥¸ ì£¼ìš” ì˜¤ë¥˜ì— ì§‘ì¤‘í•˜ê¸°
[ì‹¤ì‹œê°„ ì˜ˆì‹œ â†’](https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU#top-errors-over-time)

ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì˜ëª»ëœ ì˜ˆì¸¡("ì¶”ì¸¡" != "ì§„ì‹¤")ì„ ë³´ê³ , í–‰ì„ í•„í„°ë§í•©ë‹ˆë‹¤. 1 ì—í¬í¬ íŠ¸ë ˆì´ë‹ í›„ì— 229ê°œì˜ ì˜ëª»ëœ ì¶”ì¸¡ì´ ìˆì§€ë§Œ, 5 ì—í¬í¬ í›„ì—ëŠ” ì˜¤ì§ 98ê°œì…ë‹ˆë‹¤.
<img src="https://i.imgur.com/7g8nodn.png" alt="side by side, 1 ëŒ€ 5 ì—í¬í¬ì˜ íŠ¸ë ˆì´ë‹"/>

## ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° íŒ¨í„´ ì°¾ê¸°

[ì‹¤ì‹œê°„ ì˜ˆì‹œì—ì„œ ì „ì²´ ì„¸ë¶€ ì •ë³´ ë³´ê¸° â†’](https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU#false-positives-grouped-by-guess)

ì •ë‹µì„ í•„í„°ë§í•œ í›„, ì¶”ì¸¡ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë‘ ëª¨ë¸ ì˜†ìœ¼ë¡œ ì˜¤ë¶„ë¥˜ëœ ì´ë¯¸ì§€ ì˜ˆì‹œì™€ ì§„ì§œ ë¼ë²¨ì˜ ê¸°ë³¸ ë¶„í¬ë¥¼ ë´…ë‹ˆë‹¤. ë ˆì´ì–´ í¬ê¸°ì™€ í•™ìŠµë¥ ì´ 2ë°°ì¸ ëª¨ë¸ ë³€í˜•ì´ ì™¼ìª½ì— ìˆê³ , ë² ì´ìŠ¤ë¼ì¸ì€ ì˜¤ë¥¸ìª½ì— ìˆìŠµë‹ˆë‹¤. ë² ì´ìŠ¤ë¼ì¸ì´ ê° ì¶”ì¸¡ëœ í´ë˜ìŠ¤ì— ëŒ€í•´ ì•½ê°„ ë” ë§ì€ ì‹¤ìˆ˜ë¥¼ í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
<img src="https://i.imgur.com/i5PP9AE.png" alt="ê¸°ë³¸ê°’ ëŒ€ë¹„ ë‘ ë°° ë³€í˜•ì— ëŒ€í•œ ê·¸ë£¹í™”ëœ ì˜¤ë¥˜"/>

# ê°€ì… ë˜ëŠ” ë¡œê·¸ì¸

[W&Bì— ê°€ì…í•˜ê±°ë‚˜ ë¡œê·¸ì¸](https://wandb.ai/login)í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤í—˜ì„ ë³´ê³  ìƒí˜¸ ì‘ìš©í•˜ì„¸ìš”.

ì´ ì˜ˆì‹œì—ì„œëŠ” í¸ë¦¬í•œ í˜¸ìŠ¤íŒ… í™˜ê²½ìœ¼ë¡œ Google Colabë¥¼ ì‚¬ìš©í•˜ê³  ìˆì§€ë§Œ, ì–´ë””ì„œë“  ìì‹ ë§Œì˜ íŠ¸ë ˆì´ë‹ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  W&Bì˜ ì‹¤í—˜ ì¶”ì  íˆ´ë¡œ ë©”íŠ¸ë¦­ì„ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
!pip install wandb -qqq
```

ê³„ì •ì— ë¡œê·¸ì¸


```python

import wandb
wandb.login()

WANDB_PROJECT = "mnist-viz"
```

# 0. ì¤€ë¹„

ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•˜ê³ , MNISTë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ , PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ë ˆì¸ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.


```python
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as T 
import torch.nn.functional as F


device = "cuda:0" if torch.cuda.is_available() else "cpu"

# íŠ¸ë ˆì¸ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„±
def get_dataloader(is_train, batch_size, slice=5):
    "íŠ¸ë ˆì´ë‹ ë°ì´í„°ë¡œë” ì–»ê¸°"
    ds = torchvision.datasets.MNIST(root=".", train=is_train, transform=T.ToTensor(), download=True)
    loader = torch.utils.data.DataLoader(dataset=ds, 
                                         batch_size=batch_size, 
                                         shuffle=True if is_train else False, 
                                         pin_memory=True, num_workers=2)
    return loader
```

# 1. ëª¨ë¸ ë° íŠ¸ë ˆì´ë‹ ì¼ì • ì •ì˜í•˜ê¸°

* ì‹¤í–‰í•  ì—í¬í¬ ìˆ˜ë¥¼ ì„¤ì •í•˜ê³ , ê° ì—í¬í¬ëŠ” íŠ¸ë ˆì´ë‹ ë‹¨ê³„ì™€ ê²€ì¦(í…ŒìŠ¤íŠ¸) ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì„ íƒì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë‹¨ê³„ ë‹¹ ë¡œê·¸í•˜ëŠ” ë°ì´í„° ì–‘ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œëŠ” ë°ëª¨ë¥¼ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ ë°°ì¹˜ ìˆ˜ì™€ ë°°ì¹˜ ë‹¹ ì´ë¯¸ì§€ ìˆ˜ê°€ ë‚®ê²Œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
* ê°„ë‹¨í•œ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ ì •ì˜í•˜ê¸°([pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial) ì½”ë“œë¥¼ ë”°ë¦„).
* PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ë ˆì¸ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¡œë“œí•˜ê¸°



```python
# ì‹¤í–‰í•  ì—í¬í¬ ìˆ˜
# ê° ì—í¬í¬ëŠ” íŠ¸ë ˆì´ë‹ ë‹¨ê³„ì™€ í…ŒìŠ¤íŠ¸ ë‹¨ê³„ë¥¼ í¬í•¨í•˜ë¯€ë¡œ, ì´ëŠ”
# í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì„ ë¡œê·¸í•  í…Œì´ë¸” ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤
EPOCHS = 1

# ê° í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì— ëŒ€í•´ ë¡œê·¸í•  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë°°ì¹˜ ìˆ˜
# (ë°ëª¨ë¥¼ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ ê¸°ë³¸ê°’ì´ ë‚®ê²Œ ì„¤ì •ë¨)
NUM_BATCHES_TO_LOG = 10 #79

# í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ë‹¹ ë¡œê·¸í•  ì´ë¯¸ì§€ ìˆ˜
# (ë°ëª¨ë¥¼ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ ê¸°ë³¸ê°’ì´ ë‚®ê²Œ ì„¤ì •ë¨)
NUM_IMAGES_PER_BATCH = 32 #128

# íŠ¸ë ˆì´ë‹ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
NUM_CLASSES = 10
BATCH_SIZE = 32
LEARNING_RATE = 0.001
L1_SIZE = 32
L2_SIZE = 64
# ì´ë¥¼ ë³€ê²½í•˜ë©´ ì¸ì ‘í•œ ë ˆì´ì–´ì˜ í˜•íƒœë¥¼ ë³€ê²½í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
CONV_KERNEL_SIZE = 5

# ë‘ ë ˆì´ì–´ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ ì •ì˜í•˜ê¸°
class ConvNet(nn.Module):
    def __init__(self, num_classes=10):
        super(ConvNet, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, L1_SIZE, CONV_KERNEL_SIZE, stride=1, padding=2),
            nn.BatchNorm2d(L1_SIZE),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(L1_SIZE, L2_SIZE, CONV_KERNEL_SIZE, stride=1, padding=2),
            nn.BatchNorm2d(L2_SIZE),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.fc = nn.Linear(7*7*L2_SIZE, NUM_CLASSES)
        self.softmax = nn.Softmax(NUM_CLASSES)

    def forward(self, x):
        # ì£¼ì–´ì§„ ë ˆì´ì–´ì˜ í˜•íƒœë¥¼ ë³´ê³  ì‹¶ë‹¤ë©´ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”:
        #print("x: ", x.size())
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        return out

train_loader = get_dataloader(is_train=True, batch_size=BATCH_SIZE)
test_loader = get_dataloader(is_train=False, batch_size=2*BATCH_SIZE)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

# 2. íŠ¸ë ˆì´ë‹ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ë¡œê·¸í•˜ê¸°

ë§¤ ì—í¬í¬ë§ˆë‹¤, íŠ¸ë ˆì´ë‹ ë‹¨ê³„ì™€ í…ŒìŠ¤íŠ¸ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. ê° í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì— ëŒ€í•´, í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì„ ì €ì¥í•  wandb.Table()ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ë“¤ì€ ë¸Œë¼ìš°ì €ì—ì„œ ì‹œê°í™”ë˜ê³ , ë™ì ìœ¼ë¡œ ì¿¼ë¦¬ë˜ë©°, ë‚˜ë€íˆ ë¹„êµë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
# âœ¨ W&B: ì´ ëª¨ë¸ì˜ íŠ¸ë ˆì´ë‹ì„ ì¶”ì í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ run ì´ˆê¸°í™”í•˜ê¸°
wandb.init(project="table-quickstart")

# âœ¨ W&B: configë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê·¸í•˜ê¸°
cfg = wandb.config
cfg.update({"epochs" : EPOCHS, "batch_size": BATCH_SIZE, "lr" : LEARNING_RATE,
            "l1_size" : L1_SIZE, "l2_size": L2_SIZE,
            "conv_kernel" : CONV_KERNEL_SIZE,
            "img_count" : min(10000, NUM_IMAGES_PER_BATCH*NUM_BATCHES_TO_LOG)})

# ëª¨ë¸, ì†ì‹¤, ì˜µí‹°ë§ˆì´ì € ì •ì˜í•˜ê¸°
model = ConvNet(NUM_CLASSES).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë°°ì¹˜ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ë¡œê·¸í•˜ê¸° ìœ„í•œ í¸ë¦¬í•œ í•¨ìˆ˜
def log_test_predictions(images, labels, outputs, predicted, test_table, log_counter):
  # ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ ì‹ ë¢°ë„ ì ìˆ˜ ì–»ê¸°
  scores = F.softmax(outputs.data, dim=1)
  log_scores = scores.cpu().numpy()
  log_images = images.cpu().numpy()
  log_labels = labels.cpu().numpy()
  log_preds = predicted.cpu().numpy()
  # ì´ë¯¸ì§€ ìˆœì„œì— ë”°ë¼ id ì¶”ê°€í•˜ê¸°
  _id = 0
  for i, l, p, s in zip(log_images, log_labels, log_preds, log_scores):
    # ë°ì´í„° í…Œì´ë¸”ì— í•„ìš”í•œ ì •ë³´ ì¶”ê°€í•˜ê¸°:
    # id, ì´ë¯¸ì§€ í”½ì…€, ëª¨ë¸ì˜ ì¶”ì¸¡, ì§„ì§œ ë¼ë²¨, ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ ì ìˆ˜
    img_id = str(_id) + "_" + str(log_counter)
    test_table.add_data(img_id, wandb.Image(i), p, l, *s)
    _id += 1
    if _id == NUM_IMAGES_PER_BATCH:
      break

# ëª¨ë¸ íŠ¸ë ˆì´ë‹í•˜ê¸°
total_step = len(train_loader)
for epoch in range(EPOCHS):
    # íŠ¸ë ˆì´ë‹ ë‹¨ê³„
    for i, (images, labels) in enumerate(train_loader):
        images = images.to(device)
        labels = labels.to(device)
        # forward íŒ¨ìŠ¤
        outputs = model(images)
        loss = criterion(outputs, labels)
        # ì—­ì „íŒŒ ë° ìµœì í™”
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
  
        # âœ¨ W&B: íŠ¸ë ˆì´ë‹ ë‹¨ê³„ì—ì„œ ì†ì‹¤ì„ ë¡œê·¸í•˜ê¸°, UIì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹œê°í™”ë¨
        wandb.log({"loss" : loss})
        if (i+1) % 100 == 0:
            print ('ì—í¬í¬ [{}/{}], ìŠ¤í… [{}/{}], ì†ì‹¤: {:.4f}'
                .format(epoch+1, EPOCHS, i+1, total_step, loss.item()))
            

    # âœ¨ W&B: ê° í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì €ì¥í•  í…Œì´ë¸” ìƒì„±í•˜ê¸°
    columns=["id", "image", "guess", "truth"]
    for digit in range(10):
      columns.append("score_" + str(digit))
    test_table = wandb.Table(columns=columns)

    # ëª¨ë¸ í…ŒìŠ¤íŠ¸í•˜ê¸°
    model.eval()
    log_counter = 0
    with torch.no_grad():
        correct = 0
        total = 0
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            if log_counter < NUM_BATCHES_TO_LOG:
              log_test_predictions(images, labels, outputs, predicted, test_table, log_counter)
              log_counter += 1
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        acc = 100 * correct / total
        # âœ¨ W&B: íŠ¸ë ˆì´ë‹ ì—í¬í¬ì— ê±¸ì³ ì •í™•ë„ë¥¼ ë¡œê·¸í•˜ì—¬ UIì—ì„œ ì‹œê°í™”í•˜ê¸°
        wandb.log({"epoch" : epoch, "acc" : acc})
        print('10000 í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {} %'.format(acc))

    # âœ¨ W&B: ì˜ˆì¸¡ í…Œì´ë¸”ì„ wandbì— ë¡œê·¸í•˜ê¸°
    wandb.log({"test_predictions" : test_table})

# âœ¨ W&B: (ë‹¤ì¤‘ ì…€ ë…¸íŠ¸ë¶ì— ìœ ìš©í•¨) ì‹¤í–‰ì„ ì™„ë£Œë¡œ í‘œì‹œí•˜ê¸°
wandb.finish()
```

# ë‹¤ìŒì€ ë¬´ì—‡ì¸ê°€ìš”?
ë‹¤ìŒ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” W&B Sweepsë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤:

## ğŸ‘‰ [í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”í•˜ê¸°](sweeps)