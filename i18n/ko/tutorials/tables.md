
# ì˜ˆì¸¡ ì‹œê°í™”í•˜ê¸°

[**Colab ë…¸íŠ¸ë¶ì—ì„œ ì‹œë„í•´ë³´ê¸° â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/datasets-predictions/W&B_Tables_Quickstart.ipynb)

ì´ ë¬¸ì„œëŠ” PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ MNIST ë°ì´í„°ì— ëŒ€í•œ ëª¨ë¸ ì˜ˆì¸¡ì„ ì¶”ì , ì‹œê°í™”, ë¹„êµí•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

ë‹¤ìŒì„ ë°°ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
1. ëª¨ë¸ í•™ìŠµì´ë‚˜ í‰ê°€ ì¤‘ì— ë©”íŠ¸ë¦­, ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ ë“±ì„ `wandb.Table()`ì— ë¡œê·¸í•˜ê¸°
2. ì´ëŸ¬í•œ í…Œì´ë¸”ì„ ë³´ê³ , ì •ë ¬í•˜ê³ , í•„í„°ë§í•˜ê³ , ê·¸ë£¹í™”í•˜ê³ , ì¡°ì¸í•˜ê³ , ëŒ€í™”í˜•ìœ¼ë¡œ ì¿¼ë¦¬í•˜ê³ , íƒìƒ‰í•˜ê¸°
3. ëª¨ë¸ ì˜ˆì¸¡ì´ë‚˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ê¸°: íŠ¹ì • ì´ë¯¸ì§€, í•˜ì´í¼íŒŒë¼ë¯¸í„°/ëª¨ë¸ ë²„ì „ ë˜ëŠ” ì‹œê°„ ë‹¨ê³„ì— ê±¸ì³ ë™ì ìœ¼ë¡œ ë¹„êµí•˜ê¸°

# ì˜ˆì‹œ

## íŠ¹ì • ì´ë¯¸ì§€ì— ëŒ€í•œ ì˜ˆì¸¡ ì ìˆ˜ ë¹„êµ

[ì‹¤ì œ ì˜ˆì‹œ: 1 ì—í¬í¬ ëŒ€ë¹„ 5 ì—í¬í¬ í•™ìŠµ í›„ ì˜ˆì¸¡ ë¹„êµ â†’](https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU#compare-predictions-after-1-vs-5-epochs)
<img src="https://i.imgur.com/NMme6Qj.png" alt="1 epoch vs 5 epochs of training"/>
íˆìŠ¤í† ê·¸ë¨ì€ ë‘ ëª¨ë¸ ê°„ì˜ í´ë˜ìŠ¤ë³„ ì ìˆ˜ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. ê° íˆìŠ¤í† ê·¸ë¨ì—ì„œ ìƒë‹¨ì˜ ì´ˆë¡ìƒ‰ ë°”ëŠ” ëª¨ë¸ "CNN-2, 1 ì—í¬í¬"(id 0)ë¥¼ ë‚˜íƒ€ë‚´ë©°, 1 ì—í¬í¬ë§Œ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. í•˜ë‹¨ì˜ ë³´ë¼ìƒ‰ ë°”ëŠ” ëª¨ë¸ "CNN-2, 5 ì—í¬í¬"(id 1)ë¥¼ ë‚˜íƒ€ë‚´ë©°, 5 ì—í¬í¬ ë™ì•ˆ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ëŠ” ëª¨ë¸ì´ ë™ì˜í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì— í•„í„°ë§ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì²« ë²ˆì§¸ í–‰ì—ì„œ "4"ëŠ” 1 ì—í¬í¬ í›„ ëª¨ë“  ê°€ëŠ¥í•œ ìˆ«ìì— ëŒ€í•´ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì§€ë§Œ, 5 ì—í¬í¬ í›„ì—ëŠ” ì˜¬ë°”ë¥¸ ë ˆì´ë¸”ì— ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ê³  ë‚˜ë¨¸ì§€ì—ëŠ” ë§¤ìš° ë‚®ì€ ì ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤.

## ì‹œê°„ì— ë”°ë¥¸ ì£¼ìš” ì˜¤ë¥˜ ì§‘ì¤‘
[ì‹¤ì œ ì˜ˆì‹œ â†’](https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU#top-errors-over-time)

ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì˜ëª»ëœ ì˜ˆì¸¡("ì¶”ì¸¡" != "ì§„ì‹¤")ì„ ë³´ê³  í•„í„°ë§í•©ë‹ˆë‹¤. 1ë²ˆì˜ í•™ìŠµ ì—í¬í¬ í›„ì—ëŠ” 229ê°œì˜ ì˜ëª»ëœ ì¶”ì¸¡ì´ ìˆì§€ë§Œ, 5 ì—í¬í¬ í›„ì—ëŠ” 98ê°œ ë¿ì…ë‹ˆë‹¤.
<img src="https://i.imgur.com/7g8nodn.png" alt="side by side, 1 vs 5 epochs of training"/>

## ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° íŒ¨í„´ ì°¾ê¸°

[ì‹¤ì œ ì˜ˆì‹œì—ì„œ ì „ì²´ ìƒì„¸ ë³´ê¸° â†’](https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU#false-positives-grouped-by-guess)

ì •ë‹µì„ í•„í„°ë§í•œ í›„ ì¶”ì¸¡ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë‘ ëª¨ë¸ ì˜†ì— ì˜¤ë¶„ë¥˜ëœ ì´ë¯¸ì§€ì™€ ì§„ì§œ ë ˆì´ë¸”ì˜ ê¸°ë³¸ ë¶„í¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ëª¨ë¸ ë³€í˜• ì¤‘ ë ˆì´ì–´ í¬ê¸°ì™€ í•™ìŠµë¥ ì´ 2ë°°ì¸ ê²ƒì´ ì™¼ìª½ì— ìˆê³ , ê¸°ì¤€ì´ ì˜¤ë¥¸ìª½ì— ìˆìŠµë‹ˆë‹¤. ê¸°ì¤€ì€ ê° ì¶”ì¸¡ëœ í´ë˜ìŠ¤ì— ëŒ€í•´ ì•½ê°„ ë” ë§ì€ ì‹¤ìˆ˜ë¥¼ í•©ë‹ˆë‹¤.
<img src="https://i.imgur.com/i5PP9AE.png" alt="grouped errors for baseline vs double variant"/>

# ê°€ì…í•˜ê±°ë‚˜ ë¡œê·¸ì¸í•˜ê¸°

[W&Bì— ê°€ì…í•˜ê±°ë‚˜ ë¡œê·¸ì¸](https://wandb.ai/login)í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤í—˜ì„ ë³´ê³  ìƒí˜¸ ì‘ìš©í•©ë‹ˆë‹¤.

ì´ ì˜ˆì‹œì—ì„œëŠ” í¸ë¦¬í•œ í˜¸ìŠ¤íŒ… í™˜ê²½ìœ¼ë¡œ Google Colabì„ ì‚¬ìš©í•˜ê³  ìˆì§€ë§Œ, ì–´ë””ì—ì„œë‚˜ ìì‹ ì˜ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  W&Bì˜ ì‹¤í—˜ ì¶”ì  ë„êµ¬ë¡œ ë©”íŠ¸ë¦­ì„ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
!pip install wandb -qqq
```

ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸í•˜ê¸°


```python

import wandb
wandb.login()

WANDB_PROJECT = "mnist-viz"
```

# 0. ì¤€ë¹„

ì˜ì¡´ì„± ì„¤ì¹˜, MNIST ë‹¤ìš´ë¡œë“œ, PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì„¸íŠ¸ ìƒì„±í•˜ê¸°


```python
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as T 
import torch.nn.functional as F


device = "cuda:0" if torch.cuda.is_available() else "cpu"

# í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„±
def get_dataloader(is_train, batch_size, slice=5):
    "í•™ìŠµ ë°ì´í„°ë¡œë” ê°€ì ¸ì˜¤ê¸°"
    ds = torchvision.datasets.MNIST(root=".", train=is_train, transform=T.ToTensor(), download=True)
    loader = torch.utils.data.DataLoader(dataset=ds, 
                                         batch_size=batch_size, 
                                         shuffle=True if is_train else False, 
                                         pin_memory=True, num_workers=2)
    return loader
```

# 1. ëª¨ë¸ ë° í•™ìŠµ ì¼ì • ì •ì˜í•˜ê¸°

* ì‹¤í–‰í•  ì—í¬í¬ ìˆ˜ ì„¤ì •í•˜ê¸°, ì—¬ê¸°ì„œ ê° ì—í¬í¬ëŠ” í•™ìŠµ ë‹¨ê³„ì™€ ê²€ì¦(í…ŒìŠ¤íŠ¸) ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì„ íƒì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë‹¨ê³„ë§ˆë‹¤ ë¡œê·¸í•  ë°ì´í„°ì˜ ì–‘ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë°ëª¨ë¥¼ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ ë°°ì¹˜ ìˆ˜ì™€ ë°°ì¹˜ë‹¹ ì´ë¯¸ì§€ ìˆ˜ë¥¼ ë‚®ê²Œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
* ê°„ë‹¨í•œ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ ì •ì˜í•˜ê¸° ([pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial) ì½”ë“œë¥¼ ë”°ë¦„).
* PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¡œë“œí•˜ê¸°



```python
# ì‹¤í–‰í•  ì—í¬í¬ ìˆ˜
# ê° ì—í¬í¬ëŠ” í•™ìŠµ ë‹¨ê³„ì™€ í…ŒìŠ¤íŠ¸ ë‹¨ê³„ë¥¼ í¬í•¨í•˜ë¯€ë¡œ, ì´ëŠ” í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì„ ë¡œê·¸í•  í…Œì´ë¸”ì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤
EPOCHS = 1

# ê° í…ŒìŠ¤íŠ¸ ë‹¨ê³„ë§ˆë‹¤ ë¡œê·¸í•  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë°°ì¹˜ ìˆ˜
# (ë°ëª¨ë¥¼ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ ê¸°ë³¸ê°’ì´ ë‚®ê²Œ ì„¤ì •ë¨)
NUM_BATCHES_TO_LOG = 10 #79

# í…ŒìŠ¤íŠ¸ ë°°ì¹˜ë‹¹ ë¡œê·¸í•  ì´ë¯¸ì§€ ìˆ˜
# (ë°ëª¨ë¥¼ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ ê¸°ë³¸ê°’ì´ ë‚®ê²Œ ì„¤ì •ë¨)
NUM_IMAGES_PER_BATCH = 32 #128

# í•™ìŠµ êµ¬ì„± ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
NUM_CLASSES = 10
BATCH_SIZE = 32
LEARNING_RATE = 0.001
L1_SIZE = 32
L2_SIZE = 64
# ì´ë¥¼ ë³€ê²½í•˜ë©´ ì¸ì ‘í•œ ë ˆì´ì–´ì˜ ëª¨ì–‘ì„ ë³€ê²½í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤
CONV_KERNEL_SIZE = 5

# ë‘ ë ˆì´ì–´ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ ì •ì˜
class ConvNet(nn.Module):
    def __init__(self, num_classes=10):
        super(ConvNet, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, L1_SIZE, CONV_KERNEL_SIZE, stride=1, padding=2),
            nn.BatchNorm2d(L1_SIZE),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(L1_SIZE, L2_SIZE, CONV_KERNEL_SIZE, stride=1, padding=2),
            nn.BatchNorm2d(L2_SIZE),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.fc = nn.Linear(7*7*L2_SIZE, NUM_CLASSES)
        self.softmax = nn.Softmax(NUM_CLASSES)

    def forward(self, x):
        # ì£¼ì–´ì§„ ë ˆì´ì–´ì˜ ëª¨ì–‘ì„ ë³´ë ¤ë©´ ì£¼ì„ í•´ì œí•˜ì„¸ìš”:
        #print("x: ", x.size())
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        return out

train_loader = get_dataloader(is_train=True, batch_size=BATCH_SIZE)
test_loader = get_dataloader(is_train=False, batch_size=2*BATCH_SIZE)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

# 2. í•™ìŠµ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ë¡œê·¸í•˜ê¸°

ë§¤ ì—í¬í¬ë§ˆë‹¤ í•™ìŠµ ë‹¨ê³„ì™€ í…ŒìŠ¤íŠ¸ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. ê° í…ŒìŠ¤íŠ¸ ë‹¨ê³„ë§ˆë‹¤, í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì„ ì €ì¥í•  wandb.Table()ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ë“¤ì€ ë¸Œë¼ìš°ì €ì—ì„œ ì‹œê°ì ìœ¼ë¡œ, ë™ì ìœ¼ë¡œ ì¿¼ë¦¬í•˜ê³ , ë‚˜ë€íˆ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
# âœ¨ W&B: ì´ ëª¨ë¸ í•™ìŠµì„ ì¶”ì í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì‹¤í–‰ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤
wandb.init(project="table-quickstart")

# âœ¨ W&B: í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸í•˜ê¸°
cfg = wandb.config
cfg.update({"epochs" : EPOCHS, "batch_size": BATCH_SIZE, "lr" : LEARNING_RATE,
            "l1_size" : L1_SIZE, "l2_size": L2_SIZE,
            "conv_kernel" : CONV_KERNEL_SIZE,
            "img_count" : min(10000, NUM_IMAGES_PER_BATCH*NUM_BATCHES_TO_LOG)})

# ëª¨ë¸, ì†ì‹¤, ì˜µí‹°ë§ˆì´ì € ì •ì˜
model = ConvNet(NUM_CLASSES).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë°°ì¹˜ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ë¡œê·¸í•˜ëŠ” í¸ë¦¬í•œ í•¨ìˆ˜
def log_test_predictions(images, labels, outputs, predicted, test_table, log_counter):
  # ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ ì‹ ë¢°ë„ ì ìˆ˜ ì–»ê¸°
  scores = F.softmax(outputs.data, dim=1)
  log_scores = scores.cpu().numpy()
  log_images = images.cpu().numpy()
  log_labels = labels.cpu().numpy()
  log_preds = predicted.cpu().numpy()
  # ì´ë¯¸ì§€ ìˆœì„œì— ë”°ë¼ id ì¶”ê°€
  _id = 0
  for i, l, p, s in zip(log_images, log_labels, log_preds, log_scores):
    # ë°ì´í„° í…Œì´ë¸”ì— í•„ìš”í•œ ì •ë³´ ì¶”ê°€:
    # id, ì´ë¯¸ì§€ í”½ì…€, ëª¨ë¸ì˜ ì¶”ì¸¡, ì§„ì§œ ë ˆì´ë¸”, ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ ì ìˆ˜
    img_id = str(_id) + "_" + str(log_counter)
    test_table.add_data(img_id, wandb.Image(i), p, l, *s)
    _id += 1
    if _id == NUM_IMAGES_PER_BATCH:
      break

# ëª¨ë¸ í•™ìŠµ
total_step = len(train_loader)
for epoch in range(EPOCHS):
    # í•™ìŠµ ë‹¨ê³„
    for i, (images, labels) in enumerate(train_loader):
        images = images.to(device)
        labels = labels.to(device)
        # ìˆœë°©í–¥ ì „ë‹¬
        outputs = model(images)
        loss = criterion(outputs, labels)
        # ì—­ë°©í–¥ ë° ìµœì í™”
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
  
        # âœ¨ W&B: í•™ìŠµ ë‹¨ê³„ë³„ë¡œ ì†ì‹¤ ë¡œê·¸, UIì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹œê°í™”
        wandb.log({"loss" : loss})
        if (i+1) % 100 == 0:
            print ('ì—í¬í¬ [{}/{}], ë‹¨ê³„ [{}/{}], ì†ì‹¤: {:.4f}'
                .format(epoch+1, EPOCHS, i+1, total_step, loss.item()))
            

    # âœ¨ W&B: ê° í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì €ì¥í•  í…Œì´ë¸” ìƒì„±
    columns=["id", "image", "guess", "truth"]
    for digit in range(10):
      columns.append("score_" + str(digit))
    test_table = wandb.Table(columns=columns)

    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    model.eval()
    log_counter = 0
    with torch.no_grad():
        correct = 0
        total = 0
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            if log_counter < NUM_BATCHES_TO_LOG:
              log_test_predictions(images, labels, outputs, predicted, test_table, log_counter)
              log_counter += 1
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        acc = 100 * correct / total
        # âœ¨ W&B: í•™ìŠµ ì—í¬í¬ë³„ë¡œ ì •í™•ë„ ë¡œê·¸, UIì—ì„œ ì‹œê°í™”
        wandb.log({"epoch" : epoch, "acc" : acc})
        print('10000ê°œì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ëª¨ë¸ì˜ ì •í™•ë„: {} %'.format(acc))

    # âœ¨ W&B: ì˜ˆì¸¡ í…Œì´ë¸”ì„ wandbì— ë¡œê·¸í•˜ê¸°
    wandb.log({"test_predictions" : test_table})

# âœ¨ W&B: ì‹¤í–‰ì„ ì™„ë£Œë¡œ í‘œì‹œ(ì—¬ëŸ¬ ì…€ ë…¸íŠ¸ë¶ì— ìœ ìš©)
wandb.finish()
```

# ë‹¤ìŒ ë‹¨ê³„ëŠ”?
ë‹¤ìŒ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” W&B ìŠ¤ìœ•ì„ ì‚¬ìš©í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ë°°ìš¸ ê²ƒì…ë‹ˆë‹¤:

## ğŸ‘‰ [í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”í•˜ê¸°](sweeps)