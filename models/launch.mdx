---
description: Evaluate model checkpoints or hosted API models within W&B and analyze the results using automatically generated leaderboards.
title: LLM Evaluation Jobs
---
import PreviewLink from '/snippets/en/_includes/llm-eval-jobs/preview.mdx';

[W&B LLM Evaluation Jobs](/models/launch) is a framework for measuring and analyzing how an LLM model performs, using infrastructure managed by CoreWeave. Run a comprehensive suite of modern, industry-standard [model evaluation benchmarks](/models/launch/evaluations), then view and analyze and share your model's performance using automatic leaderboards and charts in W&B. LLM Evaluation Jobs removes the complexity of deploying and maintaining advanced compute infrastructure yourself.

<PreviewLink />

{/*
<CardGroup cols={4}>
<ColabLink url="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb" />
<TryProductLink url="https://wandb.ai/stacey/deep-drive/workspace?workspace=user-lavanyashukla" />
</CardGroup>
*/}

## How it works
Evaluate a model checkpoint or a publicly accessible hosted OpenAI-compatible model in just a few steps:

1. Set up an evaluation job in the W&B App. Define its benchmarks and configuration, such as whether to generate a leaderboard.
1. Launch the evaluation job.
1. View and analyze the results and leaderboard.

Each time you launch an evaluation job with the same destination project, the project's leaderboard updates automatically.

<Frame>
![Example evalution job leaderboard](/images/models/llm-evaluation-jobs/model-checkpoint-leaderboard-example.png)
</Frame>

## Next steps
- Browse the [Evaluation benchmark catalog](/models/launch/evaluations)
- [Evaluate a model checkpoint](/models/launch/evaluate-model-checkpoint)
- [Evaluate an API-hosted model](/models/launch/evaluate-hosted-model)

## More details

### Pricing
LLM Evaluation Jobs is in **Preview**. During the preview period, there is no charge.

### Job limits
An individual evaluation job has these limits:
- The maximum size for a model to evaluate is 86 GB, including context.
- Each job is limited to two GPUs.

### Requirements
- To evaluate a model checkpoint, the model weights must be packaged as a VLLM-compatible artifact. See [Example: Prepare a model](/models/launch/evaluate-model-checkpoint#example-prepare-a-model) for details and example code.
- To evaluate an OpenAI-compatible model, it must be accessible at a public URL, and an organization or team admin must configure a team secret with the the API key for authentication.
- Certain benchmarks use OpenAI models for scoring. To run these benchmarks, an organization or team admin must configure a team secret with the the API key. This requirement is documented in [Evaluation benchmark catalog](/models/launch/evaluations).
- Certain benchmarks require access to gated datasets in Hugging Face. To run one of these benchmarks, an organization or team admin must request access to the gated dataset in Hugging face, generate a Hugging Face user access token, and configure it as a team secret. This requirement is documented in [Evaluation benchmark catalog](/models/launch/evaluations).

For more details and instructions for meeting these requirements, see:
- [Evaluate a model checkpoint](/models/launch/evaluate-hosted-model)
- [Evaluate a hosted API model](/models/launch/evaluate-model-checkpoint)
