---
description: Evaluate model checkpoints or hosted API models within W&B and analyze the results using automatically generated leaderboards.
title: LLM Evaluation Jobs
---
import PreviewLink from '/snippets/en/_includes/llm-eval-jobs/preview.mdx';

[LLM Evaluation Jobs](/models/launch) is a benchmarking framework for evaluating an LLM model's performance, using infrastructure managed by CoreWeave. Choose from a comprehensive suite of modern, industry-standard [model evaluation benchmarks](/models/launch/evaluations), then view, analyze, and share the results using automatic leaderboards and charts in W&B Models. LLM Evaluation Jobs removes the complexity of deploying and maintaining GPU infrastructure yourself.

<Note><PreviewLink /></Note>

{/*
<CardGroup cols={4}>
<ColabLink url="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb" />
<TryProductLink url="https://wandb.ai/stacey/deep-drive/workspace?workspace=user-lavanyashukla" />
</CardGroup>
*/}

## How it works
Evaluate a model checkpoint or a publicly accessible hosted OpenAI-compatible model in just a few steps:

1. Set up an evaluation job in W&B Models. Define its benchmarks and configuration, such as whether to generate a leaderboard.
1. Launch the evaluation job.
1. View and analyze the results and leaderboard.

Each time you launch an evaluation job with the same destination project, the project's leaderboard updates automatically.

<Frame>
![Example evalution job leaderboard](/images/models/llm-evaluation-jobs/model-checkpoint-leaderboard-example.png)
</Frame>

## Next steps
- Browse the [Evaluation benchmark catalog](/models/launch/evaluations)
- [Evaluate a model checkpoint](/models/launch/evaluate-model-checkpoint)
- [Evaluate an API-hosted model](/models/launch/evaluate-hosted-model)

## More details

### Pricing
LLM Evaluation Jobs evaluates a model checkpoint or hosted API against popular benchmarks on fully-managed CoreWeave compute, with no infrastructure to manage. You pay only for resources consumed, not idle time. Pricing has two components: compute and storage. Compute is free during public preview, and we will announce pricing at general availability. Stored results include metrics and per-example traces saved in W&B Models runs. Storage is billed monthly based on data volume. Every plan includes some free storage. See [Pricing](https://wandb.ai/site/pricing/) for details.

### Job limits
An individual evaluation job has these limits:
- The maximum size for a model to evaluate is 86 GB, including context.
- Each job is limited to two GPUs.

### Requirements
- To evaluate a model checkpoint, the model weights must be packaged as a VLLM-compatible artifact. See [Example: Prepare a model](/models/launch/evaluate-model-checkpoint#example-prepare-a-model) for details and example code.
- To evaluate an OpenAI-compatible model, it must be accessible at a public URL, and an organization or team admin must configure a team secret with the the API key for authentication.
- Certain benchmarks use OpenAI models for scoring. To run these benchmarks, an organization or team admin must configure a team secret with the API key. See the [Evaluation benchmark catalog](/models/launch/evaluations) to determine whether a benchmark has this requirement.
- Certain benchmarks require access to gated datasets in Hugging Face. To run one of these benchmarks, an organization or team admin must request access to the gated dataset in Hugging face, generate a Hugging Face user access token, and configure it as a team secret. See the [Evaluation benchmark catalog](/models/launch/evaluations) to determine whether a benchmark has this requirement.

For more details and instructions for meeting these requirements, see:
- [Evaluate a model checkpoint](/models/launch/evaluate-hosted-model)
- [Evaluate a hosted API model](/models/launch/evaluate-model-checkpoint)
