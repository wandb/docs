---
description: Evaluate model checkpoints or hosted API models within W&B and analyze the results using automatically generated leaderboards.
title: LLM Evaluation Jobs
---
import PreviewLink from '/snippets/en/_includes/llm-eval-jobs/preview.mdx';

[W&B LLM Evaluation Jobs](/models/launch) is a framework for measuring and analyzing how an LLM model performs, using infrastructure managed by CoreWeave. Run a comprehensive suite of modern, industry-standard [model evaluation benchmarks](/models/launch/evaluations), then view and analyze and share your model's performance over time using automatic leaderboards and charts in W&B. LLM Evaluation Jobs removes the complexity of deploying and maintaining advanced compute infrastructure yourself.

<PreviewLink />

{/*
<CardGroup cols={4}>
<ColabLink url="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb" />
<TryProductLink url="https://wandb.ai/stacey/deep-drive/workspace?workspace=user-lavanyashukla" />
</CardGroup>
*/}

## How it works
Evaluate a model checkpoint or a publicly accessible hosted OpenAI-compatible model in just a few steps:

1. Set up an evaluation job programmatically or in the W&B App, defining the benchmarks to run and whether to generate a leaderboard.
1. Launch the evaluation job.
1. View and analyze the results and leaderboard.

Each time you launch an evaluation job with the same destination project, the leaderboard is updated automatically.

<Frame>
![Example leaderboard](/images/models/llm-evaluation-jobs/model-checkpoint-leaderboard-example.png)
</Frame>

## Next steps
- Learn more about the [available evaluation benchmarks](/models/launch/evaluations)
- [Evaluate a model checkpoint](/models/launch/evaluate-model-checkpoint)
- [Evaluate an API-hosted model](/models/launch/evaluate-hosted-model)

## More details

### Pricing
LLM Evaluation Jobs is in **Preview**. During the preview period, there is no charge.

### Job limits
An individual evaluation job has these limits:
- The maximum size for a model to evaluate is 86 GB, including context.
- Each job is limited to two GPUs.

### Requirements
- To evaluate a model checkpoint, the model weights must be packaged as a VLLM-compatible artifact.
- To evaluate an OpenAI-compatible model, it must be accessible at a public URL, and the API key to authenticate to it must be available as a team secret. An organization admin or team admin can create a team secret.
- Certain benchmarks use OpenAI for scoring. To run these benchmarks the OpenAI API key must be available to as a team secret.
- Certain benchmarks require access to gated datasets in Hugging Face. To run these benchmarks, a Hugging Face user access token with access to the relevant data must be available as a team secret and must request access to the relevant dataset.

For more details about meeting these requirements, see:
- [Evaluate a model checkpoint](/models/launch/evaluate-hosted-model)
- [Evaluate a hosted API model](/models/launch/evaluate-model-checkpoint)
