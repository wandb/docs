---
description: Learn how to use pinned and baseline runs to keep track of important runs and efficiently evaluate model experiments.
title: Pin and compare runs
---
import PinRunsCondensed from '/snippets/en/_includes/pinned-and-baseline-runs/pin-runs-condensed.mdx';

In a workspace with many runs, it can be difficult to keep track of your best performers, production models, failed experiments, or important reference points. The W&B App provides features to help organize and compare runs:

- **Pinned runs**: Pin up to 6 runs to keep them visible in the workspace and at the top of the runs list. If you have a baseline run, you can pin up to 5 runs because the baseline is implicitly pinned.
- **Baseline run**: Specify a baseline run as your reference point for comparisons. The baseline run is always visible in the workspace and at the top of the runs list. In the runs table, summary metric deltas show how each run compares to the baseline. In line plots, the baseline appears with visually distinct styling to help with comparison.

![Line plot with baseline and pinned runs](/images/m
odels/pinned-and-baseline-runs/baseline-run-visual-distinction.png)

These features are particularly useful for:
- Comparing new experiments against your production model.
- Tracking multiple candidate models during experimentation.
- Evaluating whether new runs improve on your best results.

See [Limitations](#limitations).

## Pin runs

<PinRunsCondensed/>

<Frame>
![Runs table with pinned runs](/images/models/pinned-and-baseline-runs/runs-table-with-pinned-runs.png)
</Frame>

To unpin a run, click the pin icon, or follow the procedure to pin the run, but select **Unpin run** instead.

## Manage the baseline run

You can designate one run as the baseline for the workspace to use it as a reference point for evaluating other runs in your workspace.

In the runs selector and runs table, the baseline run appears at the top alongside pinned runs, and has a bookmark icon instead of a pin.

In line plots, lines for the baseline run appear bolder than other lines. When hovering over the plot or legend, the baseline run's line is dashed.

<Frame>
![Demo of comparing another run with the baseline](/images/models/pinned-and-baseline-runs/line-plot-baseline-comparison.png)
</Frame>

### Set a baseline run
To set a baseline run:

1. Navigate to your workspace.
2. In the run selector or runs table, find the run you want to use as your baseline.
3. Click the action `...` menu, then select **Set as baseline**.

The baseline run appears at the top of the run selector, separated from other runs by a visual divider. The baseline run has a bookmark icon instead of a circle.

<Frame>
![Runs table with a baseline run and pinned runs](/images/models/pinned-and-baseline-runs/runs-table-with-pinned-and-baseline-runs.png)
</Frame>

### Change the baseline run
Only one run can be the baseline at a time. To change which run is your baseline:

1. Navigate to your workspace.
2. In the run selector or runs table, find the run you want to use as your new baseline.
3. Click the action `...` menu, then select **Replace baseline**. <Note>If the menu item is inactive, ensure that you have at least one pinning slot available. If necessary, unpin a pinned run by clicking the circular pin icon next to a pinned run.</Note>
4. The new run becomes the baseline, and the previous baseline is automatically pinned so you can find it easily. Optionally, unpin it by clicking its pin icon.

### Remove the baseline designation
To remove the baseline designation:

1. Navigate to your workspace.
2. In the run selector or runs table, find the current baseline run.
3. Click the action `...` menu, then select **Remove baseline**. <Note>If the menu item is inactive, ensure that you have at least one pinning slot available. If necessary, unpin a pinned run by clicking the circular pin icon next to a pinned run.</Note>
4. The previous baseline is automatically pinned so you can find it easily. Optionally, unpin it by clicking its pin icon.

## Compare runs to the baseline
The baseline run is always visible in line plots for metrics the run has logged. In line plots, lines for the baseline run appear bolder than other lines.

- Hover over a part of the plot to display a tooltip with values for all visible runs, including the baseline run and pinned runs.
  <Frame>
  ![Demo showing details for all visible runs at a given point](/images/models/pinned-and-baseline-runs/line-plot-point-details.png)
  </Frame>
- Hover over the baseline run's legend label to display the line prominently. It appears as a heavy dashed line. Lines for other visible runs appear with reduced saturation.
  <Frame>
  ![Demo showing details for the baseline run](/images/models/pinned-and-baseline-runs/line-plot-baseline-run-foreground.png)
  </Frame>
- Hover over another run's legend label to display that run's line prominently and compare it with the baseline, which appears as a heavy dashed line. Lines for other visible runs appear with reduced saturation.
  <Frame>
  ![Demo of comparing another run with the baseline](/images/models/pinned-and-baseline-runs/line-plot-baseline-comparison.png)
  </Frame>

### Summary metric deltas
When a run is set as the baseline, every other run that logs a summary metric that is also logged by the baseline run shows the delta between the baseline value and that run's value. The delta appears to the right of the metric's value in the run's row in the runs table.

By default, the delta is shown with dark gray text on a dark gray background. You can set the **Metric directionality** for a column so that deltas are colored for quick visual reference:

- If the other run **outperforms** the baseline, the delta is shown in dark red text with a light red background.
- If the other run **underperforms** the baseline, the delta is shown in dark teal text with a light teal background.

To set the directionality for a metric:

1. In the runs table, hover over the column heading for the metric.
2. Click the `...` action menu that appears.
3. Set **Metric directionality** to **Higher values are best** or **Lower values are best**.

The following screenshot shows how the runs `nanochat-train-base` and `nanochat-train-mid` compare with the baseline run `nanochat-train`. Delta metrics are shown for `TOTAL_TRAINING_TIME`, `TRAIN/DT`, AND `TRAIN/GRAD_NORM`.
![Screenshot comparing summary metric deltas from the baseline run](/images/models/pinned-and-baseline-runs/baseline_run_deltas.png)


## Use cases
This section describes some scenarios where pinned and baseline runs can help guide your experiments.

- **Track production models**: Ensure that new models meet your quality bar before deployment.
  1. Set your production model as the baseline.
  2. Compare all experiments against your deployed model to identify candidates that outperform production.
- **Compare hyperparameter experiments**: Evaluate hyperparameter sweeps or manual experiments against your best-known configuration.
  1. Set your best known configuration as the baseline.
  2. Pin promising candidates as you discover them.
  3. Use the line plots to visually compare runs against the baseline.
  4. Continue to update the baseline as you find better configurations.

## Example workflow
This section illustrates how pinned and baseline runs can help you to compare runs.

1. Run this example code, which simulates a hyperparameter-tuning scenario with a series of runs. Replace placeholders surrounded with angle brackets (`<>`) with your own values.

    ```python
    import wandb
    import random
    import math

    def train_model(learning_rate, batch_size, run_name, tags=None):
        """Simulate training a model with given hyperparameters."""
        config = {
            "learning_rate": learning_rate,
            "batch_size": batch_size,
            "optimizer": "adam",
            "architecture": "resnet50"
        }
        
        with wandb.init(
          # Replace with your team and project name
            project="hyperparameter-tuning",
            entity="<team>",
            name=run_name,
            config=config,
            tags=tags or []
        ) as run:
            # Simulate training loop
            for epoch in range(50):
                # Simulated metrics
                accuracy = 0.6 + 0.3 * (1 - math.exp(-learning_rate * epoch / 10))
                loss = 1.0 * math.exp(-learning_rate * epoch / 10)
                
                run.log({
                    "epoch": epoch,
                    "accuracy": accuracy,
                    "loss": loss
                })

    # Create baseline run with standard configuration
    train_model(
        learning_rate=0.001,
        batch_size=64,
        run_name="baseline-config",
        tags=["baseline", "production"]
    )

    # Experiment with different learning rates
    train_model(
        learning_rate=0.003,
        batch_size=64,
        run_name="lr-experiment-0.003",
        tags=["experiment"]
    )

    train_model(
        learning_rate=0.0001,
        batch_size=64,
        run_name="lr-experiment-0.0001",
        tags=["experiment"]
    )
    ```

    After running this code, your workspace has three runs.
2. Set `baseline-config` as your baseline run.
3. Pin `baseline-config` to keep it visible.
4. Compare the experiment runs against the baseline: in the runs table, use the summary metric deltas next to each run's values; in the workspace, use the line plots.
5. Pin promising experiments for further investigation. In this example, after 50 epochs, `lr-experiment-0.003` has the highest accuracy (`~0.64`) and the lowest loss (`~0.86`).

{/* TODO screenshot */}

## Limitations

The following features are not yet supported for pinned and baseline runs:

- **Grouping**: When [viewing runs](/models/runs#view-logged-runs) in the run selector or runs table, if runs are grouped by a column, pinned and baseline runs are not visually distinct from other runs.
- **Reports**: In a run set in a [W&B Report](/models/reports), pinned and baseline runs are not visually distinct from other runs.
- **Workspace view only**: The baseline does not appear when viewing a single run's workspace.
- **Line plots only**: Baseline comparison is available only for line plots, and is not yet available for other panels such as bar plots or media panels.
