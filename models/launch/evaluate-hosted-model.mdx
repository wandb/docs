---
description: "Evaluate a hosted API model using infrastructure managed by CoreWeave"
title: "Evaluate a hosted API model"
---
import ReviewEvaluationResults from "/snippets/_includes/llm-eval-jobs/review-evaluation-results.mdx";
import RerunEvaluation from "/snippets/_includes/llm-eval-jobs/rerun-evaluation.mdx";
import ExportEvaluation from "/snippets/_includes/llm-eval-jobs/export-evaluation.mdx";
import PreviewLink from '/snippets/_includes/llm-eval-jobs/preview.mdx';

<PreviewLink />

This page shows how to use [LLM Evaluation Jobs](/models/launch) to run a series of evaluation benchmarks on a hosted API model at a publicly-accessible URL, using infrastructure managed by CoreWeave. To evaluate a model checkpoint saved as an artifact in W&B Models, see [Evaluate a model checkpoint](/models/launch/evaluate-model-checkpoint) instead.

## Prerequisites
1. Review the [requirements and limitations](/models/launch#more-details) for LLM Evaluation Jobs.
1. To run certain benchmarks, a team admin must add the required API keys as team-scoped secrets. Any team member can specify the secret when configuring an evaluation job.
    - An **OpenAPI API key**: Used by benchmarks that use OpenAI models for scoring. Required if the field **Scorer API key** appears after you select a benchmark. The secret must be named `OPENAI_API_KEY`.
    - A **Hugging Face user access token**: Required for certain benchmarks like `lingoly` and `lingoly2` that require access to one or more gated Hugging Face datasets. Required if the field **Hugging Face Token** appears after selecting a benchmark. The API key must have access to the relevant dataset. See the Hugging Face documentation for [User access tokens](https://huggingface.co/docs/hub/en/security-tokens) and [accessing gated datasets](https://huggingface.co/docs/hub/en/datasets-gated#access-gated-datasets-as-a-user).
    - To evaluate a model provided by [W&B Inference](/inference), an organization or team admin must create `WANDB_API_KEY` with any value. The secret is not actually used for authentication. 
1. The model to evaluate must be available at a publicly accessible URL. An organization or team admin must create a team-scoped secret with the API key for authentication.
1. Create a new [W&B project](/models/track/project-page) for the evaluation results. From the project sidebar, click **Create new project**.
1. Review the documentation for a given benchmark to understand how it works and learn about specific requirements. For convenience, the [Available evaluation benchmarks](/models/launch/evaluations) reference includes relevant links.

## Evaluate your model
Follow these steps to set up and launch an evaluation job:

1. Log in to W&B, then click **Launch** in the project sidebar. The **LLM Evaluation Jobs** page displays.
1. Click **Evaluate hosted API model** to set up the evaluation.
1. Select a destination project to save the evaluation results to.
1. In the **Model** section, specify the base URL and model name to evaluate, and select the API key to use for authentication. Provide the model name in OpenAI-compatible format defined by the [AI Security Institute](https://inspect.aisi.org.uk/providers.html#openai-api). For example, specify an OpenAI mode in the following syntax: `openai/<model-name>`. For a comprehensive list of hosted model providers and models, see [AI Security Institute's model provider reference](https://inspect.aisi.org.uk/providers.html).
      - To evaluate a model provided by [W&B Inference](/inference), set the base URL to `https://api.inference.wandb.ai/v1` and specify the model name in the following syntax: `openai-api/wandb/<model_id>`. Refer to the [Inference model catalog](/inference/models) for details.
      - To use the [OpenRouter](https://inspect.aisi.org.uk/providers.html#openrouter) provider, prefix the model name with `openrouter` in the following syntax: `openrouter/<model-name>`.
      - To evaluate a custom OpenAPI-compliant model, specify the model name in the following syntax: `openai-api/wandb/<model-name>`.
1. Click **Select evaluations**, then select up to four benchmarks to run.
1. If you select benchmarks that use OpenAI models for scoring, the **Scorer API key** field displays. Click it, then select the `OPENAI_API_KEY` secret. For convenience, a team admin can create a secret from this drawer by clicking **Create secret**.
1. If you select benchmarks that require access to gated datasets in Hugging Face, a **Hugging Face token** field displays. [Request access to the relevant dataset](https://huggingface.co/docs/hub/en/datasets-gated#access-gated-datasets-as-a-user), then select the secret that contains the Hugging Face user access token.
1. Optionally, set **Sample limit** to a positive integer to limit the maximum number of benchmark samples to evaluate. Otherwise, all samples in the task are included.
1. To create a leaderboard automatically, click **Publish results to leaderboard**. The leaderboard will display all evaluations together in a workspace panel, and you can also share it in a report.
1. Click **Launch** to launch the evaluation job.
1. Click the circular arrow icon at the top of the page to open the recent run modal. Evaluation jobs appear with your other recent runs. Click the name of a finished run to open it in single-run view, or click the **Leaderboard** link to open the leaderboard directly. For details, see [View the results](#view-the-results).

This example job runs the `simpleqa` benchmark against the OpenAI model `o4-mini`:

<Frame>
![Example hosted model evaluation job](/images/models/llm-evaluation-jobs/hosted-model-job-example.png)
</Frame>

This example leaderboard visualizes the performance of several OpenAI models together:

<Frame>
![Example leaderboard visualizing the performance of several hosted models](/images/models/llm-evaluation-jobs/hosted-model-leaderboard-example.png)
</Frame>

<ReviewEvaluationResults />

<RerunEvaluation />

<ExportEvaluation />
