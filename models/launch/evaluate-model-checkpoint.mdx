---
description: "Evaluate a VLLM-compatible model checkpoint using infrastructure managed by CoreWeave"
title: "Evaluate a model checkpoint"
---
import ReviewEvaluationResults from "/snippets/en/_includes/llm-eval-jobs/review-evaluation-results.mdx";
import RerunEvaluation from "/snippets/en/_includes/llm-eval-jobs/rerun-evaluation.mdx";
import ExportEvaluation from "/snippets/en/_includes/llm-eval-jobs/export-evaluation.mdx";

This page shows how to use [W&B LLM Evaluation Jobs](/models/launch) to run a series of evaluation benchmarks on a model checkpoint using infrastructure managed by CoreWeave. To evaluate a hosted API model instead, see [Evaluate an API-hosted model](/models/launch/evaluate-hosted-model) instead.

## Prerequisites
1. Review the [requirements and limitations](/models/launch#more-details) for LLM Evaluation Jobs.
1. To run certain benchmarks, a team admin must add the required API keys as [team-scoped secrets](/platform/secrets#add-a-secret). Any team member can specify the secret when configuring an evaluation job.
    - **OpenAPI API key**: Used by many benchmarks for scoring. Required if the field **Scorer API key** appears after you select a benchmark.
    - **Hugging Face Dataset token**: Used by benchmarks like `lingoly` and `lingoly2` to access Hugging Face data. Required if the field **Hugging Face Token** appears after selecting a benchmark. See the Hugging Face documentation for [User access tokens](https://huggingface.co/docs/hub/en/security-tokens).
1. Create a new [W&B project](/models/track/project-page) for the evaluation results. From the left navigation, click **Create new project**.
1. Package the model in VLLM-compatible format and save it as an artifact in W&B. An attempt to benchmark any other type of artifact will fail. See [Example: Prepare a model](#example-prepare-your-model) for one approach.
1. Review the documentation for a given benchmark to understand how it works and learn about specific requirements. For convenience, the [Available evaluation benchmarks](/models/launch/evaluations) reference includes relevant links.

## Evaluate your model
Follow these steps to set up and launch an evaluation:

1. Log in to W&B, then click **Launch** in the left navigation. The **LLM Evaluation Jobs** page displays.
1. Click **Evaluate model checkpoint** to set up the evaluation.
1. Select a destination project where the evaluation run and results will be saved.
1. In the **Model artifact** section, specify the project, artifact, and version of the prepared model to evaluate.
1. Click **Evaluations**, then select up to six benchmarks to run.
1. If you select one or more benchmarks that require an OpenAI API key, the **Scorer API key** field displays. Click it, then select the OpenAI API key's secret by name. For convenience, a team admin can create a secret from this drawer by clicking **Create secret**.
1. Optionally, set **Sample limit** to a positive integer to limit the maximum number of benchmark samples to evaluate. Otherwise, all samples in the task are included.
1. To create a leaderboard automatically, click **Publish results to leaderboard**. The leaderboard will display all evaluations together in a workspace panel, and you can also share it in a report.
1. Click **Launch** to add the evaluation to the job queue.
1. To track the status of the evaluation job, click the the circular arrows at the top of the page. Click the name of a finished run to open it in the project, where you can view the results, leaderboard, configuration, and logs. For details, see [View the results](#view-the-results).

<Tip>
After you evaluate your first model, many fields are pre-filled with the most recent values when you configure your next evaluation job.
</Tip>

This example job runs two benchmarks against an artifact:
![Example model checkpoint evaluation job](/images/models/llm-evaluation-jobs/model-checkpoint-job-example.png)

This example leaderboard visualizes the performance of several models together:
![Example leaderboard visualizing the performance of several models against several benchmark tasks](/images/models/llm-evaluation-jobs/model-checkpoint-leaderboard-example.png)

<ReviewEvaluationResults />

<RerunEvaluation />

<ExportEvaluation />

## Example: Prepare a model

To prepare your model, you load it in W&B, package the model weights in VLLM-compatible format, and save the result. This example shows one way to do this:

```python
import os
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load your model
model_name = "your-model-name"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Save in vLLM-compatible format
save_dir = "path/to/save"
tokenizer.save_pretrained(save_dir)
model.save_pretrained(save_dir)

# Save to W&B
import wandb
wandb_run = wandb.init(entity="your-entity-name", project="your-project-name")
artifact = wandb.Artifact(name="your-artifact-name")
artifact.add_dir(save_dir)
logged_artifact = wandb_run.log_artifact(artifact)
logged_artifact.wait()
wandb.finish()
```
