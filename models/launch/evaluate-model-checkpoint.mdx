---
description: "Evaluate a VLLM-compatible model checkpoint using infrastructure managed by CoreWeave"
title: "Evaluate a model checkpoint"
---
import ReviewEvaluationResults from "/snippets/en/_includes/llm-eval-jobs/review-evaluation-results.mdx";
import RerunEvaluation from "/snippets/en/_includes/llm-eval-jobs/rerun-evaluation.mdx";
import ExportEvaluation from "/snippets/en/_includes/llm-eval-jobs/export-evaluation.mdx";

This page shows how to use [W&B LLM Evaluation Jobs](/models/launch) to run a series of evaluation benchmarks on a model checkpoint using infrastructure managed by CoreWeave. To evaluate a hosted API model instead, see [Evaluate an API-hosted model](/models/launch/evaluate-hosted-model) instead.

## Prerequisites
1. Review the [requirements and limitations](/models/launch#more-details) for LLM Evaluation Jobs.
1. A team admin must add an OpenAI API key as a [team-scoped secret](/platform/secrets#add-a-secret). This is required by many benchmarks for scoring. Any team member can specify the secret when configuring an evaluation job.
1. Review the documentation for a given benchmark to understand how it works and learn about specific requirements. For convenience, the [Available evaluation benchmarks](/models/launch/evaluations) reference includes relevant links.
1. Package the model in VLLM-compatible format and save it as an artifact in W&B. An attempt to benchmark any other type of artifact will fail. See [Example: Prepare a model](#example-prepare-your-model) for one approach.

## Evaluate your model
Follow these steps to set up and launch an evaluation:

1. Log in to W&B, then click **Launch** in the left navigation. The **LLM Evaluation Jobs** page displays.
1. Click **Evaluate model checkpoint** to set up the evaluation.
1. Select a destination project where the evaluation run and results will be saved.
1. In the **Model artifact** section, specify the project, artifact, and version of the prepared model to evaluate.
1. Click **Evaluations**, then select up to six benchmarks to run.
1. If you select one or more benchmarks that require an OpenAI API key, the **Scorer API key** field displays. Click it, then select the OpenAI API key's secret by name. For convenience, a team admin can create a secret from this drawer by clicking **Create secret**.
1. Optionally, limit the maximum number of samples to evaluate by setting **Sample limit** to an integer greater than 0. Otherwise, all samples in the task are included.
1. To create a leaderboard automatically, click **Publish results to leaderboard**. It will display as a panel in the workspace and you can also add it to a report.
1. Click **Launch** to add the evaluation run to the job queue.

This example runs two benchmarks against an artifact.

![Example model checkpoint evaluation job](/images/models/llm-evaluation-jobs/evaluate-model-checkpoint-example.png)

The run's logs and artifacts are saved in the destination project you selected. To view the status of recent evaluations, open the queue by clicking the circular arrows at the top of the page. For running or finished jobs, click a run name to [review the results](#review-the-results).

To open the run in the destination workspace automatically when it finishes, select **Open automatically after launch** and leave the run queue open.

<Tip>
After you evaluate your first model, the model artifact details and the selected benchmarks you select are automatically set to the last values you used.
</Tip>

<ReviewEvaluationResults />

<RerunEvaluation />

<ExportEvaluation />

## Example: Prepare a model
To prepare your model, you load it in W&B, package the model weights in VLLM-compatible format, and save the result. This example shows one way to do this:

```python
import os
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load your model
model_name = "your-model-name"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Save in vLLM-compatible format
save_dir = "path/to/save"
tokenizer.save_pretrained(save_dir)
model.save_pretrained(save_dir)

# Save to W&B
import wandb
wandb_run = wandb.init(entity="your-entity-name", project="your-project-name")
artifact = wandb.Artifact(name="your-artifact-name")
artifact.add_dir(save_dir)
logged_artifact = wandb_run.log_artifact(artifact)
logged_artifact.wait()
wandb.finish()
```
