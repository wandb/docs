---
title: ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ì¶”ì í•˜ê¸°
menu:
  tutorials:
    identifier: ko-tutorials-artifacts
weight: 4
---

{{< cta-button colabLink="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W&B_Artifacts.ipynb" >}}
ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” W&B Artifacts ë¥¼ ì‚¬ìš©í•´ ML ì‹¤í—˜ íŒŒì´í”„ë¼ì¸ì„ ì¶”ì í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ë“œë¦´ê²Œìš”.

[ë¹„ë””ì˜¤ íŠœí† ë¦¬ì–¼](https://tiny.cc/wb-artifacts-video)ë„ í•¨ê»˜ ì°¸ê³ í•´ì£¼ì„¸ìš”.

## ì•„í‹°íŒ©íŠ¸(Artifacts)ë€?

ì•„í‹°íŒ©íŠ¸(artifact)ëŠ” ê·¸ë¦¬ìŠ¤ì˜ [ì•°í¬ë¼](https://en.wikipedia.org/wiki/Amphora)ì²˜ëŸ¼ ì–´ë–¤ í”„ë¡œì„¸ìŠ¤ì˜ ê²°ê³¼ë¬¼, ì¦‰ ìƒì„±ëœ ì˜¤ë¸Œì íŠ¸ì…ë‹ˆë‹¤.  
MLì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ artifact ëŠ” _ë°ì´í„°ì…‹_ ê³¼ _ëª¨ë¸_ ì…ë‹ˆë‹¤.

ê·¸ë¦¬ê³ , [ì½”ë¡œë‚˜ë„ì˜ ì‹­ìê°€](https://indianajones.fandom.com/wiki/Cross_of_Coronado)ì²˜ëŸ¼ ì´ëŸ¬í•œ ì¤‘ìš”í•œ artifact ë“¤ì€ ë§ˆì¹˜ ë°•ë¬¼ê´€ì— ì†Œì¥ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.  
ì¦‰, ì—¬ëŸ¬ë¶„ê³¼ íŒ€, ì „ì²´ ML ì»¤ë®¤ë‹ˆí‹°ê°€ ì´ë¡œë¶€í„° ë” ë§ì€ ê²ƒì„ ë°°ìš¸ ìˆ˜ ìˆë„ë¡ ì˜ ì •ë¦¬í•˜ê³  ê´€ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.  
íŠ¸ë ˆì´ë‹ ê³¼ì •ì„ ì¶”ì í•˜ì§€ ì•Šìœ¼ë©´, ê°™ì€ ì‹¤ìˆ˜ë¥¼ ë°˜ë³µí•˜ê¸° ë§ˆë ¨ì´ë‹ˆê¹Œìš”!

Artifiacts APIë¥¼ í™œìš©í•˜ë©´, W&Bì˜ `Run`ì—ì„œ ìƒì„±ëœ ê²°ê³¼ë¬¼ì„ `Artifact`ë¡œ ê¸°ë¡í•  ìˆ˜ ìˆê³ , ë‹¤ë¥¸ `Run`ì—ì„œ ì…ë ¥ê°’ìœ¼ë¡œ í•´ë‹¹ `Artifact`ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.  
ì•„ë˜ ë‹¤ì´ì–´ê·¸ë¨ì²˜ëŸ¼, íŠ¸ë ˆì´ë‹ run ì´ ë°ì´í„°ì…‹ì„ ì…ë ¥ ë°›ì•„ ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” í˜•íƒœì…ë‹ˆë‹¤.
 
 {{< img src="/images/tutorials/artifacts-diagram.png" alt="Artifacts workflow diagram" >}}

í•˜ë‚˜ì˜ runì—ì„œ ë‚˜ì˜¨ outputì„ ë‹¤ë¥¸ runì—ì„œ inputìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, `Artifact`ì™€ `Run`ì€ í•¨ê»˜ ë°©í–¥ì„± ìˆëŠ” ê·¸ë˜í”„(ì–‘ë¶„ [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph))ë¥¼ í˜•ì„±í•˜ê²Œ ë©ë‹ˆë‹¤.  
ë…¸ë“œëŠ” ê°ê° `Artifact`ì™€ `Run`ì´ê³ , í™”ì‚´í‘œë¡œ ì–´ë–¤ `Run`ì´ ì–´ë–¤ `Artifact`ë¥¼ ìƒì„±/ì†Œë¹„í•˜ëŠ”ì§€ í‘œí˜„í•©ë‹ˆë‹¤.

## Artifactsë¡œ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ì¶”ì í•˜ê¸°

### ì„¤ì¹˜ ë° ì„í¬íŠ¸

Artifacts ëŠ” wandb Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `0.9.2` ë²„ì „ë¶€í„° ì§€ì›ë©ë‹ˆë‹¤.

ì£¼ìš” ML Python ìƒíƒœê³„ì™€ ê°™ì´, `pip`ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# wandb 0.9.2+ ë²„ì „ì—ì„œ ë™ì‘í•©ë‹ˆë‹¤
!pip install wandb -qqq
!apt install tree
```

```python
import os
import wandb
```

### ë°ì´í„°ì…‹ ê¸°ë¡(Log a Dataset)

ì´ì œ Artifacts ë¥¼ ì •ì˜í•´ë´…ì‹œë‹¤.

ì•„ë˜ ì˜ˆì œëŠ” PyTorchì˜
["Basic MNIST Example"](https://github.com/pytorch/examples/tree/master/mnist/) ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.  
ë¬¼ë¡  [TensorFlow](https://wandb.me/artifacts-colab)ë‚˜ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ ë˜ëŠ” pure Python ìœ¼ë¡œë„ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

ë°ì´í„°ì…‹ì„ ë¨¼ì € ì¤€ë¹„í•©ë‹ˆë‹¤:
- íŒŒë¼ë¯¸í„°(ëª¨ë¸ íŒŒë¼ë¯¸í„°) ì„ íƒì— ì‚¬ìš©í•˜ëŠ” `train` ì„¸íŠ¸,
- í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìœ„í•´ ì‚¬ìš©í•˜ëŠ” `validation` ì„¸íŠ¸,
- ìµœì¢… ëª¨ë¸ í‰ê°€ì— ì‚¬ìš©í•˜ëŠ” `test` ì„¸íŠ¸

ì•„ë˜ ì²« ë²ˆì§¸ ì…€ì—ì„œ ì´ ì„¸ ê°€ì§€ ë°ì´í„°ì…‹ì„ ì •ì˜í•©ë‹ˆë‹¤.

```python
import random 

import torch
import torchvision
from torch.utils.data import TensorDataset
from tqdm.auto import tqdm

# ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ ê²°ì •ë¡ ì ìœ¼ë¡œ ë§Œë“¦
torch.backends.cudnn.deterministic = True
random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# ë°ì´í„° íŒŒë¼ë¯¸í„°
num_classes = 10
input_shape = (1, 28, 28)

# MNIST ë¯¸ëŸ¬ ì‚¬ì´íŠ¸ ì¤‘ ëŠë¦° ê³³ ì œì™¸
torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors
                                      if not mirror.startswith("http://yann.lecun.com")]

def load(train_size=50_000):
    """
    # ë°ì´í„° ë¡œë“œ
    """

    # train/test ì„¸íŠ¸ë¡œ ë°ì´í„° ë¶„í• 
    train = torchvision.datasets.MNIST("./", train=True, download=True)
    test = torchvision.datasets.MNIST("./", train=False, download=True)
    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìœ„í•´ validation ì„¸íŠ¸ ë¶„ë¦¬
    x_train, x_val = x_train[:train_size], x_train[train_size:]
    y_train, y_val = y_train[:train_size], y_train[train_size:]

    training_set = TensorDataset(x_train, y_train)
    validation_set = TensorDataset(x_val, y_val)
    test_set = TensorDataset(x_test, y_test)

    datasets = [training_set, validation_set, test_set]

    return datasets
```

ì´ì œ 'ë°ì´í„°ë¥¼ ê¸°ë¡í•˜ëŠ” ì½”ë“œ'ëŠ” 'ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œ'ë¥¼ ê°ì‹¸ëŠ” í˜•íƒœê°€ ë©ë‹ˆë‹¤.  
ì¦‰, ë°ì´í„°ë¥¼ `load`í•˜ëŠ” ì½”ë“œëŠ” ë”°ë¡œ ë¶„ë¦¬í•˜ê³ ,  
ì´ë¥¼ ê¸°ë¡í•˜ëŠ” `load_and_log` í•¨ìˆ˜ëŠ” ê·¸ ë°”ê¹¥ì—ì„œ ë°ì´í„°ë¥¼ ë°›ì•„ ê¸°ë¡í•©ë‹ˆë‹¤.

ì´ ë°©ì‹ì´ ì¢‹ì€ ìŠµê´€ì…ë‹ˆë‹¤.

ì´ì œ ì´ ë°ì´í„°ì…‹ë“¤ì„ Artifacts ë¡œ ê¸°ë¡í•˜ë ¤ë©´,
1.  `wandb.init()`ë¡œ `Run`ì„ ìƒì„±í•˜ê³  (L4)
2.  ë°ì´í„°ì…‹ì„ ìœ„í•œ `Artifact`ë¥¼ ë§Œë“¤ê³  (L10)
3.  ì—°ê´€ëœ íŒŒì¼ì„ ì €ì¥í•˜ê³  ê¸°ë¡í•©ë‹ˆë‹¤ (L20, L23)

ì•„ë˜ ì˜ˆì œ ì½”ë“œë¥¼ ë³´ê³ ,  
ë” ìì„¸í•œ ì„¤ëª…ì€ ë‹¤ìŒ ì„¹ì…˜ë“¤ì„ í¼ì³ë³´ì„¸ìš”.

```python
def load_and_log():

    # Run ìƒì„±, íƒ€ì…(job_type) ì§€ì • ë° í”„ë¡œì íŠ¸ ì„¤ì •
    with wandb.init(project="artifacts-example", job_type="load-data") as run:
        
        datasets = load()  # ë°ì´í„°ì…‹ ë¡œë“œ
        names = ["training", "validation", "test"]

        # ğŸº Artifact ìƒì„±
        raw_data = wandb.Artifact(
            "mnist-raw", type="dataset",
            description="Raw MNIST dataset, split into train/val/test",
            metadata={"source": "torchvision.datasets.MNIST",
                      "sizes": [len(dataset) for dataset in datasets]})

        for name, data in zip(names, datasets):
            # ğŸ£ Artifactì— ìƒˆ íŒŒì¼ì„ ì €ì¥í•˜ê³ , ë‚´ìš©ì„ ì“´ë‹¤.
            with raw_data.new_file(name + ".pt", mode="wb") as file:
                x, y = data.tensors
                torch.save((x, y), file)

        # âœï¸ Artifactë¥¼ W&Bì— ì €ì¥
        run.log_artifact(raw_data)

load_and_log()
```

#### `wandb.init()`

`Artifact`ë¥¼ ìƒì„±í•  `Run`ì„ ì‹œì‘í•  ë•ŒëŠ”  
ì–´ë–¤ `project`ì— ì†Œì†ë  ê±´ì§€ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.

ì›Œí¬í”Œë¡œìš°ì— ë”°ë¼  
í”„ë¡œì íŠ¸ì˜ ë²”ìœ„ëŠ” 'car-that-drives-itself'ì²˜ëŸ¼ í¬ê±°ë‚˜  
'iterative-architecture-experiment-117'ì²˜ëŸ¼ ì‘ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤**: ê°™ì€ `Artifact`ë¥¼ ê³µìœ í•˜ëŠ” ëª¨ë“  `Run`ì€ í•œ í”„ë¡œì íŠ¸ì— ë‘ì„¸ìš”. ê´€ë¦¬ê°€ ì‰¬ì›Œì§‘ë‹ˆë‹¤.  
> í•˜ì§€ë§Œ `Artifact`ëŠ” í”„ë¡œì íŠ¸ ê°„ ì´ë™ë„ ê°€ëŠ¥í•˜ë‹ˆ ë„ˆë¬´ ê±±ì •í•˜ì§€ ë§ˆì„¸ìš”.

ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì‘ì—…ì„ êµ¬ë¶„í•´ì„œ ì¶”ì í•  ìˆ˜ ìˆë„ë¡,  
`Run`ì„ ë§Œë“¤ ë•Œ `job_type`ì„ ì§€ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.  
ì´ë ‡ê²Œ í•˜ë©´ Artifacts ê·¸ë˜í”„ê°€ êµ°ë”ë”ê¸° ì—†ì´ ê¹”ë”í•´ì§‘ë‹ˆë‹¤.

> **ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤**: `job_type`ì€ íŒŒì´í”„ë¼ì¸ì˜ ë‹¨ê³„ë¥¼ ì˜ ì„¤ëª…í•˜ê³ , í•œ ë‹¨ê³„ë§Œ ë‚˜íƒ€ë‚´ë„ë¡ ì‘ì„±í•˜ì„¸ìš”.  
> ì—¬ê¸°ì„œëŠ” ë°ì´í„°ë¥¼ `load`í•˜ëŠ” ê²ƒê³¼ `preprocess`í•˜ëŠ” ê²ƒì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.

#### `wandb.Artifact`

ë¬´ì–¸ê°€ë¥¼ `Artifact`ë¡œ ê¸°ë¡í•˜ë ¤ë©´  
ìš°ì„  `Artifact` ì˜¤ë¸Œì íŠ¸ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

ëª¨ë“  `Artifact`ëŠ” `name`ì„ ê°€ì§‘ë‹ˆë‹¤ â€” ì²« ë²ˆì§¸ ì¸ìˆ˜ê°€ ê·¸ ì—­í• ì„ í•©ë‹ˆë‹¤.

> **ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤**: `name`ì€ ì§ê´€ì ì´ê³  íƒ€ì´í•‘í•˜ê¸° í¸í•˜ë„ë¡ í•˜ì´í”ˆ(-)ìœ¼ë¡œ êµ¬ë¶„í•˜ë©°, ì½”ë“œ ë‚´ ë³€ìˆ˜ëª…ê³¼ ëŒ€ì‘ë˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.

ê·¸ë¦¬ê³  `type`ë„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.  
`Run`ì˜ `job_type`ì²˜ëŸ¼,  
ì „ì²´ ì‹¤í—˜ì˜ `Run`ê³¼ `Artifact` ê·¸ë˜í”„ë¥¼ êµ¬ì¡°í™”í•˜ëŠ” ë° ì“°ì…ë‹ˆë‹¤.

> **ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤**: `type`ì€ ê°„ë‹¨í•˜ê²Œ!  
> `dataset`ì´ë‚˜ `model`ì²˜ëŸ¼ ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ê³ ,  
> `mnist-data-YYYYMMDD`ì²˜ëŸ¼ ë„ˆë¬´ êµ¬ì²´ì ì´ì§€ ì•Šì•„ë„ ì¢‹ìŠµë‹ˆë‹¤.

`description`ê³¼ ì¶”ê°€ì ì¸ `metadata`ë„ ë”•ì…”ë„ˆë¦¬ë¡œ ë‹¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
ì—¬ê¸°ì„œ `metadata`ëŠ” JSONìœ¼ë¡œ ì§ë ¬í™” ê°€ëŠ¥í•œ ê°’ì´ë©´ ë©ë‹ˆë‹¤.

> **ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤**: `metadata`ì—ëŠ” ê°€ëŠ¥í•œ í•œ ìì„¸í•œ ì •ë³´ë¥¼ ë„£ìœ¼ì„¸ìš”.

#### `artifact.new_file` ë° `run.log_artifact()`

`Artifact` ì˜¤ë¸Œì íŠ¸ë¥¼ ë§Œë“¤ì—ˆìœ¼ë©´, ì—¬ê¸°ì— íŒŒì¼ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

ì •í™•íˆ ë§ì”€ë“œë¦¬ë©´, _íŒŒì¼ë“¤_ ì…ë‹ˆë‹¤.  
`Artifact`ëŠ” ë””ë ‰í† ë¦¬ì²˜ëŸ¼ êµ¬ì¡°í™”ë˜ì–´ ìˆê³   
ì—¬ëŸ¬ íŒŒì¼ê³¼ ì„œë¸Œë””ë ‰í† ë¦¬ë¥¼ ë‹´ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤**: ê°€ëŠ¥í•˜ë‹¤ë©´ `Artifact` ë‚´ìš©ì„ ì—¬ëŸ¬ íŒŒì¼ë¡œ ë‚˜ëˆ  ê´€ë¦¬í•˜ì„¸ìš”.  
> í™•ì¥ì„±ì„ ìœ„í•´ ê¶Œì¥í•©ë‹ˆë‹¤.

`new_file` ë©”ì†Œë“œëŠ” íŒŒì¼ ì‘ì„±ê³¼ ë™ì‹œì—  
Artifact ì— íŒŒì¼ì„ ë¶€ì°©í•©ë‹ˆë‹¤.  
ì•„ë˜ì— ë‚˜ì˜¤ëŠ” `add_file` ë©”ì†Œë“œëŠ” ë‘ ë‹¨ê³„ë¥¼ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ëª¨ë“  íŒŒì¼ì„ ì¶”ê°€í–ˆë‹¤ë©´, ë§ˆì§€ë§‰ìœ¼ë¡œ [wandb.ai](https://wandb.ai) ì— `log_artifact` í•©ë‹ˆë‹¤.

ì‹¤í–‰ ê²°ê³¼ë¥¼ ë³´ë©´,  
`Run` í˜ì´ì§€ë¡œ ì—°ê²°ë˜ëŠ” URLì´ ë‚˜ì˜¤ê³   
ì—¬ê¸°ì„œ ê¸°ë¡ëœ `Artifact`ë„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì•„ë˜ì—ì„œ Run í˜ì´ì§€ì˜ ë‹¤ë¥¸ ì˜ì—­ë„ í™œìš©í•˜ëŠ” ì˜ˆì‹œë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.

### ê¸°ë¡ëœ Dataset Artifact í™œìš©í•˜ê¸°

W&Bì˜ `Artifact`ëŠ” ë°•ë¬¼ê´€ì— ì „ì‹œëœ ê²ƒê³¼ ë‹¬ë¦¬  
_ì‹¤ì œë¡œ ì‚¬ìš©_í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ ê²ƒì…ë‹ˆë‹¤.  
ì €ì¥ë§Œ í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ ì§ì ‘ í™œìš©í•  ìˆ˜ ìˆì–´ìš”.

ì•„ë˜ ì…€ì—ì„œëŠ” íŒŒì´í”„ë¼ì¸ì˜ í•œ ë‹¨ê³„ë¡œ  
raw ë°ì´í„°ì…‹ì„ ì…ë ¥ë°›ì•„ ì´ë¥¼  
ì „ì²˜ë¦¬(`preprocess`)í•´ì„œ ì˜¬ë°”ë¥¸ í˜•íƒœë¡œ ë³€ê²½í•©ë‹ˆë‹¤.

ë§ˆì°¬ê°€ì§€ë¡œ, í•µì‹¬ êµ¬í˜„ í•¨ìˆ˜ `preprocess`ì™€  
wandb ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•œ ì½”ë“œë¥¼ ë¶„ë¦¬í•˜ëŠ” íŒ¨í„´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
def preprocess(dataset, normalize=True, expand_dims=True):
    """
    ## ë°ì´í„° ì¤€ë¹„
    """
    x, y = dataset.tensors

    if normalize:
        # ì´ë¯¸ì§€ë¥¼ [0, 1] ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ ì¡°ì •
        x = x.type(torch.float32) / 255

    if expand_dims:
        # ì´ë¯¸ì§€ ëª¨ì–‘ì„ (1, 28, 28)ë¡œ ë§ì¶”ê¸°
        x = torch.unsqueeze(x, 1)
    
    return TensorDataset(x, y)
```

ì´ë²ˆì—” ì´ `preprocess` ë‹¨ê³„ë¥¼ wandb.Artifact ë¡œ ê¸°ë¡í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œëŠ”  
ê¸°ì¡´ ë‹¨ê³„ì™€ ë‹¬ë¦¬  
`Artifact`ë¥¼ `use`í•´ì„œ ì…ë ¥ìœ¼ë¡œ ì“°ê³ ,  
(`use`ëŠ” ìƒˆ ê¸°ëŠ¥!)  
`log`í•˜ëŠ” ë¶€ë¶„ì€ ì´ì „ ë‹¨ê³„ì™€ ê°™ìŠµë‹ˆë‹¤.  
ì¦‰, `Artifact`ëŠ” `Run`ì˜ ì…ë ¥ë„ ë  ìˆ˜ ìˆê³  ì¶œë ¥ë„ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì—¬ê¸°ì„œëŠ” ìƒˆë¡œìš´ `job_type`ì¸ `preprocess-data`ë¥¼ ì‚¬ìš©í•´  
ì´ ë‹¨ê³„ê°€ ì´ì „ê³¼ ë‹¤ë¥´ë‹¤ëŠ” ê±¸ ë¶„ëª…íˆ í•©ë‹ˆë‹¤.

```python
def preprocess_and_log(steps):

    with wandb.init(project="artifacts-example", job_type="preprocess-data") as run:

        processed_data = wandb.Artifact(
            "mnist-preprocess", type="dataset",
            description="Preprocessed MNIST dataset",
            metadata=steps)
         
        # âœ”ï¸ ì‚¬ìš©í•  artifactë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„ ì–¸
        raw_data_artifact = run.use_artifact('mnist-raw:latest')

        # ğŸ“¥ í•„ìš”ì‹œ artifactë¥¼ ë‹¤ìš´ë¡œë“œ
        raw_dataset = raw_data_artifact.download()
        
        for split in ["training", "validation", "test"]:
            raw_split = read(raw_dataset, split)
            processed_dataset = preprocess(raw_split, **steps)

            with processed_data.new_file(split + ".pt", mode="wb") as file:
                x, y = processed_dataset.tensors
                torch.save((x, y), file)

        run.log_artifact(processed_data)


def read(data_dir, split):
    filename = split + ".pt"
    x, y = torch.load(os.path.join(data_dir, filename))

    return TensorDataset(x, y)
```

ì—¬ê¸°ì„œ ì£¼ëª©í•  ì :  
ì „ì²˜ë¦¬ `steps`ê°€  
`preprocessed_data`ì˜ `metadata`ë¡œ í•¨ê»˜ ì €ì¥ë©ë‹ˆë‹¤.

ì‹¤í—˜ì„ ì¬í˜„(reproducible)í•˜ê²Œ ë§Œë“¤ê³  ì‹¶ë‹¤ë©´,  
ë©”íƒ€ë°ì´í„°ë¥¼ ìµœëŒ€í•œ ê¼¼ê¼¼íˆ ê¸°ë¡í•˜ì„¸ìš”.

ë˜í•œ, ìš°ë¦¬ì˜ ë°ì´í„°ì…‹ì´ ê½¤ "í° artifact"ì„ì—ë„ ë¶ˆêµ¬í•˜ê³ ,  
`download` ë‹¨ê³„ê°€ 1ì´ˆë„ ì±„ ê±¸ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.

ì•„ë˜ ë§ˆí¬ë‹¤ìš´ ì…€ì„ í¼ì³ì„œ ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•´ë³´ì„¸ìš”.

```python
steps = {"normalize": True,
         "expand_dims": True}

preprocess_and_log(steps)
```

#### `run.use_artifact()`

ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
ì‚¬ìš©ìëŠ” `Artifact`ì˜ `name`, ê·¸ë¦¬ê³  ì•½ê°„ì˜ ë¶€ê°€ ì •ë³´ë¥¼ ì•Œë©´ ë©ë‹ˆë‹¤.

ë°”ë¡œ ê·¸ ë¶€ê°€ ì •ë³´ê°€ ë°”ë¡œ ë²„ì „ì„ ì„ íƒí•  ë•Œ ì‚¬ìš©í•˜ëŠ” `alias` ì…ë‹ˆë‹¤.

ê¸°ë³¸ì ìœ¼ë¡œ, ë§ˆì§€ë§‰ìœ¼ë¡œ ì—…ë¡œë“œëœ ë²„ì „ì€ `latest`ë¡œ íƒœê·¸ë©ë‹ˆë‹¤.  
ì•„ë‹ˆë©´ `v0`, `v1` ë“±ìœ¼ë¡œ ì´ì „ ë²„ì „ì„ ì„ íƒí•  ìˆ˜ë„ ìˆê³ ,  
`best`, `jit-script`ì™€ ê°™ì´ ì§ì ‘ ì—ì¼ë¦¬ì–´ìŠ¤(alias)ë¥¼ ì§€ì •í•´ì¤„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.  
[Docker Hub](https://hub.docker.com/)ì˜ íƒœê·¸ì™€ ë§ˆì°¬ê°€ì§€ë¡œ,  
ì´ë¦„ê³¼ aliasëŠ” `:`ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.  
ì¦‰, ìš°ë¦¬ê°€ ì›í•˜ëŠ” `Artifact`ëŠ” `mnist-raw:latest`ê°€ ë©ë‹ˆë‹¤.

> **ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤**: ì—ì¼ë¦¬ì–´ìŠ¤ëŠ” ì§§ê³  ê¸°ì–µí•˜ê¸° ì‰½ë„ë¡ í•˜ì„¸ìš”.  
> `latest`ë‚˜ `best`ì²˜ëŸ¼ ì»¤ìŠ¤í…€ aliasë¡œ íŠ¹ì„±ì„ ë°”ë¡œ ì•Œ ìˆ˜ ìˆê²Œ í•˜ëŠ” ê²Œ ì¢‹ìŠµë‹ˆë‹¤.

#### `artifact.download`

`download` í˜¸ì¶œì´ ê±±ì •ë˜ì‹¤ ìˆ˜ë„ ìˆê² ì£ .  
"íŒŒì¼ì„ í•œ ë²ˆ ë” ë‹¤ìš´ë¡œë“œí•˜ë©´ ë©”ëª¨ë¦¬ ë¶€ë‹´ì´ ì»¤ì§€ì§€ ì•Šì„ê¹Œ?"

ê±±ì •í•˜ì§€ ë§ˆì„¸ìš”! ë‹¤ìš´ë¡œë“œ ì „ì—  
í•´ë‹¹ ë²„ì „ì´ ì´ë¯¸ ë¡œì»¬ì— ìˆëŠ”ì§€ ë¨¼ì € ì²´í¬í•©ë‹ˆë‹¤.  
ì´ëŠ” [í† ë ŒíŠ¸](https://en.wikipedia.org/wiki/Torrent_file)ë‚˜  
[git ë²„ì „ ê´€ë¦¬](https://blog.thoughtram.io/git/2014/11/18/the-anatomy-of-a-git-commit.html)ì—ì„œ ì‚¬ìš©ë˜ëŠ” í•´ì‹œ ê¸°ìˆ ì„ í™œìš©í•©ë‹ˆë‹¤.

Artifactê°€ ìƒì„±ë˜ê³  ê¸°ë¡ë  ë•Œë§ˆë‹¤  
ì‘ì—… ë””ë ‰í† ë¦¬ì— `artifacts` í´ë”ê°€ ìƒê¸°ê³   
ê° Artifactë§ˆë‹¤ í•˜ìœ„ í´ë”ê°€ ì±„ì›Œì§‘ë‹ˆë‹¤.  
ì•„ë˜ ëª…ë ¹ì–´ë¡œ êµ¬ì¡°ë¥¼ ì‚´í´ë³¼ ìˆ˜ ìˆì–´ìš”:

```python
!tree artifacts
```

#### Artifacts í˜ì´ì§€

ì´ì œ Artifactë¥¼ ê¸°ë¡í•˜ê³  ì‚¬ìš©í–ˆìœ¼ë‹ˆ,  
Run í˜ì´ì§€ì˜ Artifacts íƒ­ì„ í™•ì¸í•´ë´…ì‹œë‹¤.

wandb ì¶œë ¥ì—ì„œ Run í˜ì´ì§€ URLë¡œ ì´ë™í•´  
ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ "Artifacts" íƒ­ì„ ì„ íƒí•˜ì„¸ìš”  
(ë°ì´í„°ë² ì´ìŠ¤ ì•„ì´ì½˜, ì¦‰ í•˜í‚¤ í½ 3ê°œê°€ ìŒ“ì¸ ëª¨ì–‘ì…ë‹ˆë‹¤).

**Input Artifacts** í…Œì´ë¸”ì´ë‚˜ **Output Artifacts** í…Œì´ë¸”ì—ì„œ  
í–‰ì„ í´ë¦­í•´ë³´ë©´  
(**Overview**, **Metadata**) íƒ­ì—ì„œ Artifactì— ê¸°ë¡ëœ  
ëª¨ë“  ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

íŠ¹íˆ **Graph View**ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.  
ê¸°ë³¸ì ìœ¼ë¡œ  
Artifactì˜ `type`ê³¼
Runì˜ `job_type`ì´ ë…¸ë“œê°€ ë˜ê³ ,  
ì†Œë¹„ì™€ ìƒì„± ê´€ê³„ê°€ í™”ì‚´í‘œë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.

### ëª¨ë¸ ê¸°ë¡(Log a Model)

ì—¬ê¸°ê¹Œì§€ê°€ Artifact APIì˜ ê¸°ë³¸ ì›ë¦¬ì´ì§€ë§Œ,  
ë§ˆì§€ë§‰ ë‹¨ê³„ê¹Œì§€ íŒŒì´í”„ë¼ì¸ì„ ë”°ë¼ê°€ë©´ì„œ  
ì–´ë–»ê²Œ ML ì›Œí¬í”Œë¡œìš°ë¥¼ ê°œì„ í•  ìˆ˜ ìˆëŠ”ì§€ ë³´ì—¬ë“œë¦´ê²Œìš”.

ì•„ë˜ ì…€ì—ì„œëŠ” PyTorchë¡œ ì‹¬í”Œí•œ ConvNet êµ¬ì¡°ì˜ DNN ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.

ìš°ì„  ëª¨ë¸ì„ ì´ˆê¸°í™”ë§Œ í•˜ê³  í•™ìŠµì€ í•˜ì§€ ì•Šê² ìŠµë‹ˆë‹¤.  
ì´ë ‡ê²Œ í•˜ë©´ íŠ¸ë ˆì´ë‹ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•´ë„ ë‚˜ë¨¸ì§€ëŠ” ë™ì¼í•˜ê²Œ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from math import floor

import torch.nn as nn

class ConvNet(nn.Module):
    def __init__(self, hidden_layer_sizes=[32, 64],
                  kernel_sizes=[3],
                  activation="ReLU",
                  pool_sizes=[2],
                  dropout=0.5,
                  num_classes=num_classes,
                  input_shape=input_shape):
      
        super(ConvNet, self).__init__()

        self.layer1 = nn.Sequential(
              nn.Conv2d(in_channels=input_shape[0], out_channels=hidden_layer_sizes[0], kernel_size=kernel_sizes[0]),
              getattr(nn, activation)(),
              nn.MaxPool2d(kernel_size=pool_sizes[0])
        )
        self.layer2 = nn.Sequential(
              nn.Conv2d(in_channels=hidden_layer_sizes[0], out_channels=hidden_layer_sizes[-1], kernel_size=kernel_sizes[-1]),
              getattr(nn, activation)(),
              nn.MaxPool2d(kernel_size=pool_sizes[-1])
        )
        self.layer3 = nn.Sequential(
              nn.Flatten(),
              nn.Dropout(dropout)
        )

        fc_input_dims = floor((input_shape[1] - kernel_sizes[0] + 1) / pool_sizes[0]) # layer 1 output size
        fc_input_dims = floor((fc_input_dims - kernel_sizes[-1] + 1) / pool_sizes[-1]) # layer 2 output size
        fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-1] # layer 3 output size

        self.fc = nn.Linear(fc_input_dims, num_classes)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.fc(x)
        return x
```

ì—¬ê¸°ì„œëŠ” W&Bë¡œ runì„ ì¶”ì í•©ë‹ˆë‹¤.  
ê·¸ë˜ì„œ ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼  
[`run.config`](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-config/Configs_in_W%26B.ipynb)  
ì˜¤ë¸Œì íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤.

ì´ config ì˜¤ë¸Œì íŠ¸ì˜ `dict` ë²„ì „ì€  
ë©”íƒ€ë°ì´í„°ë¡œì„œ ìœ ìš©í•˜ê²Œ ì“°ì´ë‹ˆ ê¼­ í¬í•¨í•˜ì„¸ìš”.

```python
def build_model_and_log(config):
    with wandb.init(project="artifacts-example", job_type="initialize", config=config) as run:
        config = run.config
        
        model = ConvNet(**config)

        model_artifact = wandb.Artifact(
            "convnet", type="model",
            description="Simple AlexNet style CNN",
            metadata=dict(config))

        torch.save(model.state_dict(), "initialized_model.pth")
        # â• Artifactì— íŒŒì¼ì„ ì¶”ê°€í•˜ëŠ” ë˜ë‹¤ë¥¸ ë°©ë²•
        model_artifact.add_file("initialized_model.pth")

        run.save("initialized_model.pth")

        run.log_artifact(model_artifact)

model_config = {"hidden_layer_sizes": [32, 64],
                "kernel_sizes": [3],
                "activation": "ReLU",
                "pool_sizes": [2],
                "dropout": 0.5,
                "num_classes": 10}

build_model_and_log(model_config)
```

#### `artifact.add_file()`

ë°ì´í„°ì…‹ ê¸°ë¡ ì˜ˆì œì—ì„œëŠ”  
`new_file`ë¡œ íŒŒì¼ ì‘ì„±ê³¼ ë™ì‹œì— Artifact ë“±ë¡ì„ í–ˆì§€ë§Œ,  
ì—¬ê¸°ì„œëŠ” íŒŒì¼ì„ ë¨¼ì € ì €ì¥(`torch.save`)í•˜ê³   
ê·¸ ë‹¤ìŒ `add_file`ë¡œ Artifactì— ì¶”ê°€í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

> **ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤**: ì¤‘ë³µ ì €ì¥ì„ ë§‰ìœ¼ë ¤ë©´ `new_file`ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

#### ê¸°ë¡ëœ ëª¨ë¸ Artifact ì‚¬ìš©í•˜ê¸°

ë°ì´í„°ì…‹ì²˜ëŸ¼  
`use_artifact`ë¥¼ í†µí•´  
`initialized_model`ë„ ë‹¤ë¥¸ Runì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ë²ˆì—ëŠ” ëª¨ë¸ì„ `train`í•´ë³´ê² ìŠµë‹ˆë‹¤.

ë” ìì„¸í•œ ì •ë³´ëŠ”  
[PyTorch + W&B ì—°ë™ Colab](https://wandb.me/pytorch-colab)ì„ ì°¸ê³ í•˜ì„¸ìš”.

```python
import wandb
import torch.nn.functional as F

def train(model, train_loader, valid_loader, config):
    optimizer = getattr(torch.optim, config.optimizer)(model.parameters())
    model.train()
    example_ct = 0
    for epoch in range(config.epochs):
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = F.cross_entropy(output, target)
            loss.backward()
            optimizer.step()

            example_ct += len(data)

            if batch_idx % config.batch_log_interval == 0:
                print('Train Epoch: {} [{}/{} ({:.0%})]\tLoss: {:.6f}'.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                    batch_idx / len(train_loader), loss.item()))
                
                train_log(loss, example_ct, epoch)

        # ë§¤ epoch ë§ˆë‹¤ validation ì„¸íŠ¸ë¡œ ëª¨ë¸ í‰ê°€
        loss, accuracy = test(model, valid_loader)  
        test_log(loss, accuracy, example_ct, epoch)

    
def test(model, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.cross_entropy(output, target, reduction='sum')  # ë°°ì¹˜ ì†ì‹¤ í•©
            pred = output.argmax(dim=1, keepdim=True)  # ì˜ˆì¸¡ ê²°ê³¼
            correct += pred.eq(target.view_as(pred)).sum()

    test_loss /= len(test_loader.dataset)

    accuracy = 100. * correct / len(test_loader.dataset)
    
    return test_loss, accuracy


def train_log(loss, example_ct, epoch):
    loss = float(loss)

    # magicì´ ì¼ì–´ë‚˜ëŠ” ê³³
    with wandb.init(project="artifacts-example", job_type="train") as run:
        run.log({"epoch": epoch, "train/loss": loss}, step=example_ct)
        print(f"Loss after " + str(example_ct).zfill(5) + f" examples: {loss:.3f}")
    

def test_log(loss, accuracy, example_ct, epoch):
    loss = float(loss)
    accuracy = float(accuracy)

    # magicì´ ì¼ì–´ë‚˜ëŠ” ê³³
    with wandb.init() as run:
        run.log({"epoch": epoch, "validation/loss": loss, "validation/accuracy": accuracy}, step=example_ct)
        print(f"Loss/accuracy after " + str(example_ct).zfill(5) + f" examples: {loss:.3f}/{accuracy:.3f}")
```

ì´ë²ˆì—” ë‘ ê°œì˜ `Artifact`ë¥¼ ìƒì„±í•˜ëŠ” Runì„ ë”°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì²« ë²ˆì§¸ Runì—ì„œ ëª¨ë¸ íŠ¸ë ˆì´ë‹ì„ ëë‚´ë©´,  
ë‘ ë²ˆì§¸ Runì—ì„œëŠ”  
`trained-model` Artifactë¥¼ ë°›ì•„  
í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ í‰ê°€í•©ë‹ˆë‹¤.

ë˜í•œ, ë„¤íŠ¸ì›Œí¬ê°€ ê°€ì¥ ë§ì´ í—·ê°ˆë¦°  
ì¦‰, `categorical_crossentropy`ê°€ ê°€ì¥ ì»¸ë˜ 32ê°œì˜ ì˜ˆì œë¥¼ ë½‘ì•„ë´…ë‹ˆë‹¤.

ì´ ë°©ë²•ì€ ë°ì´í„°ì™€ ëª¨ë¸ì˜ ë¬¸ì œë¥¼  
ì§„ë‹¨í•˜ê¸°ì— ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.

```python
def evaluate(model, test_loader):
    """
    ## í›ˆë ¨ëœ ëª¨ë¸ í‰ê°€
    """

    loss, accuracy = test(model, test_loader)
    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)

    return loss, accuracy, highest_losses, hardest_examples, true_labels, predictions

def get_hardest_k_examples(model, testing_set, k=32):
    model.eval()

    loader = DataLoader(testing_set, 1, shuffle=False)

    # ë°ì´í„°ì…‹ì˜ ê° í•­ëª©ì— ëŒ€í•´ lossì™€ ì˜ˆì¸¡ê°’ ê³„ì‚°
    losses = None
    predictions = None
    with torch.no_grad():
        for data, target in loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = F.cross_entropy(output, target)
            pred = output.argmax(dim=1, keepdim=True)
            
            if losses is None:
                losses = loss.view((1, 1))
                predictions = pred
            else:
                losses = torch.cat((losses, loss.view((1, 1))), 0)
                predictions = torch.cat((predictions, pred), 0)

    argsort_loss = torch.argsort(losses, dim=0)

    highest_k_losses = losses[argsort_loss[-k:]]
    hardest_k_examples = testing_set[argsort_loss[-k:]][0]
    true_labels = testing_set[argsort_loss[-k:]][1]
    predicted_labels = predictions[argsort_loss[-k:]]

    return highest_k_losses, hardest_k_examples, true_labels, predicted_labels
```

ì—¬ê¸°ì„œì˜ ë¡œê¹… í•¨ìˆ˜ëŠ” íŠ¹ë³„í•œ Artifact ê¸°ëŠ¥ì„ ë”í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.  
ë‹¨ìˆœíˆ `use`, `download`,  
ê·¸ë¦¬ê³  `log`ë§Œ í•  ë¿ì…ë‹ˆë‹¤.

```python
from torch.utils.data import DataLoader

def train_and_log(config):

    with wandb.init(project="artifacts-example", job_type="train", config=config) as run:
        config = run.config

        data = run.use_artifact('mnist-preprocess:latest')
        data_dir = data.download()

        training_dataset =  read(data_dir, "training")
        validation_dataset = read(data_dir, "validation")

        train_loader = DataLoader(training_dataset, batch_size=config.batch_size)
        validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size)
        
        model_artifact = run.use_artifact("convnet:latest")
        model_dir = model_artifact.download()
        model_path = os.path.join(model_dir, "initialized_model.pth")
        model_config = model_artifact.metadata
        config.update(model_config)

        model = ConvNet(**model_config)
        model.load_state_dict(torch.load(model_path))
        model = model.to(device)
 
        train(model, train_loader, validation_loader, config)

        model_artifact = wandb.Artifact(
            "trained-model", type="model",
            description="Trained NN model",
            metadata=dict(model_config))

        torch.save(model.state_dict(), "trained_model.pth")
        model_artifact.add_file("trained_model.pth")
        run.save("trained_model.pth")

        run.log_artifact(model_artifact)

    return model

    
def evaluate_and_log(config=None):
    
    with wandb.init(project="artifacts-example", job_type="report", config=config) as run:
        data = run.use_artifact('mnist-preprocess:latest')
        data_dir = data.download()
        testing_set = read(data_dir, "test")

        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=128, shuffle=False)

        model_artifact = run.use_artifact("trained-model:latest")
        model_dir = model_artifact.download()
        model_path = os.path.join(model_dir, "trained_model.pth")
        model_config = model_artifact.metadata

        model = ConvNet(**model_config)
        model.load_state_dict(torch.load(model_path))
        model.to(device)

        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)

        run.summary.update({"loss": loss, "accuracy": accuracy})

        run.log({"high-loss-examples":
            [wandb.Image(hard_example, caption=str(int(pred)) + "," +  str(int(label)))
             for hard_example, pred, label in zip(hardest_examples, preds, true_labels)]})
```

```python
train_config = {"batch_size": 128,
                "epochs": 5,
                "batch_log_interval": 25,
                "optimizer": "Adam"}

model = train_and_log(train_config)
evaluate_and_log()
```