---
title: TensorFlow
menu:
  tutorials:
    identifier: ja-tutorials-integration-tutorials-tensorflow
    parent: integration-tutorials
weight: 4
---

{{< cta-button colabLink="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/tensorflow/Simple_TensorFlow_Integration.ipynb" >}}

Weights & Biases ã‚’ä½¿ç”¨ã—ã¦ã€æ©Ÿæ¢°å­¦ç¿’ã® å®Ÿé¨“ç®¡ç† ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã® ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† ã€ãŠã‚ˆã³ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ ã® ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã¾ã™ã€‚

{{< img src="/images/tutorials/huggingface-why.png" alt="" >}}

## ã“ã® ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ ã®å†…å®¹

*   Weights & Biases ã¨ TensorFlow ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ã‚’ç°¡å˜ã« çµ±åˆã—ã¦ã€ å®Ÿé¨“ç®¡ç† ã‚’è¡Œã„ã¾ã™ã€‚
*   `keras.metrics` ã§ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
*   `wandb.log` ã‚’ä½¿ç”¨ã—ã¦ã€ã‚«ã‚¹ã‚¿ãƒ  ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ«ãƒ¼ãƒ— ã§ã“ã‚Œã‚‰ã® ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ã‚’ ãƒ­ã‚° ã«è¨˜éŒ²ã—ã¾ã™ã€‚

{{< img src="/images/tutorials/tensorflow/dashboard.png" alt="dashboard" >}}

**æ³¨æ„**: _Step_ ã§å§‹ã¾ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€W&B ã‚’æ—¢å­˜ã® ã‚³ãƒ¼ãƒ‰ ã« çµ±åˆã™ã‚‹ãŸã‚ã«å¿…è¦ãªã™ã¹ã¦ã§ã™ã€‚æ®‹ã‚Šã¯ã€æ¨™æº–çš„ãª MNIST ã®ä¾‹ã§ã™ã€‚

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.datasets import cifar10

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€ãƒ­ã‚°ã‚¤ãƒ³

### W&B ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```python
%%capture
!pip install wandb
```

### W&B ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ã¨ ãƒ­ã‚°ã‚¤ãƒ³

```python
import wandb
from wandb.integration.keras import WandbMetricsLogger

wandb.login()
```

> è£œè¶³: W&B ã‚’åˆã‚ã¦ä½¿ç”¨ã™ã‚‹å ´åˆã€ã¾ãŸã¯ ãƒ­ã‚°ã‚¤ãƒ³ ã—ã¦ã„ãªã„å ´åˆã¯ã€`wandb.login()` ã‚’å®Ÿè¡Œã—ãŸå¾Œã«è¡¨ç¤ºã•ã‚Œã‚‹ ãƒªãƒ³ã‚¯ ã‹ã‚‰ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—/ãƒ­ã‚°ã‚¤ãƒ³ ãƒšãƒ¼ã‚¸ã«ç§»å‹•ã—ã¾ã™ã€‚ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ— ã¯ ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ ã§ç°¡å˜ã«ã§ãã¾ã™ã€‚

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã®æº–å‚™

```python
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã—ã¾ã™ã€‚
BATCH_SIZE = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

# tf.data ã‚’ä½¿ç”¨ã—ã¦ å…¥åŠ› ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)

val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
val_dataset = val_dataset.batch(BATCH_SIZE)
```

## ãƒ¢ãƒ‡ãƒ« ã¨ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ«ãƒ¼ãƒ— ã®å®šç¾©

```python
def make_model():
    inputs = keras.Input(shape=(784,), name="digits")
    x1 = keras.layers.Dense(64, activation="relu")(inputs)
    x2 = keras.layers.Dense(64, activation="relu")(x1)
    outputs = keras.layers.Dense(10, name="predictions")(x2)

    return keras.Model(inputs=inputs, outputs=outputs)
```

```python
def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)

    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))

    train_acc_metric.update_state(y, logits)

    return loss_value
```

```python
def test_step(x, y, model, loss_fn, val_acc_metric):
    val_logits = model(x, training=False)
    loss_value = loss_fn(y, val_logits)
    val_acc_metric.update_state(y, val_logits)

    return loss_value
```

## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ«ãƒ¼ãƒ— ã« `wandb.log` ã‚’è¿½åŠ 

```python
def train(train_dataset, val_dataset,  model, optimizer,
          train_acc_metric, val_acc_metric,
          epochs=10,  log_step=200, val_log_step=50):
  
    for epoch in range(epochs):
        print("\nStart of epoch %d" % (epoch,))

        train_loss = []   
        val_loss = []

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã® ãƒãƒƒãƒ ã‚’åå¾©å‡¦ç†ã—ã¾ã™ã€‚
        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
            loss_value = train_step(x_batch_train, y_batch_train, 
                                    model, optimizer, 
                                    loss_fn, train_acc_metric)
            train_loss.append(float(loss_value))

        # å„ epoch ã®æœ€å¾Œã« æ¤œè¨¼ ãƒ«ãƒ¼ãƒ— ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):
            val_loss_value = test_step(x_batch_val, y_batch_val, 
                                       model, loss_fn, 
                                       val_acc_metric)
            val_loss.append(float(val_loss_value))
            
        # å„ epoch ã®æœ€å¾Œã« ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
        train_acc = train_acc_metric.result()
        print("Training acc over epoch: %.4f" % (float(train_acc),))

        val_acc = val_acc_metric.result()
        print("Validation acc: %.4f" % (float(val_acc),))

        # å„ epoch ã®æœ€å¾Œã« ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ã‚’ ãƒªã‚»ãƒƒãƒˆ ã—ã¾ã™ã€‚
        train_acc_metric.reset_states()
        val_acc_metric.reset_states()

        # â­: wandb.log ã‚’ä½¿ç”¨ã—ã¦ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ã‚’ ãƒ­ã‚° ã«è¨˜éŒ²ã—ã¾ã™ã€‚
        wandb.log({'epochs': epoch,
                   'loss': np.mean(train_loss),
                   'acc': float(train_acc), 
                   'val_loss': np.mean(val_loss),
                   'val_acc':float(val_acc)})
```

## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã®å®Ÿè¡Œ

### `wandb.init` ã‚’å‘¼ã³å‡ºã—ã¦ run ã‚’é–‹å§‹ã—ã¾ã™ã€‚

ã“ã‚Œã«ã‚ˆã‚Šã€ å®Ÿé¨“ ã‚’é–‹å§‹ã—ãŸã“ã¨ãŒé€šçŸ¥ã•ã‚Œã€ä¸€æ„ã® ID ã¨ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ ãŒæä¾›ã•ã‚Œã¾ã™ã€‚

[å…¬å¼ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ã‚’ç¢ºèªã—ã¦ãã ã•ã„]({{< relref path="/ref/python/init" lang="ja" >}})

```python
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ åã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ æ§‹æˆ ã§ wandb ã‚’ åˆæœŸåŒ– ã—ã¾ã™ã€‚
# æ§‹æˆ ã® å€¤ ã‚’è‰²ã€…è©¦ã—ã¦ã€wandb ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ ã§ çµæœ ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
config = {
              "learning_rate": 0.001,
              "epochs": 10,
              "batch_size": 64,
              "log_step": 200,
              "val_log_step": 50,
              "architecture": "CNN",
              "dataset": "CIFAR-10"
           }

run = wandb.init(project='my-tf-integration', config=config)
config = wandb.config

# ãƒ¢ãƒ‡ãƒ« ã‚’ åˆæœŸåŒ– ã—ã¾ã™ã€‚
model = make_model()

# ãƒ¢ãƒ‡ãƒ« ã‚’ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã™ã‚‹ãŸã‚ã® ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ ã‚’ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ– ã—ã¾ã™ã€‚
optimizer = keras.optimizers.SGD(learning_rate=config.learning_rate)
# æå¤±é–¢æ•° ã‚’ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ– ã—ã¾ã™ã€‚
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ã‚’æº–å‚™ã—ã¾ã™ã€‚
train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

train(train_dataset,
      val_dataset, 
      model,
      optimizer,
      train_acc_metric,
      val_acc_metric,
      epochs=config.epochs, 
      log_step=config.log_step, 
      val_log_step=config.val_log_step)

run.finish()  # Jupyter/Colab ã§ã€å®Œäº†ã—ãŸã“ã¨ã‚’ãŠçŸ¥ã‚‰ã›ãã ã•ã„!
```

### çµæœ ã® å¯è¦–åŒ–

ä¸Šè¨˜ã® [**run ãƒšãƒ¼ã‚¸**]({{< relref path="/guides/models/track/runs/#view-logged-runs" lang="ja" >}}) ãƒªãƒ³ã‚¯ ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€ãƒ©ã‚¤ãƒ– ã® çµæœ ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

## Sweep 101

Weights & Biases Sweeps ã‚’ä½¿ç”¨ã—ã¦ã€ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ ã® æœ€é©åŒ– ã‚’è‡ªå‹•åŒ–ã—ã€å¯èƒ½ãª ãƒ¢ãƒ‡ãƒ« ã® ç©ºé–“ ã‚’æ¢ç´¢ã—ã¾ã™ã€‚

## [W&B Sweeps ã‚’ä½¿ç”¨ã—ãŸ TensorFlow ã§ã® ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ ã® æœ€é©åŒ– ã‚’ç¢ºèªã—ã¦ãã ã•ã„](http://wandb.me/tf-sweeps-colab)

### W&B Sweeps ã‚’ä½¿ç”¨ã™ã‚‹ åˆ©ç‚¹

*   **ç°¡å˜ãª ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: æ•°è¡Œã® ã‚³ãƒ¼ãƒ‰ ã ã‘ã§ W&B sweeps ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚
*   **é€éçš„**: ä½¿ç”¨ã—ã¦ã„ã‚‹ã™ã¹ã¦ã® ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  ã‚’å¼•ç”¨ã—ã€[ã‚³ãƒ¼ãƒ‰ ã¯ ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ ã§ã™](https://github.com/wandb/sweeps)ã€‚
*   **å¼·åŠ›**: sweeps ã¯å®Œå…¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ã§ã€æ§‹æˆå¯èƒ½ã§ã™ã€‚æ•°åå°ã® ãƒã‚·ãƒ³ ã§ sweep ã‚’ èµ·å‹•ã§ãã€ ãƒ©ãƒƒãƒ—ãƒˆãƒƒãƒ— ã§ sweep ã‚’é–‹å§‹ã™ã‚‹ã®ã¨åŒã˜ãã‚‰ã„ç°¡å˜ã§ã™ã€‚

{{< img src="/images/tutorials/tensorflow/sweeps.png" alt="Sweep result" >}}

## ã‚µãƒ³ãƒ—ãƒ« ã‚®ãƒ£ãƒ©ãƒªãƒ¼

W&B ã§ ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚° ãŠã‚ˆã³ å¯è¦–åŒ– ã•ã‚ŒãŸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ ã® ä¾‹ã«ã¤ã„ã¦ã¯ã€ ã‚µãƒ³ãƒ—ãƒ« ã® ã‚®ãƒ£ãƒ©ãƒªãƒ¼ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚[å®Œå…¨ã« æ¥ç¶š â†’](https://wandb.me/fc)

# ğŸ“ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

1.  **Projects**: è¤‡æ•°ã® run ã‚’ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ ã« ãƒ­ã‚° ã—ã¦ã€ãã‚Œã‚‰ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚`wandb.init(project="project-name")`
2.  **Groups**: è¤‡æ•°ã® ãƒ—ãƒ­ã‚»ã‚¹ ã¾ãŸã¯ äº¤å·®æ¤œè¨¼ ã® folds ã«ã¤ã„ã¦ã¯ã€å„ ãƒ—ãƒ­ã‚»ã‚¹ ã‚’ run ã¨ã—ã¦ ãƒ­ã‚° ã—ã€ãã‚Œã‚‰ã‚’ ã‚°ãƒ«ãƒ¼ãƒ—åŒ– ã—ã¾ã™ã€‚`wandb.init(group='experiment-1')`
3.  **Tags**: ç¾åœ¨ã® ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ ã¾ãŸã¯ ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ ãƒ¢ãƒ‡ãƒ« ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã« ã‚¿ã‚° ã‚’è¿½åŠ ã—ã¾ã™ã€‚
4.  **Notes**: ãƒ†ãƒ¼ãƒ–ãƒ« ã« ãƒãƒ¼ãƒˆ ã‚’å…¥åŠ›ã—ã¦ã€run é–“ã®å¤‰æ›´ã‚’è¿½è·¡ã—ã¾ã™ã€‚
5.  **Reports**: åŒåƒšã¨å…±æœ‰ã™ã‚‹ãŸã‚ã« é€²æ—çŠ¶æ³ ã«ã¤ã„ã¦ç°¡å˜ãª ãƒ¡ãƒ¢ ã‚’å–ã‚Šã€ML ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ ã® ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ ã¨ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ ã‚’ä½œæˆã—ã¾ã™ã€‚

## é«˜åº¦ãª ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

1.  [ç’°å¢ƒå¤‰æ•°]({{< relref path="/guides/hosting/env-vars/" lang="ja" >}}): ãƒãƒãƒ¼ã‚¸ãƒ‰ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ ã§ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã€ç’°å¢ƒå¤‰æ•° ã« APIã‚­ãƒ¼ ã‚’è¨­å®šã—ã¾ã™ã€‚
2.  [ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ ãƒ¢ãƒ¼ãƒ‰]({{< relref path="/support/run_wandb_offline.md" lang="ja" >}})
3.  [ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹]({{< relref path="/guides/hosting/hosting-options/self-managed" lang="ja" >}}): ç‹¬è‡ªã® ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ¼ ã® ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚¯ãƒ©ã‚¦ãƒ‰ ã¾ãŸã¯ ã‚¨ã‚¢ã‚®ãƒ£ãƒƒãƒ— ã‚µãƒ¼ãƒãƒ¼ ã« W&B ã‚’ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« ã—ã¾ã™ã€‚ å­¦è¡“é–¢ä¿‚è€…ã‹ã‚‰ ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚º ãƒãƒ¼ãƒ  ã¾ã§ã€ã‚ã‚‰ã‚†ã‚‹äººã«å¯¾å¿œã§ãã‚‹ ãƒ­ãƒ¼ã‚«ãƒ« ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« ãŒã‚ã‚Šã¾ã™ã€‚
4.  [Artifacts]({{< relref path="/guides/core/artifacts/" lang="ja" >}}): ãƒ¢ãƒ‡ãƒ« ã‚’ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã™ã‚‹éš›ã« ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ã‚¹ãƒ†ãƒƒãƒ— ã‚’è‡ªå‹•çš„ã« å–å¾—ã™ã‚‹ åˆç†åŒ–ã•ã‚ŒãŸæ–¹æ³•ã§ã€ ãƒ¢ãƒ‡ãƒ« ã¨ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚’è¿½è·¡ãŠã‚ˆã³ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† ã—ã¾ã™ã€‚
