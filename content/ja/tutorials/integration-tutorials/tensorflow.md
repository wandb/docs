---
title: TensorFlow
menu:
  tutorials:
    identifier: ja-tutorials-integration-tutorials-tensorflow
    parent: integration-tutorials
weight: 4
---

{{< cta-button colabLink="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/tensorflow/Simple_TensorFlow_Integration.ipynb" >}}

Weights & Biases ã‚’ä½¿ç”¨ã—ã¦ æ©Ÿæ¢°å­¦ç¿’ ã® å®Ÿé¨“ç®¡ç† ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ãŠã‚ˆã³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã¾ã™ã€‚

{{< img src="/images/tutorials/huggingface-why.png" alt="" >}}

## ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã‚«ãƒãƒ¼ã™ã‚‹å†…å®¹

* TensorFlow ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ã¸ã® Weights and Biases ã®ç°¡å˜ãªã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹ å®Ÿé¨“ç®¡ç† ã€‚
* `keras.metrics` ã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã¾ã™
* `wandb.log` ã‚’ä½¿ç”¨ã—ã¦ã€ã‚«ã‚¹ã‚¿ãƒ  ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ«ãƒ¼ãƒ—ã«ãã‚Œã‚‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã—ã¾ã™ã€‚


{{< img src="/images/tutorials/tensorflow/dashboard.png" alt="dashboard" >}}

**æ³¨æ„**: _Step_ ã‹ã‚‰å§‹ã¾ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€W&B ã‚’æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã«çµ±åˆã™ã‚‹ãŸã‚ã«å¿…è¦ãªã™ã¹ã¦ã®å†…å®¹ã§ã™ã€‚ãã‚Œä»¥å¤–ã¯æ¨™æº–çš„ãª MNIST ã®ä¾‹ã§ã™ã€‚

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.datasets import cifar10

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€ãƒ­ã‚°ã‚¤ãƒ³

### W&B ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«


```python
%%capture
!pip install wandb
```

### W&B ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãƒ­ã‚°ã‚¤ãƒ³


```python
import wandb
from wandb.integration.keras import WandbMetricsLogger

wandb.login()
```

> è£œè¶³: åˆã‚ã¦ W&B ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãªã„å ´åˆã¯ã€`wandb.login()` ã‚’å®Ÿè¡Œã—ãŸå¾Œã«è¡¨ç¤ºã•ã‚Œã‚‹ãƒªãƒ³ã‚¯ã§ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—/ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—ã¯ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã§ç°¡å˜ã§ã™ã€‚

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™


```python
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™
BATCH_SIZE = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

# tf.data ã‚’ä½¿ç”¨ã—ã¦å…¥åŠ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)

val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
val_dataset = val_dataset.batch(BATCH_SIZE)
```

## ãƒ¢ãƒ‡ãƒ«ã¨ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ«ãƒ¼ãƒ—ã‚’å®šç¾©


```python
def make_model():
    inputs = keras.Input(shape=(784,), name="digits")
    x1 = keras.layers.Dense(64, activation="relu")(inputs)
    x2 = keras.layers.Dense(64, activation="relu")(x1)
    outputs = keras.layers.Dense(10, name="predictions")(x2)

    return keras.Model(inputs=inputs, outputs=outputs)
```


```python
def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)

    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))

    train_acc_metric.update_state(y, logits)

    return loss_value
```


```python
def test_step(x, y, model, loss_fn, val_acc_metric):
    val_logits = model(x, training=False)
    loss_value = loss_fn(y, val_logits)
    val_acc_metric.update_state(y, val_logits)

    return loss_value
```

## `wandb.log` ã‚’ã‚ãªãŸã® ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ«ãƒ¼ãƒ—ã«è¿½åŠ 


```python
def train(train_dataset, val_dataset,  model, optimizer,
          train_acc_metric, val_acc_metric,
          epochs=10,  log_step=200, val_log_step=50):
  
    for epoch in range(epochs):
        print("\nStart of epoch %d" % (epoch,))

        train_loss = []   
        val_loss = []

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒƒãƒã‚’ç¹°ã‚Šè¿”ã—å‡¦ç†
        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
            loss_value = train_step(x_batch_train, y_batch_train, 
                                    model, optimizer, 
                                    loss_fn, train_acc_metric)
            train_loss.append(float(loss_value))

        # å„ã‚¨ãƒãƒƒã‚¯ã®æœ€å¾Œã«æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ
        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):
            val_loss_value = test_step(x_batch_val, y_batch_val, 
                                       model, loss_fn, 
                                       val_acc_metric)
            val_loss.append(float(val_loss_value))
            
        # å„ã‚¨ãƒãƒƒã‚¯ã®æœ€å¾Œã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º
        train_acc = train_acc_metric.result()
        print("Training acc over epoch: %.4f" % (float(train_acc),))

        val_acc = val_acc_metric.result()
        print("Validation acc: %.4f" % (float(val_acc),))

        # å„ã‚¨ãƒãƒƒã‚¯ã®æœ€å¾Œã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
        train_acc_metric.reset_states()
        val_acc_metric.reset_states()

        # â­: wandb.log ã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°
        wandb.log({'epochs': epoch,
                   'loss': np.mean(train_loss),
                   'acc': float(train_acc), 
                   'val_loss': np.mean(val_loss),
                   'val_acc':float(val_acc)})
```

## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ

### `wandb.init` ã‚’å‘¼ã³å‡ºã—ã¦ run ã‚’é–‹å§‹

ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ãªãŸãŒå®Ÿé¨“ã‚’é–‹å§‹ã—ãŸã“ã¨ã‚’çŸ¥ã‚‰ã›ã‚‹ã“ã¨ãŒã§ãã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªIDã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’æä¾›ã—ã¾ã™ã€‚

[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèª]({{< relref path="/ref/python/init" lang="ja" >}})

```python
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã¨ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§è¨­å®šã‚’æŒ‡å®šã—ã¦ wandb ã‚’åˆæœŸåŒ–
# è¨­å®šå€¤ã‚’ã„ã˜ã£ã¦ã€wandb ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„
config = {
              "learning_rate": 0.001,
              "epochs": 10,
              "batch_size": 64,
              "log_step": 200,
              "val_log_step": 50,
              "architecture": "CNN",
              "dataset": "CIFAR-10"
           }

run = wandb.init(project='my-tf-integration', config=config)
config = wandb.config

# ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
model = make_model()

# ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã‚‹ãŸã‚ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
optimizer = keras.optimizers.SGD(learning_rate=config.learning_rate)
# æå¤±é–¢æ•°ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æº–å‚™
train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

train(train_dataset,
      val_dataset, 
      model,
      optimizer,
      train_acc_metric,
      val_acc_metric,
      epochs=config.epochs, 
      log_step=config.log_step, 
      val_log_step=config.val_log_step)

run.finish()  # Jupyter/Colab ã§ã¯ã€çµ‚äº†ã—ãŸã“ã¨ã‚’çŸ¥ã‚‰ã›ã¦ãã ã•ã„ï¼
```

### çµæœã®å¯è¦–åŒ–

ä¸Šè¨˜ã® [**run ãƒšãƒ¼ã‚¸**]({{< relref path="/guides/models/track/runs/#view-logged-runs" lang="ja" >}}) ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€ãƒ©ã‚¤ãƒ–ã®çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

## Sweep 101

Weights & Biases Sweeps ã‚’ä½¿ç”¨ã—ã¦ã€ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è‡ªå‹•åŒ–ã—ã€å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ç©ºé–“ã‚’æ¢ç´¢ã—ã¾ã™ã€‚

## [TensorFlowã§ã®W&Bã‚¹ã‚¤ãƒ¼ãƒ—ã‚’ä½¿ç”¨ã—ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯](http://wandb.me/tf-sweeps-colab)

### W&B Sweeps ã‚’ä½¿ç”¨ã™ã‚‹åˆ©ç‚¹

* **ã‚¯ã‚¤ãƒƒã‚¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: ã‚ãšã‹æ•°è¡Œã® ã‚³ãƒ¼ãƒ‰ ã§ W&B ã‚¹ã‚¤ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚
* **é€æ˜æ€§**: ä½¿ç”¨ã—ã¦ã„ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å¼•ç”¨ã—ã€[ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹](https://github.com/wandb/sweeps) ã§ã™ã€‚
* **å¼·åŠ›**: ã‚¹ã‚¤ãƒ¼ãƒ—ã¯å®Œå…¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ã§è¨­å®šå¯èƒ½ã§ã™ã€‚æ•°åå°ã®ãƒã‚·ãƒ³ã§ã‚¹ã‚¤ãƒ¼ãƒ—ã‚’é–‹å§‹ã™ã‚‹ã“ã¨ãŒã§ãã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã§ã‚¹ã‚¤ãƒ¼ãƒ—ã‚’é–‹å§‹ã™ã‚‹ã®ã¨åŒã˜ãã‚‰ã„ç°¡å˜ã§ã™ã€‚

{{< img src="/images/tutorials/tensorflow/sweeps.png" alt="Sweep result" >}}

## ã‚®ãƒ£ãƒ©ãƒªãƒ¼ä¾‹

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒ W&B ã§ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã•ã‚Œã€å¯è¦–åŒ–ã•ã‚Œã¦ã„ã‚‹ä¾‹ã¯ä¾‹ã®ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã§ç¢ºèªã§ãã¾ã™ [Fully Connected â†’](https://wandb.me/fc)

# ğŸ“ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
1. **Projects**: è¤‡æ•°ã® run ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãƒ­ã‚°ã—ã¦æ¯”è¼ƒã—ã¾ã™ã€‚`wandb.init(project="project-name")`
2. **Groups**: è¤‡æ•°ã®ãƒ—ãƒ­ã‚»ã‚¹ã¾ãŸã¯äº¤å·®æ¤œè¨¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®å ´åˆã¯ã€å„ãƒ—ãƒ­ã‚»ã‚¹ã‚’ run ã¨ã—ã¦ãƒ­ã‚°ã—ã€ãã‚Œã‚‰ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¾ã™ã€‚`wandb.init(group='experiment-1')`
3. **Tags**: ç¾åœ¨ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¾ãŸã¯ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ©ãƒƒã‚¯ã™ã‚‹ãŸã‚ã«ã‚¿ã‚°ã‚’è¿½åŠ ã—ã¾ã™ã€‚
4. **Notes**: ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã§ãƒ¡ãƒ¢ã‚’å…¥åŠ›ã—ã¦ã€run é–“ã®å¤‰æ›´ã‚’ãƒˆãƒ©ãƒƒã‚¯ã—ã¾ã™ã€‚
5. **Reports**: é€²æ—ã«ã¤ã„ã¦åŒåƒšã¨å…±æœ‰ã™ã‚‹ãŸã‚ã«ã‚¯ã‚¤ãƒƒã‚¯ãƒ¡ãƒ¢ã‚’å–ã£ãŸã‚Šã€MLãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¨ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½œæˆã—ãŸã‚Šã—ã¾ã™ã€‚

## é«˜åº¦ãªè¨­å®š
1. [ç’°å¢ƒå¤‰æ•°]({{< relref path="/guides/hosting/env-vars/" lang="ja" >}}): ãƒãƒãƒ¼ã‚¸ãƒ‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã€ç’°å¢ƒå¤‰æ•°ã« APIã‚­ãƒ¼ ã‚’è¨­å®šã—ã¾ã™ã€‚
2. [ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰]({{< relref path="/support/run_wandb_offline.md" lang="ja" >}})
3. [ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹]({{< relref path="/guides/hosting/hosting-options/self-managed" lang="ja" >}}): W&B ã‚’ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚¯ãƒ©ã‚¦ãƒ‰ã‚„ã‚¨ã‚¢ã‚®ãƒ£ãƒƒãƒ—ã®ã‚µãƒ¼ãƒãƒ¼ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚å­¦è¡“é–¢ä¿‚è€…ã‹ã‚‰ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒãƒ¼ãƒ ã¾ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚
4. [Artifacts]({{< relref path="/guides/core/artifacts/" lang="ja" >}}): ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«è‡ªå‹•çš„ã«å–ã‚Šè¾¼ã‚€ã‚ˆã†ã«è¨­è¨ˆã•ã‚ŒãŸã€ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚’è¡Œã„ã¾ã™ã€‚