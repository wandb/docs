---
title: Track models and datasets
menu:
  tutorials:
    identifier: ja-tutorials-artifacts
weight: 4
---

{{< cta-button colabLink="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W&B_Artifacts.ipynb" >}}
ã“ã® notebook ã§ã¯ã€W&B Artifacts ã‚’ä½¿ç”¨ã—ã¦ ML ã®å®Ÿé¨“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¿½è·¡ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

[ãƒ“ãƒ‡ã‚ªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](http://tiny.cc/wb-artifacts-video)ã‚’ã”è¦§ãã ã•ã„ã€‚

## Artifacts ã«ã¤ã„ã¦

Artifacts ã¯ã€ã‚®ãƒªã‚·ãƒ£ã®[ã‚¢ãƒ³ãƒ•ã‚©ãƒ©](https://en.wikipedia.org/wiki/Amphora)ã®ã‚ˆã†ã«ã€
ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã§ã‚ã‚‹ç”Ÿæˆç‰©ã§ã™ã€‚
ML ã«ãŠã„ã¦ã€æœ€ã‚‚é‡è¦ãª artifacts ã¯ _ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_ ã¨ _ãƒ¢ãƒ‡ãƒ«_ ã§ã™ã€‚

ãã—ã¦ã€[ã‚³ã‚³ãƒ­ãƒŠãƒ‰ã®åå­—æ¶](https://indianajones.fandom.com/wiki/Cross_of_Coronado)ã®ã‚ˆã†ã«ã€ã“ã‚Œã‚‰ã®é‡è¦ãª artifacts ã¯åšç‰©é¤¨ã«åè”µã•ã‚Œã‚‹ã¹ãã§ã™ã€‚
ã¤ã¾ã‚Šã€ã‚ãªãŸã€ã‚ãªãŸã® Teamã€ãã—ã¦ ML ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å…¨ä½“ãŒãã‚Œã‚‰ã‹ã‚‰å­¦ã¹ã‚‹ã‚ˆã†ã«ã€ã‚«ã‚¿ãƒ­ã‚°åŒ–ã•ã‚Œã€æ•´ç†ã•ã‚Œã‚‹ã¹ãã§ã™ã€‚
çµå±€ã®ã¨ã“ã‚ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¿½è·¡ã—ãªã„è€…ã¯ã€ãã‚Œã‚’ç¹°ã‚Šè¿”ã™é‹å‘½ã«ã‚ã‚‹ã®ã§ã™ã€‚

Artifacts API ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€æ¬¡ã®å›³ã®ã‚ˆã†ã«ã€W&B `Run` ã®å‡ºåŠ›ã¨ã—ã¦ `Artifact` ã‚’ãƒ­ã‚°ã—ãŸã‚Šã€`Run` ã®å…¥åŠ›ã¨ã—ã¦ `Artifact` ã‚’ä½¿ç”¨ã—ãŸã‚Šã§ãã¾ã™ã€‚
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° run ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
 
 {{< img src="/images/tutorials/artifacts-diagram.png" alt="" >}}

1 ã¤ã® run ãŒåˆ¥ã® run ã®å‡ºåŠ›ã‚’å…¥åŠ›ã¨ã—ã¦ä½¿ç”¨ã§ãã‚‹ãŸã‚ã€`Artifact` ã¨ `Run` ã¯ã€`Artifact` ã¨ `Run` ã®ãƒãƒ¼ãƒ‰ã‚’æŒã¤æœ‰å‘ã‚°ãƒ©ãƒ• (äºŒéƒ¨[DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph))ã‚’å½¢æˆã—ã¾ã™ã€‚
ãã—ã¦ã€`Run` ã‚’ãã‚ŒãŒæ¶ˆè²»ã¾ãŸã¯ç”Ÿæˆã™ã‚‹ `Artifact` ã«æ¥ç¶šã™ã‚‹çŸ¢å°ã‚’æŒã¡ã¾ã™ã€‚

## Artifacts ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½è·¡ã™ã‚‹

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

Artifacts ã¯ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ `0.9.2` ä»¥é™ã® Python ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä¸€éƒ¨ã§ã™ã€‚

ML Python ã‚¹ã‚¿ãƒƒã‚¯ã®ã»ã¨ã‚“ã©ã®éƒ¨åˆ†ã¨åŒæ§˜ã«ã€`pip` çµŒç”±ã§åˆ©ç”¨ã§ãã¾ã™ã€‚


```python
# Compatible with wandb version 0.9.2+
!pip install wandb -qqq
!apt install tree
```


```python
import os
import wandb
```

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ã‚°

ã¾ãšã€ã„ãã¤ã‹ã® Artifacts ã‚’å®šç¾©ã—ã¾ã—ã‚‡ã†ã€‚

ã“ã®ä¾‹ã¯ã€PyTorch ã®
["Basic MNIST Example"](https://github.com/pytorch/examples/tree/master/mnist/)ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚
ã—ã‹ã—ã€[TensorFlow](http://wandb.me/artifacts-colab)ã‚„ä»–ã® frameworkã€
ã¾ãŸã¯ç´”ç²‹ãª Python ã§åŒã˜ã‚ˆã†ã«è¡Œã†ã“ã¨ãŒã§ãã¾ã—ãŸã€‚

`Dataset` ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸æŠã™ã‚‹ãŸã‚ã® `train`ing ã‚»ãƒƒãƒˆã€
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸æŠã™ã‚‹ãŸã‚ã® `validation` ã‚»ãƒƒãƒˆã€
- æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã® `test`ing ã‚»ãƒƒãƒˆ

ä»¥ä¸‹ã®æœ€åˆã®ã‚»ãƒ«ã¯ã€ã“ã‚Œã‚‰ã® 3 ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å®šç¾©ã—ã¾ã™ã€‚


```python
import random 

import torch
import torchvision
from torch.utils.data import TensorDataset
from tqdm.auto import tqdm

# Ensure deterministic behavior
torch.backends.cudnn.deterministic = True
random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)

# Device configuration
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Data parameters
num_classes = 10
input_shape = (1, 28, 28)

# drop slow mirror from list of MNIST mirrors
torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors
                                      if not mirror.startswith("http://yann.lecun.com")]

def load(train_size=50_000):
    """
    # Load the data
    """

    # the data, split between train and test sets
    train = torchvision.datasets.MNIST("./", train=True, download=True)
    test = torchvision.datasets.MNIST("./", train=False, download=True)
    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)

    # split off a validation set for hyperparameter tuning
    x_train, x_val = x_train[:train_size], x_train[train_size:]
    y_train, y_val = y_train[:train_size], y_train[train_size:]

    training_set = TensorDataset(x_train, y_train)
    validation_set = TensorDataset(x_val, y_val)
    test_set = TensorDataset(x_test, y_test)

    datasets = [training_set, validation_set, test_set]

    return datasets
```

ã“ã‚Œã¯ã€ã“ã®ä¾‹ã§ç¹°ã‚Šè¿”ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨­å®šã—ã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚’ Artifacts ã¨ã—ã¦ãƒ­ã‚°ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã¯ã€
ãã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ã®å‘¨ã‚Šã«ãƒ©ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚
ã“ã®å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚’ `load` ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã¯ã€
ãƒ‡ãƒ¼ã‚¿ã‚’ `load_and_log` ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰åˆ†é›¢ã•ã‚Œã¦ã„ã¾ã™ã€‚

ã“ã‚Œã¯è‰¯ã„ç¿’æ…£ã§ã™ã€‚

ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ Artifacts ã¨ã—ã¦ãƒ­ã‚°ã™ã‚‹ã«ã¯ã€
æ¬¡ã®ã“ã¨ãŒå¿…è¦ã§ã™ã€‚
1. `wandb.init` ã§ `Run` ã‚’ä½œæˆã— (L4)ã€
2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® `Artifact` ã‚’ä½œæˆã— (L10)ã€
3. é–¢é€£ã™ã‚‹ `file` ã‚’ä¿å­˜ã—ã¦ãƒ­ã‚°ã—ã¾ã™ (L20ã€L23)ã€‚

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚»ãƒ«ã§ä¾‹ã‚’ç¢ºèªã—ã€
ãã®å¾Œã§è©³ç´°ã«ã¤ã„ã¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å±•é–‹ã—ã¦ãã ã•ã„ã€‚


```python
def load_and_log():

    # ğŸš€ run ã‚’é–‹å§‹ã—ã¾ã™ã€‚ã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®šã—ã¦ãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æŒ‡å®šã—ã¦ãƒ›ãƒ¼ãƒ ã¨å‘¼ã¶ã“ã¨ãŒã§ãã¾ã™ã€‚
    with wandb.init(project="artifacts-example", job_type="load-data") as run:
        
        datasets = load()  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã®å€‹åˆ¥ã®ã‚³ãƒ¼ãƒ‰
        names = ["training", "validation", "test"]

        # ğŸº Artifact ã‚’ä½œæˆã—ã¾ã™
        raw_data = wandb.Artifact(
            "mnist-raw", type="dataset",
            description="Raw MNIST dataset, split into train/val/test",
            metadata={"source": "torchvision.datasets.MNIST",
                      "sizes": [len(dataset) for dataset in datasets]})

        for name, data in zip(names, datasets):
            # ğŸ£ Artifact ã«æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã€ãã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ä½•ã‹ã‚’æ›¸ãè¾¼ã¿ã¾ã™ã€‚
            with raw_data.new_file(name + ".pt", mode="wb") as file:
                x, y = data.tensors
                torch.save((x, y), file)

        # âœï¸ Artifact ã‚’ W&B ã«ä¿å­˜ã—ã¾ã™ã€‚
        run.log_artifact(raw_data)

load_and_log()
```

#### `wandb.init`


`Artifact` ã‚’ç”Ÿæˆã™ã‚‹ `Run` ã‚’ä½œæˆã™ã‚‹ã¨ãã¯ã€
ãã‚ŒãŒå±ã™ã‚‹ `project` ã‚’è¨˜è¿°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

workflow ã«ã‚ˆã£ã¦ã¯ã€
project ã¯ `car-that-drives-itself` ã»ã©å¤§ããã™ã‚‹ã“ã¨ã‚‚ã€
`iterative-architecture-experiment-117` ã»ã©å°ã•ãã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

> **çµŒé¨“å‰‡**: å¯èƒ½ã§ã‚ã‚Œã°ã€`Artifact` ã‚’å…±æœ‰ã™ã‚‹ã™ã¹ã¦ã® `Run` ã‚’
å˜ä¸€ã® project å†…ã«ä¿æŒã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç‰©äº‹ãŒã‚·ãƒ³ãƒ—ãƒ«ã«ãªã‚Šã¾ã™ãŒã€
å¿ƒé…ã—ãªã„ã§ãã ã•ã„ -- `Artifact` ã¯ project é–“ã§ç§»æ¤å¯èƒ½ã§ã™ã€‚

å®Ÿè¡Œã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã•ã¾ã–ã¾ãªç¨®é¡ã® job ã‚’ã™ã¹ã¦è¿½è·¡ã™ã‚‹ãŸã‚ã«ã€
`Run` ã‚’ä½œæˆã™ã‚‹ã¨ãã« `job_type` ã‚’æŒ‡å®šã™ã‚‹ã¨ä¾¿åˆ©ã§ã™ã€‚
ã“ã‚Œã«ã‚ˆã‚Šã€Artifacts ã®ã‚°ãƒ©ãƒ•ãŒæ•´ç†ã•ã‚Œã¾ã™ã€‚

> **çµŒé¨“å‰‡**: `job_type` ã¯è¨˜è¿°çš„ã§ã‚ã‚Šã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å˜ä¸€ã®ã‚¹ãƒ†ãƒƒãƒ—ã«å¯¾å¿œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã“ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã® `load` ã¨ãƒ‡ãƒ¼ã‚¿ã® `preprocess` ã‚’åˆ†é›¢ã—ã¾ã™ã€‚

#### `wandb.Artifact`


ä½•ã‹ã‚’ `Artifact` ã¨ã—ã¦ãƒ­ã‚°ã™ã‚‹ã«ã¯ã€æœ€åˆã« `Artifact` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ã™ã¹ã¦ã® `Artifact` ã«ã¯ `name` ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯æœ€åˆã®å¼•æ•°ã§è¨­å®šã•ã‚Œã¾ã™ã€‚

> **çµŒé¨“å‰‡**: `name` ã¯è¨˜è¿°çš„ã§ã‚ã‚Šã€è¦šãˆã‚„ã™ãå…¥åŠ›ã—ã‚„ã™ã„ã‚‚ã®ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ --
ãƒã‚¤ãƒ•ãƒ³ã§åŒºåˆ‡ã‚‰ã‚Œã€ã‚³ãƒ¼ãƒ‰å†…ã®å¤‰æ•°åã«å¯¾å¿œã™ã‚‹åå‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

ã¾ãŸã€`type` ã‚‚ã‚ã‚Šã¾ã™ã€‚`Run` ã® `job_type` ã¨åŒæ§˜ã«ã€
ã“ã‚Œã¯ `Run` ã¨ `Artifact` ã®ã‚°ãƒ©ãƒ•ã‚’æ•´ç†ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

> **çµŒé¨“å‰‡**: `type` ã¯ã‚·ãƒ³ãƒ—ãƒ«ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
`mnist-data-YYYYMMDD` ã‚ˆã‚Šã‚‚ `dataset` ã‚„ `model` ã«è¿‘ã„ã‚‚ã®ã§ã™ã€‚

`description` ã¨ã„ãã¤ã‹ã® `metadata` ã‚’ dictionary ã¨ã—ã¦ã‚¢ã‚¿ãƒƒãƒã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
`metadata` ã¯ JSON ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

> **çµŒé¨“å‰‡**: `metadata` ã¯å¯èƒ½ãªé™ã‚Šè¨˜è¿°çš„ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

#### `artifact.new_file` ã¨ `run.log_artifact`

`Artifact` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ãŸã‚‰ã€ãã‚Œã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ãã®ã¨ãŠã‚Šã§ã™ã€‚_ãƒ•ã‚¡ã‚¤ãƒ«_ ã§ã™ã€‚
`Artifact` ã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ¼ã®ã‚ˆã†ã«æ§‹æˆã•ã‚Œã¦ãŠã‚Šã€
ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ¼ãŒã‚ã‚Šã¾ã™ã€‚

> **çµŒé¨“å‰‡**: å¯èƒ½ãªå ´åˆã¯å¸¸ã«ã€`Artifact` ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’
è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã¯ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã¨ãã«å½¹ç«‹ã¡ã¾ã™ã€‚

`new_file` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€
ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒæ™‚ã«æ›¸ãè¾¼ã¿ã€`Artifact` ã«ã‚¢ã‚¿ãƒƒãƒã—ã¾ã™ã€‚
ä»¥ä¸‹ã§ã¯ã€`add_file` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
ã“ã‚Œã«ã‚ˆã‚Šã€ã“ã‚Œã‚‰ 2 ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒåˆ†é›¢ã•ã‚Œã¾ã™ã€‚

ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã—ãŸã‚‰ã€[wandb.ai](https://wandb.ai)ã« `log_artifact` ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

å‡ºåŠ›ã«ã„ãã¤ã‹ã® URL ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
Run ãƒšãƒ¼ã‚¸ã¸ã® URL ã‚‚å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
ãã“ã§ã¯ã€ãƒ­ã‚°ã«è¨˜éŒ²ã•ã‚ŒãŸ `Artifact` ã‚’å«ã‚€ã€`Run` ã®çµæœã‚’è¡¨ç¤ºã§ãã¾ã™ã€‚

ä»¥ä¸‹ã§ã¯ã€Run ãƒšãƒ¼ã‚¸ã®ä»–ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚ˆã‚Šæœ‰åŠ¹ã«æ´»ç”¨ã™ã‚‹ä¾‹ã‚’ã„ãã¤ã‹ç¤ºã—ã¾ã™ã€‚

### ãƒ­ã‚°ã«è¨˜éŒ²ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ Artifact ã®ä½¿ç”¨

åšç‰©é¤¨ã® artifacts ã¨ã¯ç•°ãªã‚Šã€W&B ã® `Artifact` ã¯ã€
ä¿å­˜ã™ã‚‹ã ã‘ã§ãªãã€_ä½¿ç”¨_ ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

ãã‚ŒãŒã©ã®ã‚ˆã†ãªã‚‚ã®ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

ä»¥ä¸‹ã®ã‚»ãƒ«ã¯ã€raw ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€
ãã‚Œã‚’ä½¿ç”¨ã—ã¦ `preprocess` ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (
`normalize` ã•ã‚Œã€æ­£ã—ãæ•´å½¢ã•ã‚Œã¦ã„ã‚‹)ã‚’ç”Ÿæˆã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®šç¾©ã—ã¾ã™ã€‚

ã‚³ãƒ¼ãƒ‰ã®ä¸­æ ¸ã§ã‚ã‚‹ `preprocess` ã‚’
`wandb` ã¨ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¨ãªã‚‹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰åˆ†é›¢ã—ã¦ã„ã‚‹ã“ã¨ã«å†ã³æ³¨æ„ã—ã¦ãã ã•ã„ã€‚


```python
def preprocess(dataset, normalize=True, expand_dims=True):
    """
    ## Prepare the data
    """
    x, y = dataset.tensors

    if normalize:
        # Scale images to the [0, 1] range
        x = x.type(torch.float32) / 255

    if expand_dims:
        # Make sure images have shape (1, 28, 28)
        x = torch.unsqueeze(x, 1)
    
    return TensorDataset(x, y)
```

æ¬¡ã«ã€ã“ã® `preprocess` ã‚¹ãƒ†ãƒƒãƒ—ã‚’ `wandb.Artifact` ãƒ­ã‚®ãƒ³ã‚°ã§è¨ˆæ¸¬ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ç¤ºã—ã¾ã™ã€‚

ä»¥ä¸‹ã®ä¾‹ã§ã¯ã€`Artifact` ã‚’ `use` ã—ã€
ã“ã‚Œã¯æ–°ã—ã„ã“ã¨ã§ã™ã€‚
ãã—ã¦ã€ãã‚Œã‚’ `log` ã—ã¾ã™ã€‚
ã“ã‚Œã¯æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã¨åŒã˜ã§ã™ã€‚
`Artifact` ã¯ `Run` ã®å…¥åŠ›ã¨å‡ºåŠ›ã®ä¸¡æ–¹ã§ã™ã€‚

æ–°ã—ã„ `job_type` ã§ã‚ã‚‹ `preprocess-data` ã‚’ä½¿ç”¨ã—ã¦ã€
ã“ã‚ŒãŒå‰ã®ã‚¸ãƒ§ãƒ–ã¨ã¯ç•°ãªã‚‹ç¨®é¡ã®ã‚¸ãƒ§ãƒ–ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¢ºã«ã—ã¾ã™ã€‚


```python
def preprocess_and_log(steps):

    with wandb.init(project="artifacts-example", job_type="preprocess-data") as run:

        processed_data = wandb.Artifact(
            "mnist-preprocess", type="dataset",
            description="Preprocessed MNIST dataset",
            metadata=steps)
         
        # âœ”ï¸ ä½¿ç”¨ã™ã‚‹ Artifact ã‚’å®£è¨€ã—ã¾ã™
        raw_data_artifact = run.use_artifact('mnist-raw:latest')

        # ğŸ“¥ å¿…è¦ã«å¿œã˜ã¦ã€Artifact ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™
        raw_dataset = raw_data_artifact.download()
        
        for split in ["training", "validation", "test"]:
            raw_split = read(raw_dataset, split)
            processed_dataset = preprocess(raw_split, **steps)

            with processed_data.new_file(split + ".pt", mode="wb") as file:
                x, y = processed_dataset.tensors
                torch.save((x, y), file)

        run.log_artifact(processed_data)


def read(data_dir, split):
    filename = split + ".pt"
    x, y = torch.load(os.path.join(data_dir, filename))

    return TensorDataset(x, y)
```

ã“ã“ã§æ³¨æ„ã™ã¹ãã“ã¨ã® 1 ã¤ã¯ã€preprocessing ã® `steps` ãŒ
`metadata` ã¨ã—ã¦ `preprocessed_data` ã¨å…±ã«ä¿å­˜ã•ã‚Œã‚‹ã“ã¨ã§ã™ã€‚

å®Ÿé¨“ã®å†ç¾æ€§ã‚’é«˜ã‚ã‚ˆã†ã¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€
å¤šãã® metadata ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

ã¾ãŸã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã€Œ`large artifact`ã€ã§ã‚ã£ã¦ã‚‚ã€
`download` ã‚¹ãƒ†ãƒƒãƒ—ã¯ 1 ç§’ä»¥å†…ã«å®Œäº†ã—ã¾ã™ã€‚

è©³ç´°ã«ã¤ã„ã¦ã¯ã€ä»¥ä¸‹ã® markdown ã‚»ãƒ«ã‚’å±•é–‹ã—ã¦ãã ã•ã„ã€‚


```python
steps = {"normalize": True,
         "expand_dims": True}

preprocess_and_log(steps)
```

#### `run.use_artifact`

ã“ã‚Œã‚‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã‚ˆã‚Šç°¡å˜ã§ã™ã€‚ã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼ã¯ã€`Artifact` ã® `name` ã«åŠ ãˆã¦ã€ã‚‚ã†å°‘ã—ã ã‘çŸ¥ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ãã®ã€Œã‚‚ã†å°‘ã—ã€ã¯ã€å¿…è¦ãª `Artifact` ã®ç‰¹å®šã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã® `alias` ã§ã™ã€‚

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€æœ€å¾Œã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã¯ `latest` ã®ã‚¿ã‚°ãŒä»˜ã‘ã‚‰ã‚Œã¾ã™ã€‚
ãã‚Œä»¥å¤–ã®å ´åˆã¯ã€`v0`/`v1` ãªã©ã®å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’é¸æŠã™ã‚‹ã‹ã€`best` ã‚„ `jit-script` ãªã©ã®ç‹¬è‡ªã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’æŒ‡å®šã§ãã¾ã™ã€‚
[Docker Hub](https://hub.docker.com/) ã‚¿ã‚°ã¨åŒæ§˜ã«ã€
ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¯ `:` ã§åå‰ã‹ã‚‰åŒºåˆ‡ã‚‰ã‚Œã¾ã™ã€‚
ã—ãŸãŒã£ã¦ã€å¿…è¦ãª `Artifact` ã¯ `mnist-raw:latest` ã§ã™ã€‚

> **çµŒé¨“å‰‡**: ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¯çŸ­ãç°¡æ½”ã«ã—ã¦ãã ã•ã„ã€‚
`Artifact` ãŒä½•ã‚‰ã‹ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’æº€ãŸã™å ´åˆã¯ã€`latest` ã‚„ `best` ãªã©ã®ã‚«ã‚¹ã‚¿ãƒ  `alias` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

#### `artifact.download`

ã“ã“ã§ã€`download` å‘¼ã³å‡ºã—ã«ã¤ã„ã¦å¿ƒé…ã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
åˆ¥ã®ã‚³ãƒ”ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€ãƒ¡ãƒ¢ãƒªã¸ã®è² æ‹…ãŒ 2 å€ã«ãªã‚‹ã®ã§ã¯ãªã„ã§ã—ã‚‡ã†ã‹?

ã”å¿ƒé…ãªãã€‚å®Ÿéš›ã«ä½•ã‹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‰ã«ã€
é©åˆ‡ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒãƒ­ãƒ¼ã‚«ãƒ«ã§åˆ©ç”¨å¯èƒ½ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚
ã“ã‚Œã«ã¯ã€[torrent](https://en.wikipedia.org/wiki/Torrent_file) ã¨ [`git` ã«ã‚ˆã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†](https://blog.thoughtram.io/git/2014/11/18/the-anatomy-of-a-git-commit.html) ã®åŸºç›¤ã¨ãªã‚‹ã®ã¨åŒã˜ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã§ã‚ã‚‹ãƒãƒƒã‚·ãƒ¥ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

`Artifact` ãŒä½œæˆãŠã‚ˆã³ãƒ­ã‚°ã•ã‚Œã‚‹ã¨ã€
ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ¼ã® `artifacts` ã¨ã„ã†åå‰ã®ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ¼ã§ã„ã£ã±ã„ã«ãªã‚Šå§‹ã‚ã¾ã™ã€‚
å„ `Artifact` ã« 1 ã¤ãšã¤ã§ã™ã€‚
`!tree artifacts` ã§ãã®å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚


```python
!tree artifacts
```

#### Artifacts ãƒšãƒ¼ã‚¸

`Artifact` ã‚’ãƒ­ã‚°ã—ã¦ä½¿ç”¨ã—ãŸã®ã§ã€
Run ãƒšãƒ¼ã‚¸ã® [Artifacts] ã‚¿ãƒ–ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚

`wandb` å‡ºåŠ›ã‹ã‚‰ Run ãƒšãƒ¼ã‚¸ã® URL ã«ç§»å‹•ã—ã€
å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ [Artifacts] ã‚¿ãƒ–ã‚’é¸æŠã—ã¾ã™ã€‚
(ã“ã‚Œã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¤ã‚³ãƒ³ãŒä»˜ã„ãŸã‚‚ã®ã§ã€
3 ã¤ã®ãƒ›ãƒƒã‚±ãƒ¼ãƒ‘ãƒƒã‚¯ãŒäº’ã„ã«ç©ã¿é‡ã­ã‚‰ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™)ã€‚

[**å…¥åŠ› Artifacts**] ãƒ†ãƒ¼ãƒ–ãƒ«ã¾ãŸã¯ [**å‡ºåŠ› Artifacts**] ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã€
æ¬¡ã«ã‚¿ãƒ– ([**æ¦‚è¦**]ã€[**Metadata**]) ã‚’ç¢ºèªã—ã¦ã€
`Artifact` ã«ã¤ã„ã¦ãƒ­ã‚°ã«è¨˜éŒ²ã•ã‚ŒãŸã™ã¹ã¦ã‚’ç¢ºèªã—ã¾ã™ã€‚

ç‰¹ã« [**ã‚°ãƒ©ãƒ•è¡¨ç¤º**] ãŒæ°—ã«å…¥ã£ã¦ã„ã¾ã™ã€‚
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ã‚°ãƒ©ãƒ•ã¯
`Artifact` ã® `type` ã¨
`Run` ã® `job_type` ã‚’ 2 ç¨®é¡ã®ãƒãƒ¼ãƒ‰ã¨ã—ã¦è¡¨ç¤ºã—ã€
æ¶ˆè²»ã¨ç”Ÿæˆã‚’è¡¨ã™çŸ¢å°ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚

### ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ã‚°

ã“ã‚Œã§ `Artifact` ã® API ãŒã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹ã®ã«ååˆ†ã§ã™ãŒã€
ã“ã®ä¾‹ã‚’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æœ€å¾Œã¾ã§å®Ÿè¡Œã—ã¦ã€
`Artifact` ãŒ ML workflow ã‚’ã©ã®ã‚ˆã†ã«æ”¹å–„ã§ãã‚‹ã‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

ã“ã®æœ€åˆã®ã‚»ãƒ«ã¯ã€PyTorch ã§ DNN `model` ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ã“ã‚Œã¯éå¸¸ã«å˜ç´”ãª ConvNet ã§ã™ã€‚

æœ€åˆã¯ `model` ã‚’åˆæœŸåŒ–ã™ã‚‹ã ã‘ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯è¡Œã„ã¾ã›ã‚“ã€‚
ãã†ã™ã‚Œã°ã€ä»–ã®ã™ã¹ã¦ã‚’ä¸€å®šã«ä¿ã¡ãªãŒã‚‰ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç¹°ã‚Šè¿”ã™ã“ã¨ãŒã§ãã¾ã™ã€‚


```python
from math import floor

import torch.nn as nn

class ConvNet(nn.Module):
    def __init__(self, hidden_layer_sizes=[32, 64],
                  kernel_sizes=[3],
                  activation="ReLU",
                  pool_sizes=[2],
                  dropout=0.5,
                  num_classes=num_classes,
                  input_shape=input_shape):
      
        super(ConvNet, self).__init__()

        self.layer1 = nn.Sequential(
              nn.Conv2d(in_channels=input_shape[0], out_channels=hidden_layer_sizes[0], kernel_size=kernel_sizes[0]),
              getattr(nn, activation)(),
              nn.MaxPool2d(kernel_size=pool_sizes[0])
        )
        self.layer2 = nn.Sequential(
              nn.Conv2d(in_channels=hidden_layer_sizes[0], out_channels=hidden_layer_sizes[-1], kernel_size=kernel_sizes[-1]),
              getattr(nn, activation)(),
              nn.MaxPool2d(kernel_size=pool_sizes[-1])
        )
        self.layer3 = nn.Sequential(
              nn.Flatten(),
              nn.Dropout(dropout)
        )

        fc_input_dims = floor((input_shape[1] - kernel_sizes[0] + 1) / pool_sizes[0]) # layer 1 output size
        fc_input_dims = floor((fc_input_dims - kernel_sizes[-1] + 1) / pool_sizes[-1]) # layer 2 output size
        fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-1] # layer 3 output size

        self.fc = nn.Linear(fc_input_dims, num_classes)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.fc(x)
        return x
```

ã“ã“ã§ã¯ã€W&B ã‚’ä½¿ç”¨ã—ã¦ run ã‚’è¿½è·¡ã—ã¦ã„ã‚‹ãŸã‚ã€
[`wandb.config`](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-config/Configs_in_W%26B.ipynb)
ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ã—ã¦ã™ã¹ã¦ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¦ã„ã¾ã™ã€‚

ãã® `config` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã® `dict`ionary ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯éå¸¸ã«å½¹ç«‹ã¤ `metadata` ã§ã‚ã‚‹ãŸã‚ã€å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚


```python
def build_model_and_log(config):
    with wandb.init(project="artifacts-example", job_type="initialize", config=config) as run:
        config = wandb.config
        
        model = ConvNet(**config)

        model_artifact = wandb.Artifact(
            "convnet", type="model",
            description="Simple AlexNet style CNN",
            metadata=dict(config))

        torch.save(model.state_dict(), "initialized_model.pth")
        # â• Artifact ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã™ã‚‹åˆ¥ã®æ–¹æ³•
        model_artifact.add_file("initialized_model.pth")

        wandb.save("initialized_model.pth")

        run.log_artifact(model_artifact)

model_config = {"hidden_layer_sizes": [32, 64],
                "kernel_sizes": [3],
                "activation": "ReLU",
                "pool_sizes": [2],
                "dropout": 0.5,
                "num_classes": 10}

build_model_and_log(model_config)
```

#### `artifact.add_file`


ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ã‚®ãƒ³ã‚°ä¾‹ã®ã‚ˆã†ã«ã€`new_file` ã‚’åŒæ™‚ã«æ›¸ãè¾¼ã‚“ã§ `Artifact` ã«è¿½åŠ ã™ã‚‹ä»£ã‚ã‚Šã«ã€
1 ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ•ã‚¡ã‚¤ãƒ« (ã“ã“ã§ã¯ `torch.save`) ã‚’æ›¸ãè¾¼ã¿ã€
åˆ¥ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ `add` ã§ `Artifact` ã«æ›¸ãè¾¼ã‚€ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

> **çµŒé¨“å‰‡**: é‡è¤‡ã‚’é˜²ããŸã‚ã«ã€å¯èƒ½ãªå ´åˆã¯ `new_file` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

#### ãƒ­ã‚°ã«è¨˜éŒ²ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ« Artifact ã®ä½¿ç”¨

`dataset` ã§ `use_artifact` ã‚’å‘¼ã³å‡ºã™ã“ã¨ãŒã§ãã‚‹ã®ã¨åŒã˜ã‚ˆã†ã«ã€
`initialized_model` ã§ `use_artifact` ã‚’å‘¼ã³å‡ºã—ã¦ã€
åˆ¥ã® `Run` ã§ä½¿ç”¨ã§ãã¾ã™ã€‚

ä»Šå›ã¯ã€`model` ã‚’ `train` ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

è©³ç´°ã«ã¤ã„ã¦ã¯ã€
[PyTorch ã§ W&B ã‚’è¨ˆæ¸¬ã™ã‚‹](http://wandb.me/pytorch-colab)ã«é–¢ã™ã‚‹ Colab ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚


```python
import torch.nn.functional as F

def train(model, train_loader, valid_loader, config):
    optimizer = getattr(torch.optim, config.optimizer)(model.parameters())
    model.train()
    example_ct = 0
    for epoch in range(config.epochs):
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = F.cross_entropy(output, target)
            loss.backward()
            optimizer.step()

            example_ct += len(data)

            if batch_idx % config.batch_log_interval == 0:
                print('Train Epoch: {} [{}/{} ({:.0%})]\tLoss: {:.6f}'.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                    batch_idx / len(train_loader), loss.item()))
                
                train_log(loss, example_ct, epoch)

        # evaluate the model on the validation set at each epoch
        loss, accuracy = test(model, valid_loader)  
        test_log(loss, accuracy, example_ct, epoch)

    
def test(model, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.cross_entropy(output, target, reduction='sum')  # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum()

    test_loss /= len(test_loader.dataset)

    accuracy = 100. * correct / len(test_loader.dataset)
    
    return test_loss, accuracy


def train_log(loss, example_ct, epoch):
    loss = float(loss)

    # where the magic happens
    wandb.log({"epoch": epoch, "train/loss": loss}, step=example_ct)
    print(f"Loss after " + str(example_ct).zfill(5) + f" examples: {loss:.3f}")
    

def test_log(loss, accuracy, example_ct, epoch):
    loss = float(loss)
    accuracy = float(accuracy)

    # where the magic happens
    wandb.log({"epoch": epoch, "validation/loss": loss, "validation/accuracy": accuracy}, step=example_ct)
    print(f"Loss/accuracy after " + str(example_ct).zfill(5) + f" examples: {loss:.3f}/{accuracy:.3f}")
```

ä»Šå›ã¯ã€2 ã¤ã®åˆ¥ã€…ã® `Artifact` ã‚’ç”Ÿæˆã™ã‚‹ `Run` ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

æœ€åˆã® run ãŒ `model` ã® `train`ing ã‚’å®Œäº†ã™ã‚‹ã¨ã€
`2 ç•ªç›®` ã¯ `trained-model` `Artifact` ã‚’æ¶ˆè²»ã—ã€
`test_dataset` ã§ãã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ `evaluate` ã—ã¾ã™ã€‚

ã¾ãŸã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒæœ€ã‚‚æ··ä¹±ã™ã‚‹ 32 å€‹ã®ä¾‹ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
ã“ã‚Œã¯ã€`categorical_crossentropy` ãŒæœ€ã‚‚é«˜ã„ä¾‹ã§ã™ã€‚

ã“ã‚Œã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã®å•é¡Œã‚’è¨ºæ–­ã™ã‚‹ã®ã«é©ã—ãŸæ–¹æ³•ã§ã™ã€‚


```python
def evaluate(model, test_loader):
    """
    ## Evaluate the trained model
    """

    loss, accuracy = test(model, test_loader)
    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)

    return loss, accuracy, highest_losses, hardest_examples, true_labels, predictions

def get_hardest_k_examples(model, testing_set, k=32):
    model.eval()

    loader = DataLoader(testing_set, 1, shuffle=False)

    # get the losses and predictions for each item in the dataset
    losses = None
    predictions = None
    with torch.no_grad():
        for data, target in loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = F.cross_entropy(output, target)
            pred = output.argmax(dim=1, keepdim=True)
            
            if losses is None:
                losses = loss.view((1, 1))
                predictions = pred
            else:
                losses = torch.cat((losses, loss.view((1, 1))), 0)
                predictions = torch.cat((predictions, pred), 0)

    argsort_loss = torch.argsort(losses, dim=0)

    highest_k_losses = losses[argsort_loss[-k:]]
    hardest_k_examples = testing_set[argsort_loss[-k:]][0]
    true_labels = testing_set[argsort_loss[-k:]][1]
    predicted_labels = predictions[argsort_loss[-k:]]

    return highest_k_losses, hardest_k_examples, true_labels, predicted_labels
```

ã“ã‚Œã‚‰ã®ãƒ­ã‚®ãƒ³ã‚°é–¢æ•°ã¯æ–°ã—ã„ `Artifact` æ©Ÿèƒ½ã‚’è¿½åŠ ã—ãªã„ãŸã‚ã€
ã‚³ãƒ¡ãƒ³ãƒˆã—ã¾ã›ã‚“ã€‚
`Artifact` ã‚’ `use`ingã€`download`ingã€
ãŠã‚ˆã³ `log`ing ã—ã¦ã„ã‚‹ã ã‘ã§ã™ã€‚


```python
from torch.utils.data import DataLoader

def train_and_log(config):

    with wandb.init(project="artifacts-example", job_type="train", config=config) as run:
        config = wandb.config

        data = run.use_artifact('mnist-preprocess:latest')
        data_dir = data.download()

        training_dataset =  read(data_dir, "training")
        validation_dataset = read(data_dir, "validation")

        train_loader = DataLoader(training_dataset, batch_size=config.batch_size)
        validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size)
        
        model_artifact = run.use_artifact("convnet:latest")
        model_dir = model_artifact.download()
        model_path = os.path.join(model_dir, "initialized_model.pth")
        model_config = model_artifact.metadata
        config.update(model_config)

        model = ConvNet(**model_config)
        model.load_state_dict(torch.load(model_path))
        model = model.to(device)
 
        train(model, train_loader, validation_loader, config)

        model_artifact = wandb.Artifact(
            "trained-model", type="model",
            description="Trained NN model",
            metadata=dict(model_config))

        torch.save(model.state_dict(), "trained_model.pth")
        model_artifact.add_file("trained_model.pth")
        wandb.save("trained_model.pth")

        run.log_artifact(model_artifact)

    return model

    
def evaluate_and_log(config=None):
    
    with wandb.init(project="artifacts-example", job_type="report", config=config) as run:
        data = run.use_artifact('mnist-preprocess:latest')
        data_dir = data.download()
        testing_set = read(data_dir, "test")

        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=128, shuffle=False)

        model_artifact = run.use_artifact("trained-model:latest")
        model_dir = model_artifact.download()
        model_path = os.path.join(model_dir, "trained_model.pth")
        model_config = model_artifact.metadata

        model = ConvNet(**model_config)
        model.load_state_dict(torch.load(model_path))
        model.to(device)

        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)

        run.summary.update({"loss": loss, "accuracy": accuracy})

        wandb.log({"high-loss-examples":
            [wandb.Image(hard_example, caption=str(int(pred)) + "," +  str(int(label)))
             for hard_example, pred, label in zip(hardest_examples, preds, true_labels)]})
```


```python
train_config = {"batch_size": 128,
                "epochs": 5,
                "batch_log_interval": 25,
                "optimizer": "Adam"}

model = train_and_log(train_config)
evaluate_and_log()
```