---
title: ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
menu:
  tutorials:
    identifier: artifacts
weight: 4
---

{{< cta-button colabLink="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W&B_Artifacts.ipynb" >}}
ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€W&B Artifacts ã‚’åˆ©ç”¨ã—ã¦ ML å®Ÿé¨“ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

[ãƒ“ãƒ‡ã‚ªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://tiny.cc/wb-artifacts-video)ã‚‚å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚

## artifacts ã«ã¤ã„ã¦

artifact ã¯ã€ã‚®ãƒªã‚·ãƒ£ã®[ã‚¢ãƒ³ãƒ•ã‚©ãƒ©å£º](https://en.wikipedia.org/wiki/Amphora)ã®ã‚ˆã†ãªã€Œç”Ÿã¿å‡ºã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆç”Ÿæˆç‰©ï¼‰ã€ã§ã‚ã‚Šã€ä½•ã‚‰ã‹ã®ãƒ—ãƒ­ã‚»ã‚¹ã®å‡ºåŠ›ã§ã™ã€‚
ML ã®é ˜åŸŸã§ã¯ã€ã‚‚ã£ã¨ã‚‚é‡è¦ãª artifact ã¯ _ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ_ ã¨ _ãƒ¢ãƒ‡ãƒ«_ ã§ã™ã€‚

ãã—ã¦ã€[ã‚³ãƒ­ãƒŠãƒ‰ã®åå­—æ¶](https://indianajones.fandom.com/wiki/Cross_of_Coronado)ã®ã‚ˆã†ã«ã€ã“ã†ã—ãŸå¤§äº‹ãª artifact ã¯ã€Œåšç‰©é¤¨ã€ã«åã‚ã‚‹ä¾¡å€¤ãŒã‚ã‚Šã¾ã™ã€‚
ã¤ã¾ã‚Šã€ãã¡ã‚“ã¨ã‚«ã‚¿ãƒ­ã‚°åŒ–ãƒ»æ•´ç†ã—ã¦ã€
ã‚ãªãŸè‡ªèº«ãƒ»ãƒãƒ¼ãƒ ã€ãã—ã¦ ML ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å…¨ä½“ãŒãã“ã‹ã‚‰å­¦ã¹ã‚‹çŠ¶æ…‹ã«ã—ã¾ã—ã‚‡ã†ã€‚
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å±¥æ­´ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ãªã„è€…ã¯ã€åŒã˜å¤±æ•—ã‚’ç¹°ã‚Šè¿”ã™é‹å‘½ã«ã‚ã‚‹ã®ã§ã™ã€‚

W&B ã® Artifacts API ã‚’ä½¿ãˆã°ã€`Artifact` ã‚’ W&B ã® `Run` ã®å‡ºåŠ›ã¨ã—ã¦è¨˜éŒ²ã—ãŸã‚Šã€é€†ã« `Artifact` ã‚’ `Run` ã®å…¥åŠ›ã¨ã—ã¦åˆ©ç”¨ã§ãã¾ã™ã€‚ãŸã¨ãˆã°ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã® run ãŒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å…¥åŠ›ã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å‡ºåŠ›ã™ã‚‹å ´åˆã§ã™ã€‚

{{< img src="/images/tutorials/artifacts-diagram.png" alt="Artifacts workflow diagram" >}}

1 ã¤ã® run ãŒã»ã‹ã® run ã®å‡ºåŠ›ã‚’å…¥åŠ›ã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã‚‹ã®ã§ã€`Artifact` ã¨ `Run` ã¯æœ‰å‘ã‚°ãƒ©ãƒ•ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ†ã‚£ãƒˆ [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph)ï¼‰ã®ã‚ˆã†ã«çµã³ä»˜ãã¾ã™ã€‚ãƒãƒ¼ãƒ‰ã¯ `Artifact` ã¨ `Run` ã§æ§‹æˆã•ã‚Œã€çŸ¢å°ãŒ `Run` ã¨ãã‚ŒãŒæ‰±ã†ï¼ˆæ¶ˆè²»ãƒ»ç”Ÿæˆã™ã‚‹ï¼‰`Artifact` ã‚’ã¤ãªãã¾ã™ã€‚

## artifacts ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç®¡ç†

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« & ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

Artifacts ã¯ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ `0.9.2` ä»¥é™ã® Python ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

ML Python ã‚¹ã‚¿ãƒƒã‚¯ã®å¤šãã¨åŒã˜ãã€`pip` ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã¾ã™ã€‚


```python
# wandb ãƒãƒ¼ã‚¸ãƒ§ãƒ³ 0.9.2 ä»¥é™ã«å¯¾å¿œ
!pip install wandb -qqq
!apt install tree
```


```python
import os
import wandb
```

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨˜éŒ²

ã¾ãšã¯ Artifacts ã®å®šç¾©ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚

ã“ã®ä¾‹ã¯ã€PyTorch ã®
["Basic MNIST Example"](https://github.com/pytorch/examples/tree/master/mnist/)
ã«åŸºã¥ã„ã¦ã„ã¾ã™ãŒã€[TensorFlow](https://wandb.me/artifacts-colab) ã‚„ä»–ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ã‚ã‚‹ã„ã¯ç´”ç²‹ãª Python ã§ã‚‚åŒã˜ã‚ˆã†ã«å®Ÿè£…ã§ãã¾ã™ã€‚

`Dataset` ã‚’å®šç¾©ã—ã¾ã™ï¼š
- `train`ç”¨ã®ã‚»ãƒƒãƒˆï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ç”¨ï¼‰
- `validation`ç”¨ã®ã‚»ãƒƒãƒˆï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ç”¨ï¼‰
- `test`ç”¨ã®ã‚»ãƒƒãƒˆï¼ˆæœ€çµ‚ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ç”¨ï¼‰

ä»¥ä¸‹ã®ã‚»ãƒ«ã§ã€ã“ã‚Œã‚‰ 3 ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å®šç¾©ã—ã¾ã™ã€‚


```python
import random 

import torch
import torchvision
from torch.utils.data import TensorDataset
from tqdm.auto import tqdm

# æŒ™å‹•ã‚’æ±ºå®šçš„ã«ã™ã‚‹
torch.backends.cudnn.deterministic = True
random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)

# ãƒ‡ãƒã‚¤ã‚¹æŒ‡å®š
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# ãƒ‡ãƒ¼ã‚¿é–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
num_classes = 10
input_shape = (1, 28, 28)

# MNIST ã®é…ã„ãƒŸãƒ©ãƒ¼ã‚µã‚¤ãƒˆã‚’é™¤å¤–
torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors
                                      if not mirror.startswith("http://yann.lecun.com")]

def load(train_size=50_000):
    """
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    """

    # ãƒ‡ãƒ¼ã‚¿ã‚’ train/test ã«åˆ†å‰²
    train = torchvision.datasets.MNIST("./", train=True, download=True)
    test = torchvision.datasets.MNIST("./", train=False, download=True)
    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)

    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ç”¨ã«æ¤œè¨¼ã‚»ãƒƒãƒˆã‚’åˆ†å‰²
    x_train, x_val = x_train[:train_size], x_train[train_size:]
    y_train, y_val = y_train[:train_size], y_train[train_size:]

    training_set = TensorDataset(x_train, y_train)
    validation_set = TensorDataset(x_val, y_val)
    test_set = TensorDataset(x_test, y_test)

    datasets = [training_set, validation_set, test_set]

    return datasets
```

ã“ã®ä¾‹ã§ç¹°ã‚Šè¿”ã—ä½¿ã†ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨­å®šã—ã¦ã„ã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚’ Artifact ã¨ã—ã¦è¨˜éŒ²ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã¯ã€ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã®ã‚³ãƒ¼ãƒ‰ã‚’ãƒ©ãƒƒãƒ—ã™ã‚‹å½¢ã§æ›¸ã‹ã‚Œã¦ã„ã¾ã™ã€‚
ã“ã®å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ `load` ã®ã‚³ãƒ¼ãƒ‰ã¨ã€ãã‚Œã‚’è¨˜éŒ²ã™ã‚‹ `load_and_log` ã®ã‚³ãƒ¼ãƒ‰ãŒåˆ†ã‘ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

ã“ã‚Œã¯ãŠã™ã™ã‚ã®ã‚„ã‚Šæ–¹ã§ã™ã€‚

ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ Artifacts ã¨ã—ã¦è¨˜éŒ²ã™ã‚‹ã«ã¯ã€
1. `wandb.init()` ã§ `Run` ã‚’ä½œæˆï¼ˆL4ï¼‰
2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã® `Artifact` ã‚’ä½œæˆï¼ˆL10ï¼‰
3. é–¢é€£ã™ã‚‹ `file` ã‚’ä¿å­˜ãƒ»è¨˜éŒ²ï¼ˆL20, L23ï¼‰

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’è¦‹ã¦ã€ãã®å¾Œã®è©³ç´°èª¬æ˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚‚å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚


```python
def load_and_log():

    # Run ã‚’é–‹å§‹ã€‚ã‚¿ã‚¤ãƒ—ã¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚‚æŒ‡å®š
    with wandb.init(project="artifacts-example", job_type="load-data") as run:
        
        datasets = load()  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¯åˆ¥é–¢æ•°ã«åˆ†é›¢
        names = ["training", "validation", "test"]

        # ğŸº Artifact ã®ä½œæˆ
        raw_data = wandb.Artifact(
            "mnist-raw", type="dataset",
            description="Raw MNIST dataset, split into train/val/test",
            metadata={"source": "torchvision.datasets.MNIST",
                      "sizes": [len(dataset) for dataset in datasets]})

        for name, data in zip(names, datasets):
            # ğŸ£ artifact ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°è¦è¿½åŠ ã—ã€ä¸­èº«ã‚’æ›¸ãè¾¼ã‚€
            with raw_data.new_file(name + ".pt", mode="wb") as file:
                x, y = data.tensors
                torch.save((x, y), file)

        # âœï¸ Artifact ã‚’ W&B ã«ä¿å­˜
        run.log_artifact(raw_data)

load_and_log()
```

#### `wandb.init()`

`Artifact` ã‚’ç”Ÿæˆã™ã‚‹ `Run` ã‚’ä½œæˆã™ã‚‹éš›ã¯ã€ãã‚ŒãŒå±ã™ã‚‹ `project` ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«ã‚ˆã£ã¦ã€
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç²’åº¦ã¯ `car-that-drives-itself` ã®ã‚ˆã†ã«å¤§ããã¦ã‚‚ã‚ˆã„ã—ã€
`iterative-architecture-experiment-117` ã®ã‚ˆã†ã«å°ã•ãã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚

> **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**: å¯èƒ½ãªã‚‰ã€åŒã˜ `Artifact` ã‚’å…±æœ‰ã™ã‚‹ `Run` ã‚’å˜ä¸€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¾ã¨ã‚ã¾ã—ã‚‡ã†ã€‚ã‚·ãƒ³ãƒ—ãƒ«ã«ç®¡ç†ã§ãã¾ã™ã€‚ãªãŠã€`Artifact` ã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è·¨ã„ã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚

æ§˜ã€…ãªã‚¸ãƒ§ãƒ–ç¨®åˆ¥ã®ç®¡ç†ã«ã¯ã€`job_type` ã‚’æŒ‡å®šã™ã‚‹ã®ã‚‚ãŠã™ã™ã‚ã§ã™ã€‚
Artifacts ã®ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’ãã‚Œã„ã«ä¿ã¦ã¾ã™ã€‚

> **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**: `job_type` ã¯ã§ãã‚‹ã ã‘å…·ä½“çš„ã«ã—ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«åˆ†ã‘ã¾ã—ã‚‡ã†ã€‚ã“ã“ã§ã¯ã€`load`ï¼ˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼‰ã¨ `preprocess`ï¼ˆå‰å‡¦ç†ï¼‰ã‚’åˆ†å‰²ã—ã¦ã„ã¾ã™ã€‚

#### `wandb.Artifact`

ä½•ã‹ã‚’ `Artifact` ã¨ã—ã¦è¨˜éŒ²ã—ãŸã„å ´åˆã€ã¾ãšã¯ `Artifact` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

ã™ã¹ã¦ã® `Artifact` ã«ã¯ `name` ãŒå¿…è¦ã§ã€ã“ã‚ŒãŒç¬¬ä¸€å¼•æ•°ã§ã™ã€‚

> **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**: `name` ã¯ã€åˆ†ã‹ã‚Šã‚„ã™ãã€è¦šãˆã‚„ã™ãã€ã‚¿ã‚¤ãƒ—ã—ã‚„ã™ã„ã‚‚ã®ã«ã—ã¾ã—ã‚‡ã†ã€‚ãƒã‚¤ãƒ•ãƒ³åŒºåˆ‡ã‚Šï¼†ã‚³ãƒ¼ãƒ‰å†…ã®å¤‰æ•°åã«åˆã‚ã›ã‚‹ã®ãŒãŠã™ã™ã‚ã§ã™ã€‚

`type` ã‚‚è¨­å®šã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€`Run` ã® `job_type` ã¨åŒã˜ãã€`Artifact` ã‚°ãƒ©ãƒ•ã®æ•´ç†ã«ä½¿ã‚ã‚Œã¾ã™ã€‚

> **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**: `type` ã¯ã§ãã‚‹ã ã‘ã‚·ãƒ³ãƒ—ãƒ«ã«ã€‚`mnist-data-YYYYMMDD` ã‚ˆã‚Š `dataset` ã‚„ `model` ãŒãŠã™ã™ã‚ã§ã™ã€‚

`description` ã‚„ `metadata`ï¼ˆè¾æ›¸å‹ï¼‰ã‚‚æ·»ä»˜å¯èƒ½ã§ã™ã€‚
`metadata` ã¯ JSON ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªã‚‚ã®ã«ã—ã¾ã—ã‚‡ã†ã€‚

> **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**: `metadata` ã«ã¯ã€ã§ãã‚‹ã ã‘è©³ç´°ãªæƒ…å ±ã‚’æ›¸ãã¾ã—ã‚‡ã†ã€‚

#### `artifact.new_file` ã¨ `run.log_artifact`

`Artifact` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ãŸã‚‰ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã—ã¾ã™ã€‚

ã€Œãƒ•ã‚¡ã‚¤ãƒ« *s*ã€ã¨è¤‡æ•°å½¢ãªã®ãŒãƒã‚¤ãƒ³ãƒˆã€‚
`Artifact` ã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚ˆã†ãªéšå±¤æ§‹é€ ã‚’æŒã£ã¦ãŠã‚Šã€
ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ ¼ç´ã§ãã¾ã™ã€‚

> **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**: å†…å®¹ã‚’è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã§ãã‚‹ãªã‚‰ã€ã§ãã‚‹ã ã‘åˆ†å‰²ã—ã¾ã—ã‚‡ã†ã€‚å¤§è¦æ¨¡åŒ–ã™ã‚‹æ™‚ã®åŠ©ã‘ã«ãªã‚Šã¾ã™ã€‚

`new_file` ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€
ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›¸ãè¾¼ã¿ã¨ artifact ã¸ã®æ·»ä»˜ã‚’åŒæ™‚ã«è¡Œã„ã¾ã™ã€‚
å¾Œã»ã© `add_file` ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆæ›¸ãè¾¼ã¿ã¨æ·»ä»˜ã‚’åˆ†é›¢ï¼‰ã‚‚ç´¹ä»‹ã—ã¾ã™ã€‚

ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ ãŒçµ‚ã‚ã£ãŸã‚‰ã€`log_artifact` ã§ [wandb.ai](https://wandb.ai) ã«è¨˜éŒ²ã—ã¾ã—ã‚‡ã†ã€‚

å‡ºåŠ›ã«ã„ãã¤ã‹ã® URL ãŒç¾ã‚Œã¾ã™ã€‚
ãã®ä¸­ã« Run ãƒšãƒ¼ã‚¸ã® URL ã‚‚å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
ã“ã“ã§ Run ã®çµæœã‚„ã€
è¨˜éŒ²ã•ã‚ŒãŸ `Artifact` ã‚’ç¢ºèªã§ãã¾ã™ã€‚

Run ãƒšãƒ¼ã‚¸ã®ä»–ã®è¦ç´ ã‚’æ´»ç”¨ã—ãŸä¾‹ã‚‚ã€å¾Œã»ã©ç´¹ä»‹ã—ã¾ã™ã€‚

### è¨˜éŒ²æ¸ˆã¿ Dataset Artifact ã®åˆ©ç”¨

W&B ã® `Artifact` ã¯ã€åšç‰©é¤¨ã® artifact ã¨ç•°ãªã‚Šã€
ã€Œä½¿ã‚ã‚Œã‚‹ã€ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚ä¿å­˜ã ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

å…·ä½“ä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

ä¸‹ã®ã‚»ãƒ«ã§ã¯ã€ç”Ÿã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å—ã‘å–ã‚Šã€
æ­£è¦åŒ–ãƒ»å½¢çŠ¶åŠ å·¥ã—ãŸ `preprocess` æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®šç¾©ã—ã¦ã„ã¾ã™ã€‚

ä»Šå›ã‚‚ã€ä¸»è¦ãªãƒ­ã‚¸ãƒƒã‚¯ï¼ˆ`preprocess`ï¼‰ã¨ã€
`wandb` ã¨ã®é€£æºéƒ¨åˆ†ã¯åˆ†ã‘ã¦å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚


```python
def preprocess(dataset, normalize=True, expand_dims=True):
    """
    ## ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
    """
    x, y = dataset.tensors

    if normalize:
        # ç”»åƒã‚’ [0, 1] ç¯„å›²ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        x = x.type(torch.float32) / 255

    if expand_dims:
        # ç”»åƒã®å½¢çŠ¶ã‚’ (1, 28, 28) ã«æƒãˆã‚‹
        x = torch.unsqueeze(x, 1)
    
    return TensorDataset(x, y)
```

æ¬¡ã¯ã€ã“ã® `preprocess` ã‚¹ãƒ†ãƒƒãƒ—ã‚’ `wandb.Artifact` ã§è¨˜éŒ²ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚

ã“ã®ä¾‹ã§ã¯ã€æ–°ãŸã« `Artifact` ã‚’ã€Œä½¿ã„ã€ã€ãã—ã¦ã€Œè¨˜éŒ²ã€ã—ã¦ã„ã¾ã™ã€‚
`Artifact` ã¯ `Run` ã®å…¥åŠ›ã«ã‚‚å‡ºåŠ›ã«ã‚‚ãªã‚Œã‚‹ç‚¹ãŒç‰¹å¾´ã§ã™ã€‚

`job_type` ã‚‚æ–°ã—ã `preprocess-data` ã¨ã—ã¦ã€å‰ã‚¹ãƒ†ãƒƒãƒ—ã¨ã®å·®åˆ¥åŒ–ã‚’å›³ã‚Šã¾ã™ã€‚


```python
def preprocess_and_log(steps):

    with wandb.init(project="artifacts-example", job_type="preprocess-data") as run:

        processed_data = wandb.Artifact(
            "mnist-preprocess", type="dataset",
            description="Preprocessed MNIST dataset",
            metadata=steps)
         
        # âœ”ï¸ ä½¿ç”¨ã™ã‚‹ artifact ã‚’å®£è¨€
        raw_data_artifact = run.use_artifact('mnist-raw:latest')

        # ğŸ“¥ å¿…è¦ãŒã‚ã‚Œã° artifact ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        raw_dataset = raw_data_artifact.download()
        
        for split in ["training", "validation", "test"]:
            raw_split = read(raw_dataset, split)
            processed_dataset = preprocess(raw_split, **steps)

            with processed_data.new_file(split + ".pt", mode="wb") as file:
                x, y = processed_dataset.tensors
                torch.save((x, y), file)

        run.log_artifact(processed_data)


def read(data_dir, split):
    filename = split + ".pt"
    x, y = torch.load(os.path.join(data_dir, filename))

    return TensorDataset(x, y)
```

ã“ã“ã§æ³¨ç›®ã—ãŸã„ã®ã¯ã€å‰å‡¦ç†ã® `steps`ï¼ˆè¨­å®šï¼‰ãŒ
`preprocessed_data` ã® `metadata` ã¨ã—ã¦ä¸€ç·’ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ç‚¹ã§ã™ã€‚

å®Ÿé¨“ã‚’å†ç¾å¯èƒ½ã«ã—ãŸã„å ´åˆã€
è±Šå¯Œãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²ã¯ã¨ã¦ã‚‚æœ‰åŠ¹ã§ã™ã€‚

ã¾ãŸã€ä»Šå›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Œå¤§ããª artifactã€ã§ã™ãŒã€
`download` ã‚¹ãƒ†ãƒƒãƒ—ã¯ 1 ç§’ã‚‚ã‹ã‹ã‚‰ãšçµ‚ã‚ã‚Šã¾ã™ã€‚

è©³ç´°ã¯ã€ä»¥ä¸‹ã® markdown ã‚»ãƒ«ã‚’é–‹ã„ã¦ç¢ºèªã§ãã¾ã™ã€‚


```python
steps = {"normalize": True,
         "expand_dims": True}

preprocess_and_log(steps)
```

#### `run.use_artifact()`

ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ã§ã™ã€‚åˆ©ç”¨è€…ã¯ã€`Artifact` ã® `name` ã¨å°‘ã—ã®è¿½åŠ æƒ…å ±ã‚’çŸ¥ã£ã¦ã„ã‚Œã°ååˆ†ã§ã™ã€‚

ãã®ã€Œå°‘ã—ã®è¿½åŠ ã€ãŒ `alias` ã§ã™ã€‚
`Artifact` ã®ç‰¹å®šã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¾ã™ã€‚

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒ `latest` ã§ã™ã€‚
`v0` / `v1` ã®ã‚ˆã†ãªç•ªå·ã‚„ `best`, `jit-script` ãªã©ç‹¬è‡ªã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚‚åˆ©ç”¨ã§ãã¾ã™ã€‚
[Docker Hub](https://hub.docker.com/) ã®ã‚¿ã‚°ã®ã‚ˆã†ã«ã€
åå‰ã¨ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¯ `:` ã§åŒºåˆ‡ã‚Šã¾ã™ã€‚
æœ¬ä¾‹ãªã‚‰ `mnist-raw:latest` ã§ã™ã€‚

> **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**: ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¯çŸ­ãåˆ†ã‹ã‚Šã‚„ã™ãã€‚`latest` ã‚„ `best` ã®ã‚ˆã†ãªç‹¬è‡ªã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¯ä¾¿åˆ©ã§ã™ã€‚

#### `artifact.download`

`download` ã®å‘¼ã³å‡ºã—ã§ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå€å¢—ã™ã‚‹ã®ã§ã¯ï¼Ÿã¨å¿ƒé…ã™ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

ã”å®‰å¿ƒãã ã•ã„ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‰ã«ã€å¯¾è±¡ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒãƒ­ãƒ¼ã‚«ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‹ã‚’ç¢ºèªã—ã¦ã„ã¾ã™ã€‚
ã“ã‚Œã¯ [torrent](https://en.wikipedia.org/wiki/Torrent_file) ã‚„ [`git`](https://blog.thoughtram.io/git/2014/11/18/the-anatomy-of-a-git-commit.html) ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨åŒã˜ããƒãƒƒã‚·ãƒ¥æŠ€è¡“ã‚’æ´»ç”¨ã—ã¦ã„ã¾ã™ã€‚

Artifact ãŒä½œæˆãƒ»è¨˜éŒ²ã•ã‚Œã‚‹ã¨ã€
ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã® `artifacts` ãƒ•ã‚©ãƒ«ãƒ€ã«
ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå¢—ãˆã¦ã„ãã¾ã™ã€‚
ãã‚Œãã‚ŒãŒä¸€ã¤ã® `Artifact` ã§ã™ã€‚
ä¸­èº«ã¯ `!tree artifacts` ã§ç¢ºèªã§ãã¾ã™ï¼š


```python
!tree artifacts
```

#### Artifacts ãƒšãƒ¼ã‚¸

`Artifact` ã‚’è¨˜éŒ²ã—ã€åˆ©ç”¨ã—ãŸä»Šã€
Run ãƒšãƒ¼ã‚¸ã® Artifacts ã‚¿ãƒ–ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

`wandb` ã®å‡ºåŠ›ã«è¡¨ç¤ºã•ã‚ŒãŸ Run ãƒšãƒ¼ã‚¸ã® URL ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€
å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€ŒArtifactsã€ã‚¿ãƒ–ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¢ã‚¤ã‚³ãƒ³ã€ã€Œãƒ›ãƒƒã‚±ãƒ¼ãƒ‘ãƒƒã‚¯ 3 ã¤é‡ã­ã€ã«è¦‹ãˆã‚‹ï¼‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

**Input Artifacts** ã¾ãŸã¯ **Output Artifacts** ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã€
**Overview** ã‚„ **Metadata** ã®ã‚¿ãƒ–ã§ãã® `Artifact` ã®å…¨è¨˜éŒ²ã‚’ç¢ºèªã§ãã¾ã™ã€‚

ç‰¹ã« **Graph View**ï¼ˆã‚°ãƒ©ãƒ•è¡¨ç¤ºï¼‰ãŒä¾¿åˆ©ã§ã™ã€‚
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€
`Artifact` ã® `type` ã¨
`Run` ã® `job_type` ã‚’ãƒãƒ¼ãƒ‰ã¨ã—ã¦çŸ¢å°ã§ã¤ãªãã€
ã©ã® Run ãŒã©ã® Artifact ã‚’æ¶ˆè²»ãƒ»ç”Ÿæˆã—ãŸã‹ãŒä¸€ç›®ã§åˆ†ã‹ã‚Šã¾ã™ã€‚

### ãƒ¢ãƒ‡ãƒ«ã®è¨˜éŒ²

ã“ã‚Œã§ `Artifact` API ã®åŸºæœ¬ã¯æŠ¼ã•ãˆã‚‰ã‚Œã¾ã—ãŸãŒã€
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€å¾Œã¾ã§ä¾‹ã‚’é€²ã‚ã¦ã€
Artifacts ãŒ ML ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«ã©ã†å½¹ç«‹ã¤ã‹ã‚‚ç´¹ä»‹ã—ã¾ã—ã‚‡ã†ã€‚

ã¾ãšã¯ PyTorch ã§ DNNï¼ˆã”ãã‚·ãƒ³ãƒ—ãƒ«ãª ConvNet ãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

ã“ã“ã§ã¯ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã®ã¿ã‚’è¡Œã„ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã—ã¾ã›ã‚“ã€‚
åŒã˜æ¡ä»¶ã§ä½•åº¦ã§ã‚‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚


```python
from math import floor

import torch.nn as nn

class ConvNet(nn.Module):
    def __init__(self, hidden_layer_sizes=[32, 64],
                  kernel_sizes=[3],
                  activation="ReLU",
                  pool_sizes=[2],
                  dropout=0.5,
                  num_classes=num_classes,
                  input_shape=input_shape):
      
        super(ConvNet, self).__init__()

        self.layer1 = nn.Sequential(
              nn.Conv2d(in_channels=input_shape[0], out_channels=hidden_layer_sizes[0], kernel_size=kernel_sizes[0]),
              getattr(nn, activation)(),
              nn.MaxPool2d(kernel_size=pool_sizes[0])
        )
        self.layer2 = nn.Sequential(
              nn.Conv2d(in_channels=hidden_layer_sizes[0], out_channels=hidden_layer_sizes[-1], kernel_size=kernel_sizes[-1]),
              getattr(nn, activation)(),
              nn.MaxPool2d(kernel_size=pool_sizes[-1])
        )
        self.layer3 = nn.Sequential(
              nn.Flatten(),
              nn.Dropout(dropout)
        )

        fc_input_dims = floor((input_shape[1] - kernel_sizes[0] + 1) / pool_sizes[0]) # layer 1 ã®å‡ºåŠ›ã‚µã‚¤ã‚º
        fc_input_dims = floor((fc_input_dims - kernel_sizes[-1] + 1) / pool_sizes[-1]) # layer 2 ã®å‡ºåŠ›ã‚µã‚¤ã‚º
        fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-1] # layer 3 ã®å‡ºåŠ›ã‚µã‚¤ã‚º

        self.fc = nn.Linear(fc_input_dims, num_classes)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.fc(x)
        return x
```

ä»Šå›ã¯ W&B ã§ run ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹ã®ã§ã€
ã™ã¹ã¦ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ [`run.config`](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-config/Configs_in_W%26B.ipynb)
ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ä¿å­˜ã—ã¾ã™ã€‚

ã“ã® `config` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆè¾æ›¸å‹ï¼‰ã¯ã€å¤§å¤‰ä¾¿åˆ©ãª `metadata` ã¨ãªã‚‹ã®ã§ã€å¿…ãšä¸€ç·’ã«è¨˜éŒ²ã—ã¾ã—ã‚‡ã†ã€‚


```python
def build_model_and_log(config):
    with wandb.init(project="artifacts-example", job_type="initialize", config=config) as run:
        config = run.config
        
        model = ConvNet(**config)

        model_artifact = wandb.Artifact(
            "convnet", type="model",
            description="Simple AlexNet style CNN",
            metadata=dict(config))

        torch.save(model.state_dict(), "initialized_model.pth")
        # â• Artifact ã¸ã®ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ æ–¹æ³•ã®åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        model_artifact.add_file("initialized_model.pth")

        run.save("initialized_model.pth")

        run.log_artifact(model_artifact)

model_config = {"hidden_layer_sizes": [32, 64],
                "kernel_sizes": [3],
                "activation": "ReLU",
                "pool_sizes": [2],
                "dropout": 0.5,
                "num_classes": 10}

build_model_and_log(model_config)
```

#### `artifact.add_file()`

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨˜éŒ²ã®ä¾‹ã§ã¯ `new_file` ã‚’ä½¿ã£ã¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã¨ artifact ã¸ã®è¿½åŠ ã‚’åŒæ™‚ã«è¡Œã„ã¾ã—ãŸãŒã€
ã“ã®æ–¹æ³•ã§ã¯ï¼ˆã“ã“ã§ã¯ `torch.save` ã®å¾Œï¼‰
ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã¨ artifact ã¸ã®è¿½åŠ ã‚’åˆ†é›¢ã—ã¦ã„ã¾ã™ã€‚

> **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**: é‡è¤‡ã‚’é˜²ããŸã‚ã€å¯èƒ½ãªé™ã‚Š `new_file` ã‚’ä½¿ã„ã¾ã—ã‚‡ã†ã€‚

#### è¨˜éŒ²æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« Artifact ã®åˆ©ç”¨

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨åŒã˜ã‚ˆã†ã«ã€`initialized_model` ã«å¯¾ã—ã¦ã‚‚
`use_artifact` ã‚’æ´»ç”¨ã—ã€ä»–ã® `Run` ã§åˆ©ç”¨ã§ãã¾ã™ã€‚

ä»Šåº¦ã¯ã€ã“ã® `model` ã‚’ `train` ã—ã¾ã—ã‚‡ã†ã€‚

è©³ç´°ã¯
[PyTorch ã§ã® W&B åˆ©ç”¨](https://wandb.me/pytorch-colab)
ã® Colab ã‚‚ã”è¦§ãã ã•ã„ã€‚


```python
import wandb
import torch.nn.functional as F

def train(model, train_loader, valid_loader, config):
    optimizer = getattr(torch.optim, config.optimizer)(model.parameters())
    model.train()
    example_ct = 0
    for epoch in range(config.epochs):
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = F.cross_entropy(output, target)
            loss.backward()
            optimizer.step()

            example_ct += len(data)

            if batch_idx % config.batch_log_interval == 0:
                print('Train Epoch: {} [{}/{} ({:.0%})]\tLoss: {:.6f}'.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                    batch_idx / len(train_loader), loss.item()))
                
                train_log(loss, example_ct, epoch)

        # å„ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«æ¤œè¨¼ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡
        loss, accuracy = test(model, valid_loader)  
        test_log(loss, accuracy, example_ct, epoch)

    
def test(model, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.cross_entropy(output, target, reduction='sum')  # ãƒãƒƒãƒæå¤±ã‚’åˆè¨ˆ
            pred = output.argmax(dim=1, keepdim=True)  # æœ€å¤§å€¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            correct += pred.eq(target.view_as(pred)).sum()

    test_loss /= len(test_loader.dataset)

    accuracy = 100. * correct / len(test_loader.dataset)
    
    return test_loss, accuracy


def train_log(loss, example_ct, epoch):
    loss = float(loss)

    # wandb ã¸ã®ãƒ­ã‚°è¨˜éŒ²
    with wandb.init(project="artifacts-example", job_type="train") as run:
        run.log({"epoch": epoch, "train/loss": loss}, step=example_ct)
        print(f"Loss after " + str(example_ct).zfill(5) + f" examples: {loss:.3f}")
    

def test_log(loss, accuracy, example_ct, epoch):
    loss = float(loss)
    accuracy = float(accuracy)

    # wandb ã¸ã®ãƒ­ã‚°è¨˜éŒ²
    with wandb.init() as run:
        run.log({"epoch": epoch, "validation/loss": loss, "validation/accuracy": accuracy}, step=example_ct)
        print(f"Loss/accuracy after " + str(example_ct).zfill(5) + f" examples: {loss:.3f}/{accuracy:.3f}")
```

ä»Šå›ã¯ 2 å›ã® `Artifact` ç”Ÿæˆã‚’ã¨ã‚‚ãªã† `Run` ã‚’è¡Œã„ã¾ã™ã€‚

æœ€åˆã® `model` ã® `train` ãŒçµ‚ã‚ã£ãŸã‚‰ã€
2 å›ç›®ã® run ã§ `trained-model` ã® `Artifact` ã‚’ä½¿ã„ã€
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ€§èƒ½è©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚

ã¾ãŸã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒã‚‚ã£ã¨ã‚‚è‹¦æ‰‹ã¨ã™ã‚‹ 32 å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ`categorical_crossentropy` ãŒæœ€å¤§ã¨ãªã‚‹ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

ã“ã‚Œã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚„ãƒ¢ãƒ‡ãƒ«ã®å•é¡Œç‚¹ã‚’ç™ºè¦‹ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚


```python
def evaluate(model, test_loader):
    """
    ## ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
    """

    loss, accuracy = test(model, test_loader)
    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)

    return loss, accuracy, highest_losses, hardest_examples, true_labels, predictions

def get_hardest_k_examples(model, testing_set, k=32):
    model.eval()

    loader = DataLoader(testing_set, 1, shuffle=False)

    # å„ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®æå¤±ã¨äºˆæ¸¬ã‚’å–å¾—
    losses = None
    predictions = None
    with torch.no_grad():
        for data, target in loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = F.cross_entropy(output, target)
            pred = output.argmax(dim=1, keepdim=True)
            
            if losses is None:
                losses = loss.view((1, 1))
                predictions = pred
            else:
                losses = torch.cat((losses, loss.view((1, 1))), 0)
                predictions = torch.cat((predictions, pred), 0)

    argsort_loss = torch.argsort(losses, dim=0)

    highest_k_losses = losses[argsort_loss[-k:]]
    hardest_k_examples = testing_set[argsort_loss[-k:]][0]
    true_labels = testing_set[argsort_loss[-k:]][1]
    predicted_labels = predictions[argsort_loss[-k:]]

    return highest_k_losses, hardest_k_examples, true_labels, predicted_labels
```

ã“ã‚Œã‚‰ã®ãƒ­ã‚°è¨˜éŒ²ç”¨é–¢æ•°ã«ã¯æ–°ã—ã„ Artifact æ©Ÿèƒ½ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
`use`ãƒ»`download`ãƒ»`log` ã®åŸºæœ¬æ“ä½œã ã‘ã§ã™ã€‚


```python
from torch.utils.data import DataLoader

def train_and_log(config):

    with wandb.init(project="artifacts-example", job_type="train", config=config) as run:
        config = run.config

        data = run.use_artifact('mnist-preprocess:latest')
        data_dir = data.download()

        training_dataset =  read(data_dir, "training")
        validation_dataset = read(data_dir, "validation")

        train_loader = DataLoader(training_dataset, batch_size=config.batch_size)
        validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size)
        
        model_artifact = run.use_artifact("convnet:latest")
        model_dir = model_artifact.download()
        model_path = os.path.join(model_dir, "initialized_model.pth")
        model_config = model_artifact.metadata
        config.update(model_config)

        model = ConvNet(**model_config)
        model.load_state_dict(torch.load(model_path))
        model = model.to(device)
 
        train(model, train_loader, validation_loader, config)

        model_artifact = wandb.Artifact(
            "trained-model", type="model",
            description="Trained NN model",
            metadata=dict(model_config))

        torch.save(model.state_dict(), "trained_model.pth")
        model_artifact.add_file("trained_model.pth")
        run.save("trained_model.pth")

        run.log_artifact(model_artifact)

    return model

    
def evaluate_and_log(config=None):
    
    with wandb.init(project="artifacts-example", job_type="report", config=config) as run:
        data = run.use_artifact('mnist-preprocess:latest')
        data_dir = data.download()
        testing_set = read(data_dir, "test")

        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=128, shuffle=False)

        model_artifact = run.use_artifact("trained-model:latest")
        model_dir = model_artifact.download()
        model_path = os.path.join(model_dir, "trained_model.pth")
        model_config = model_artifact.metadata

        model = ConvNet(**model_config)
        model.load_state_dict(torch.load(model_path))
        model.to(device)

        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)

        run.summary.update({"loss": loss, "accuracy": accuracy})

        run.log({"high-loss-examples":
            [wandb.Image(hard_example, caption=str(int(pred)) + "," +  str(int(label)))
             for hard_example, pred, label in zip(hardest_examples, preds, true_labels)]})
```


```python
train_config = {"batch_size": 128,
                "epochs": 5,
                "batch_log_interval": 25,
                "optimizer": "Adam"}

model = train_and_log(train_config)
evaluate_and_log()
```