---
title: "가드레일 설정하기"
description: "프로덕션 애플리케이션에서 LLM 안전성을 확보하고 출력 품질을 측정하세요"
---

가드레일은 LLM 심사 모델의 점수를 바탕으로 LLM 애플리케이션의 동작에 적극적으로 개입합니다. 출력이 사용자에게 전달되기 전에 실시간으로 실행되며, 점수가 임계값을 초과하면 응답을 차단하거나 수정할 수 있습니다. 가드레일을 사용해 유해한 콘텐츠를 차단하고, 개인 식별 정보(PII)가 포함된 응답을 필터링하거나, 사용자로부터의 모욕적·공격적인 입력을 차단할 수 있습니다.

<div id="how-weave-guardrails-work">
  ## Weave 가드레일 작동 방식
</div>

Weave 가드레일은 인라인 [Weave Scorers](/ko/weave/guides/evaluation/scorers)를 사용해 사용자 입력 또는 LLM 출력을 평가하고, LLM의 응답을 실시간으로 조정합니다. 사용자 정의 scorer를 구성하거나 [built-in scorers](/ko/weave/guides/evaluation/builtin_scorers)를 사용해 다양한 목적에 맞게 콘텐츠를 평가할 수 있습니다. 이 가이드는 두 종류의 scorer를 모두 가드레일로 사용하는 방법을 보여줍니다.

애플리케이션의 제어 흐름을 변경하지 않고 프로덕션 트래픽에 대해 점수만 매기고 싶다면, 대신 [monitors](/ko/weave/guides/evaluation/monitors)를 사용하십시오.

monitors와 달리, 가드레일은 애플리케이션의 제어 흐름에 영향을 주기 때문에 코드 변경이 필요합니다. 하지만 가드레일에서 생성되는 모든 scorer 결과는 자동으로 Weave 데이터베이스에 저장되므로, 별도의 추가 구성 없이도 가드레일은 monitors처럼 동작합니다. scorer가 처음 어떻게 사용되었는지와 관계없이 과거 scorer 결과를 분석할 수 있습니다.

<Note>
  Weave TypeScript SDK는 가드레일을 설정하는 데 필요한 도구를 지원하지 않습니다.
</Note>

<div id="optimize-your-weave-guardrail-performance">
  ### Weave guardrail 성능 최적화하기
</div>

Guardrail은 애플리케이션의 제어 흐름을 중단시키거나 응답의 진행을 변경할 수 있기 때문에, 과도하게 복잡하면 성능에 부정적인 영향을 줄 수 있습니다. 최적의 성능을 위해 다음을 권장합니다:

* Guardrail 로직을 단순하고 빠르게 유지하기
* 공통 결과 캐싱하기
* 무거운 외부 API 호출 피하기
* Guardrail을 메인 함수 바깥에서 초기화해 반복 초기화 비용 피하기

특히 다음과 같은 경우에는 guardrail을 메인 함수 바깥에서 초기화하는 것이 중요합니다:

* Scorer가 ML 모델을 로드하는 경우
* 지연 시간이 중요한 로컬 LLM을 사용하는 경우
* Scorer가 네트워크 연결을 유지하는 경우
* 트래픽이 많은 애플리케이션인 경우

<div id="example-create-a-guardrail-using-a-built-in-moderation-scorer">
  ### 예제: 내장 moderation scorer를 사용해 guardrail 생성하기
</div>

다음 예제에서는 사용자의 프롬프트를 OpenAI의 GPT-4o mini 모델에 전송합니다. 그런 다음 모델의 응답을 Weave의 [OpenAI moderation API](https://platform.openai.com/docs/guides/moderation)에 전달하여 LLM의 응답에 유해하거나 독성 콘텐츠가 포함되어 있는지 평가합니다. 모델의 응답은 guardrail 함수 역할을 하는 `generate_safe_response()`로 전달되며, 이 함수는 `OpenAIModerationScorer`를 사용해 LLM의 원본 응답을 검사합니다. 이후 함수 로직은 OpenAI의 평가 응답에서 `passed` 필드의 불리언 값을 확인하고, 이 값에 따라 애플리케이션의 응답 방식을 결정합니다.

```python lines {28-45}
import weave
import openai
from weave.scorers import OpenAIModerationScorer
import asyncio

# Weave 초기화
weave.init("your-team-name/your-project-name")

# OpenAI 클라이언트 초기화
client = openai.OpenAI()  # OPENAI_API_KEY 환경 변수 사용

# 모더레이션 스코어러 초기화
moderation_scorer = OpenAIModerationScorer()

# OpenAI에 프롬프트 전송
@weave.op
def generate_response(prompt: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=200
    )
    return response.choices[0].message.content

# 가드레일 함수가 응답의 유해성을 검사
async def generate_safe_response(prompt: str) -> str:
    """콘텐츠 모더레이션 가드레일을 적용하여 응답을 생성합니다."""
    # 결과와 Call 객체를 모두 가져옴
    result, call = generate_response.call(prompt)
    
    # 사용자에게 반환하기 전에 모더레이션 스코어러 적용
    score = await call.apply_scorer(moderation_scorer)
    print("This is the score object:", score)
    
    # 콘텐츠가 플래그 처리되었는지 확인
    if not score.result.get("passed", True): 
        categories = score.result.get("categories", {})
        flagged_categories = list(categories.keys()) if categories else []
        print(f"Content blocked. Flagged categories: {flagged_categories}")
        return "I'm sorry, I can't provide that response due to content policy restrictions."
    
    return result

# 예제 실행
if __name__ == "__main__":
    
    prompts = [
        "What's the capital of France?",
        "Tell me a funny fact about dogs.",
    ]
    
    for prompt in prompts:
        print(f"\nPrompt: {prompt}")
        response = asyncio.run(generate_safe_response(prompt))
        print(f"Response: {response}")
```

LLM-as-a-judge scorer를 사용할 때, scoring prompt에서 ops의 변수를 참조할 수 있습니다. 예를 들어, “`{ground_truth}`를 기준으로 `{output}`이(가) 정확한지 평가하시오.”와 같이 사용할 수 있습니다. 자세한 내용은 [프롬프트 변수](/ko/weave/guides/evaluation/scorers#access-variables-from-your-ops-in-scoring-prompts)를 참고하세요.

<div id="example-create-a-guardrail-using-a-custom-scorer">
  ### 예제: 사용자 정의 scorer를 사용해 guardrail 생성하기
</div>

다음 예제에서는 LLM 응답에서 이메일 주소, 전화번호, 주민등록번호와 같은 개인식별정보(PII)를 감지하는 사용자 정의 guardrail을 생성합니다. 이를 통해 생성된 콘텐츠에 민감한 정보가 노출되는 것을 방지할 수 있습니다. 함수 generate&#95;safe&#95;response는 사용자 정의 PIIDetectionScorer를 적용합니다.

```python lines {14-39, 57-69}
import weave
import openai
import re
import asyncio
from weave import Scorer

weave.init("your-team-name/your-project-name")

client = openai.OpenAI()

class PIIDetectionScorer(Scorer):
    """LLM 출력에서 데이터 유출을 방지하기 위해 PII를 감지합니다."""
    
    @weave.op
    def score(self, output: str) -> dict:
        """
        출력에서 일반적인 PII 패턴을 확인합니다.
        
        Returns:
            dict: 'passed' (bool) 및 'detected_types' (list) 포함
        """
        detected_types = []
        
        # 이메일 패턴
        if re.search(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', output):
            detected_types.append("email")
        
        # 전화번호 패턴 (미국 형식)
        if re.search(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', output):
            detected_types.append("phone")
        
        # SSN 패턴
        if re.search(r'\b\d{3}-\d{2}-\d{4}\b', output):
            detected_types.append("ssn")
        
        return {
            "passed": len(detected_types) == 0,
            "detected_types": detected_types
        }

# 최적의 성능을 위해 함수 외부에서 scorer 초기화
pii_scorer = PIIDetectionScorer()

@weave.op
def generate_response(prompt: str) -> str:
    """LLM을 사용하여 응답을 생성합니다."""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=200
    )
    return response.choices[0].message.content

async def generate_safe_response(prompt: str) -> str:
    """PII 감지 가드레일을 적용하여 응답을 생성합니다."""
    result, call = generate_response.call(prompt)
    
    # PII 감지 scorer 적용
    score = await call.apply_scorer(pii_scorer)
    
    # PII가 감지된 경우 응답 차단
    if not score.result.get("passed", True):
        detected_types = score.result.get("detected_types", [])
        return f"I cannot provide a response that may contain sensitive information (detected: {', '.join(detected_types)})."
    
    return result

# 사용 예시
if __name__ == "__main__":
    prompts = [
        "What's the weather like today?",
        "Can you help me contact someone at john.doe@example.com?",
        "Tell me about machine learning.",
    ]
    
    for prompt in prompts:
        print(f"\nPrompt: {prompt}")
        response = asyncio.run(generate_safe_response(prompt))
        print(f"Response: {response}")
```

<div id="integrate-weave-with-aws-bedrock-guardrails">
  ### AWS Bedrock Guardrails와 Weave 통합
</div>

`BedrockGuardrailScorer`는 설정된 정책을 기반으로 콘텐츠를 감지하고 필터링하기 위해 AWS Bedrock Guardrails를 사용합니다.

Bedrock Guardrails 통합을 설정하기 전에 다음이 필요합니다:

* Bedrock에 대한 액세스 권한이 있는 AWS 계정
* [AWS Bedrock 콘솔에서 구성된 guardrail](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-components.html)
* `boto3` [Python 패키지](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html)

직접 Bedrock 클라이언트를 생성할 필요는 없습니다. Weave가 대신 생성합니다. 리전을 지정하려면 scorer의 `bedrock_runtime_kwargs` 파라미터에 리전 값을 전달합니다.

AWS Bedrock에서 guardrail을 생성하는 방법에 대한 예시는 [Bedrock guardrails 노트북](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/responsible_ai/bedrock-guardrails/guardrails-api.ipynb)을 참고하세요.

다음 예제는 사용자에게 결과를 반환하기 전에 텍스트 생성 결과가 AWS Bedrock Guardrails 정책을 준수하는지 검사합니다:

```python
import weave
from weave.scorers.bedrock_guardrails import BedrockGuardrailScorer

weave.init("your-team-name/your-project-name")

guardrail_scorer = BedrockGuardrailScorer(
    guardrail_id="your-guardrail-id",
    guardrail_version="DRAFT",
    source="INPUT",
    bedrock_runtime_kwargs={"region_name": "us-east-1"}
)

@weave.op
def generate_text(prompt: str) -> str:
    # 텍스트 생성 로직을 여기에 작성하세요
    return "Generated text..."

async def generate_safe_text(prompt: str) -> str:
    result, call = generate_text.call(prompt)

    score = await call.apply_scorer(guardrail_scorer)

    if not score.result.passed:
        if score.result.metadata.get("modified_output"):
            return score.result.metadata["modified_output"]
        return "콘텐츠 정책 제한으로 인해 해당 콘텐츠를 생성할 수 없습니다."

    return result
```
