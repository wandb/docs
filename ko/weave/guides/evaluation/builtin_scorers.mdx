---
title: "내장 스코어러 사용하기"
description: "AI 애플리케이션 평가를 위해 Weave의 미리 정의된 스코어러 사용하기"
---

Weave는 [할루시네이션 탐지](#hallucinationfreescorer)와 [요약 품질](#summarizationscorer)처럼 AI 애플리케이션을 평가하기 위한 여러 미리 정의된 스코어러를 제공합니다. 이를 사용하면 Evaluation을 빠르게 정의하고 애플리케이션 출력에 점수를 매길 수 있습니다.

<Note>
  로컬 스코어러는 Weave Python SDK에서만 사용할 수 있습니다. 아직 Weave TypeScript SDK에서는 지원되지 않습니다.

  TypeScript에서 Weave 스코어러를 사용하려면 [함수 기반 스코어러](/ko/weave/guides/evaluation/scorers#function-based-scorers)를 참조하세요.
</Note>

<div id="installation">
  ## Installation
</div>

Weave의 미리 정의된 스코어러를 사용하려면 몇 가지 추가 종속성을 설치해야 합니다.

```bash
pip install weave[scorers]
```

**LLM-evaluators**
2025년 2월 업데이트: 이제 LLM을 활용하는 사전 정의된 스코어러가 자동으로 litellm과 연동됩니다.
더 이상 LLM 클라이언트를 직접 전달할 필요 없이 `model_id`만 설정하면 됩니다.
지원되는 모델 목록은 [여기](https://docs.litellm.ai/docs/providers)에서 확인하세요.

<div id="hallucinationfreescorer">
  ## `HallucinationFreeScorer`
</div>

이 scorer는 입력 데이터를 기준으로 AI 시스템의 출력에 환각(hallucination)이 포함되어 있는지 검사합니다.

```python lines
from weave.scorers import HallucinationFreeScorer

scorer = HallucinationFreeScorer()
```

**사용자 지정:**

* 채점기의 `system_prompt`와 `user_prompt` 필드를 원하는 방식으로 설정해, 여기서 말하는 &quot;hallucination&quot;의 의미를 정의하세요.

**참고 사항:**

* `score` 메서드는 `context`라는 이름의 입력 컬럼을 기대합니다. 데이터셋에서 다른 이름을 사용한다면, [`column_map` 속성을 사용](#use-column-mapping)하여 `context`를 해당 데이터셋 컬럼에 매핑하세요.

아래는 Evaluation 문맥에서의 예시입니다:

```python lines
import asyncio
import weave
from weave.scorers import HallucinationFreeScorer

# 필요한 경우 컬럼 매핑으로 scorer를 초기화합니다.
hallucination_scorer = HallucinationFreeScorer(
    model_id="openai/gpt-4o", # 또는 litellm이 지원하는 다른 모델
    column_map={"context": "input", "output": "other_col"}
)

# 데이터셋 생성
dataset = [
    {"input": "John likes various types of cheese."},
    {"input": "Pepe likes various types of cheese."},
]

@weave.op
def model(input: str) -> str:
    return "The person's favorite cheese is cheddar."

# Evaluation 실행
evaluation = weave.Evaluation(
    dataset=dataset,
    scorers=[hallucination_scorer],
)
result = asyncio.run(evaluation.evaluate(model))
print(result)
# 출력 예시:
# {'HallucinationFreeScorer': {'has_hallucination': {'true_count': 2, 'true_fraction': 1.0}}, 'model_latency': {'mean': ...}}
```

***

<div id="summarizationscorer">
  ## `SummarizationScorer`
</div>

LLM을 사용해 요약을 원문과 비교하고 요약의 품질을 평가합니다.

```python lines
from weave.scorers import SummarizationScorer

scorer = SummarizationScorer(
    model_id="openai/gpt-4o"  # 또는 litellm이 지원하는 다른 모델
)
```

**동작 방식:**

이 scorer는 요약문을 두 가지 방식으로 평가합니다:

1. **엔터티 밀도(Entity Density):** 요약문에 언급된 고유 엔터티(이름, 장소, 사물 등)의 개수와 요약문 전체 단어 수의 비율을 확인해 요약문의 &quot;정보 밀도&quot;를 추정합니다. LLM을 사용해 엔터티를 추출합니다. Chain of Density 논문(https://arxiv.org/abs/2309.04269)에서의 엔터티 밀도 활용 방식과 유사합니다.
2. **품질 등급(Quality Grading):** LLM evaluator가 요약문을 `poor`, `ok`, `excellent` 중 하나로 등급을 매깁니다. 그런 다음 이 등급을 점수로 매핑합니다(`poor`는 0.0, `ok`는 0.5, `excellent`는 1.0)하여 전체 성능 평가에 사용합니다.

**커스터마이즈:**

* `summarization_evaluation_system_prompt`와 `summarization_evaluation_prompt`를 조정해서 평가 프로세스를 원하는 대로 변경할 수 있습니다.

**참고 사항:**

* 이 scorer는 내부적으로 litellm을 사용합니다.
* `score` 메서드는 원본 텍스트(요약 대상 텍스트)가 `input` 열에 존재할 것으로 예상합니다. 데이터셋에서 다른 열 이름을 사용하는 경우 [`column_map` 사용](#use-column-mapping)을 활용하세요.

아래는 Evaluation 맥락에서의 사용 예시입니다:

```python lines
import asyncio
import weave
from weave.scorers import SummarizationScorer

class SummarizationModel(weave.Model):
    @weave.op()
    async def predict(self, input: str) -> str:
        return "This is a summary of the input text."

# 스코어러 초기화
summarization_scorer = SummarizationScorer(
    model_id="openai/gpt-4o"  # 또는 litellm이 지원하는 다른 모델
)
# 데이터셋 생성
dataset = [
    {"input": "The quick brown fox jumps over the lazy dog."},
    {"input": "Artificial Intelligence is revolutionizing various industries."}
]
# 평가 실행
evaluation = weave.Evaluation(dataset=dataset, scorers=[summarization_scorer])
results = asyncio.run(evaluation.evaluate(SummarizationModel()))
print(results)
# 출력 예시:
# {'SummarizationScorer': {'is_entity_dense': {'true_count': 0, 'true_fraction': 0.0}, 'summarization_eval_score': {'mean': 0.0}, 'entity_density': {'mean': 0.0}}, 'model_latency': {'mean': ...}}
```

***

<div id="openaimoderationscorer">
  ## `OpenAIModerationScorer`
</div>

`OpenAIModerationScorer`는 OpenAI의 Moderation API를 사용하여 AI 시스템의 출력 결과에 혐오 발언이나 노골적인 내용처럼 허용되지 않는 콘텐츠가 포함되어 있는지 확인합니다.

```python lines
from weave.scorers import OpenAIModerationScorer

scorer = OpenAIModerationScorer()
```

**동작 방식:**

* AI의 출력을 OpenAI Moderation 엔드포인트로 전송하고, 콘텐츠가 플래그로 표시되었는지를 나타내는 구조화된 응답을 반환합니다.

**참고:**
다음은 Evaluation 맥락에서의 예입니다:

```python lines
import asyncio
import weave
from weave.scorers import OpenAIModerationScorer

class MyModel(weave.Model):
    @weave.op
    async def predict(self, input: str) -> str:
        return input

# scorer 초기화
moderation_scorer = OpenAIModerationScorer()

# 데이터셋 생성
dataset = [
    {"input": "I love puppies and kittens!"},
    {"input": "I hate everyone and want to hurt them."}
]

# 평가 실행
evaluation = weave.Evaluation(dataset=dataset, scorers=[moderation_scorer])
results = asyncio.run(evaluation.evaluate(MyModel()))
print(results)
# 출력 예시:
# {'OpenAIModerationScorer': {'flagged': {'true_count': 1, 'true_fraction': 0.5}, 'categories': {'violence': {'true_count': 1, 'true_fraction': 1.0}}}, 'model_latency': {'mean': ...}}
```

***

<div id="embeddingsimilarityscorer">
  ## `EmbeddingSimilarityScorer`
</div>

`EmbeddingSimilarityScorer`는 AI 시스템 출력의 임베딩과 데이터셋에 있는 대상 텍스트의 임베딩 사이의 코사인 유사도를 계산합니다. 이는 AI 출력이 참조(reference) 텍스트와 얼마나 유사한지를 측정하는 데 유용합니다.

```python lines
from weave.scorers import EmbeddingSimilarityScorer

similarity_scorer = EmbeddingSimilarityScorer(
    model_id="openai/text-embedding-3-small",  # litellm이 지원하는 다른 모델도 사용 가능
    threshold=0.4  # 코사인 유사도 임계값
)
```

**매개변수:**

* `threshold` (float): 두 텍스트를 유사하다고 간주하기 위해 필요한 최소 코사인 유사도 점수(범위: -1~1, 기본값: `0.5`).

**사용 예시:**

다음 예시는 Evaluation 컨텍스트에서 `EmbeddingSimilarityScorer`를 사용하는 방법을 보여줍니다:

```python lines
import asyncio
import weave
from weave.scorers import EmbeddingSimilarityScorer

# 스코어러 초기화
similarity_scorer = EmbeddingSimilarityScorer(
    model_id="openai/text-embedding-3-small",  # 또는 litellm이 지원하는 다른 모델
    threshold=0.7
)
# 데이터셋 생성
dataset = [
    {
        "input": "He's name is John",
        "target": "John likes various types of cheese.",
    },
    {
        "input": "He's name is Pepe.",
        "target": "Pepe likes various types of cheese.",
    },
]
# 모델 정의
@weave.op
def model(input: str) -> str:
    return "John likes various types of cheese."

# 평가 실행
evaluation = weave.Evaluation(
    dataset=dataset,
    scorers=[similarity_scorer],
)
result = asyncio.run(evaluation.evaluate(model))
print(result)
# 출력 예시:
# {'EmbeddingSimilarityScorer': {'is_similar': {'true_count': 1, 'true_fraction': 0.5}, 'similarity_score': {'mean': 0.844851403}}, 'model_latency': {'mean': ...}}
```

***

<div id="validjsonscorer">
  ## `ValidJSONScorer`
</div>

`ValidJSONScorer`는 AI 시스템의 출력이 유효한 JSON인지 확인합니다. 이 스코어러는 출력이 JSON 형식일 것으로 예상되고 그 유효성을 검증해야 할 때 유용합니다.

```python lines
from weave.scorers import ValidJSONScorer

json_scorer = ValidJSONScorer()
```

다음은 Evaluation에서의 예시입니다:

```python lines
import asyncio
import weave
from weave.scorers import ValidJSONScorer

class JSONModel(weave.Model):
    @weave.op()
    async def predict(self, input: str) -> str:
        # 이것은 플레이스홀더입니다.
        # 실제 시나리오에서는 JSON을 생성합니다.
        return '{"key": "value"}'

model = JSONModel()
json_scorer = ValidJSONScorer()

dataset = [
    {"input": "Generate a JSON object with a key and value"},
    {"input": "Create an invalid JSON"}
]

evaluation = weave.Evaluation(dataset=dataset, scorers=[json_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# 출력 예시:
# {'ValidJSONScorer': {'json_valid': {'true_count': 2, 'true_fraction': 1.0}}, 'model_latency': {'mean': ...}}
```

***

<div id="validxmlscorer">
  ## `ValidXMLScorer`
</div>

`ValidXMLScorer`는 AI 시스템의 출력이 유효한 XML인지 확인합니다. XML 형식의 출력이 요구되는 경우에 유용합니다.

```python lines
from weave.scorers import ValidXMLScorer

xml_scorer = ValidXMLScorer()
```

다음은 Evaluation 상황에서의 예시입니다:

```python lines
import asyncio
import weave
from weave.scorers import ValidXMLScorer

class XMLModel(weave.Model):
    @weave.op()
    async def predict(self, input: str) -> str:
        # 이것은 플레이스홀더입니다. 실제 시나리오에서는 XML을 생성합니다.
        return '<root><element>value</element></root>'

model = XMLModel()
xml_scorer = ValidXMLScorer()

dataset = [
    {"input": "Generate a valid XML with a root element"},
    {"input": "Create an invalid XML"}
]

evaluation = weave.Evaluation(dataset=dataset, scorers=[xml_scorer])
results = asyncio.run(evaluation.evaluate(model))
print(results)
# 출력 예시:
# {'ValidXMLScorer': {'xml_valid': {'true_count': 2, 'true_fraction': 1.0}}, 'model_latency': {'mean': ...}}
```

***

<div id="pydanticscorer">
  ## `PydanticScorer`
</div>

`PydanticScorer`는 AI 시스템의 출력을 Pydantic 모델에 대해 검증하여, 지정된 스키마 또는 데이터 구조를 따르는지 확인합니다.

```python lines
from weave.scorers import PydanticScorer
from pydantic import BaseModel

class FinancialReport(BaseModel):
    revenue: int
    year: str

pydantic_scorer = PydanticScorer(model=FinancialReport)
```

***

<div id="ragas-contextentityrecallscorer">
  ## RAGAS - `ContextEntityRecallScorer`
</div>

`ContextEntityRecallScorer`는 AI 시스템의 출력과 제공된 컨텍스트 양쪽에서 엔터티를 추출한 뒤, 재현율(recall) 점수를 계산하여 컨텍스트 재현율을 추정합니다. 이 스코어러는 [RAGAS](https://github.com/explodinggradients/ragas) 평가 라이브러리를 기반으로 합니다.

```python lines
from weave.scorers import ContextEntityRecallScorer

entity_recall_scorer = ContextEntityRecallScorer(
    model_id="openai/gpt-4o"
)
```

**동작 방식:**

* LLM을 사용해 출력과 컨텍스트에서 고유 엔티티를 추출하고 `recall`을 계산합니다.
* **Recall**은 컨텍스트에 있는 중요한 엔티티 중 출력에서 포착된 비율을 나타냅니다.
* `recall` 점수가 담긴 딕셔너리를 반환합니다.

**참고 사항:**

* 데이터셋에 `context` 열이 있어야 합니다. 열 이름이 다르다면 [`column_map` 속성을 사용](#use-column-mapping)하세요.

***

<div id="ragas-contextrelevancyscorer">
  ## RAGAS - `ContextRelevancyScorer`
</div>

`ContextRelevancyScorer`는 제공된 컨텍스트가 AI 시스템의 출력 결과와 얼마나 관련 있는지를 평가합니다. 이는 [RAGAS](https://github.com/explodinggradients/ragas) 평가 라이브러리를 기반으로 합니다.

```python lines
from weave.scorers import ContextRelevancyScorer

relevancy_scorer = ContextRelevancyScorer(
    model_id="openai/gpt-4o",  # 또는 litellm이 지원하는 다른 모델
    relevancy_prompt="""
Given the following question and context, rate the relevancy of the context to the question on a scale from 0 to 1.

Question: {question}
Context: {context}
Relevancy Score (0-1):
"""
)
```

**동작 방식:**

* LLM을 사용하여 출력과 컨텍스트 간의 관련성을 0에서 1 사이의 척도로 평가합니다.
* `relevancy_score`가 포함된 딕셔너리를 반환합니다.

**참고:**

* 데이터셋에 `context` 열이 있어야 합니다. 열 이름이 다르면 [`column_map` 속성을 사용](#use-column-mapping)하세요.
* 관련성을 어떻게 평가할지 정의하려면 `relevancy_prompt`를 사용자 정의하세요.

다음은 Evaluation에서의 사용 예시입니다:

```python lines
import asyncio
from textwrap import dedent
import weave
from weave.scorers import ContextEntityRecallScorer, ContextRelevancyScorer

class RAGModel(weave.Model):
    @weave.op()
    async def predict(self, question: str) -> str:
        "Retrieve relevant context"
        return "Paris is the capital of France."

# 프롬프트 정의
relevancy_prompt: str = dedent("""
    Given the following question and context, rate the relevancy of the context to the question on a scale from 0 to 1.

    Question: {question}
    Context: {context}
    Relevancy Score (0-1):
    """)
# 스코어러 초기화
entity_recall_scorer = ContextEntityRecallScorer()
relevancy_scorer = ContextRelevancyScorer(relevancy_prompt=relevancy_prompt)
# 데이터셋 생성
dataset = [
    {
        "question": "What is the capital of France?",
        "context": "Paris is the capital city of France."
    },
    {
        "question": "Who wrote Romeo and Juliet?",
        "context": "William Shakespeare wrote many famous plays."
    }
]
# 평가 실행
evaluation = weave.Evaluation(
    dataset=dataset,
    scorers=[entity_recall_scorer, relevancy_scorer]
)
results = asyncio.run(evaluation.evaluate(RAGModel()))
print(results)
# 출력 예시:
# {'ContextEntityRecallScorer': {'recall': {'mean': ...}}, 
# 'ContextRelevancyScorer': {'relevancy_score': {'mean': ...}}, 
# 'model_latency': {'mean': ...}}
```

**참고:** 기본 제공 스코어러는 `openai/gpt-4o`, `openai/text-embedding-3-small`와 같은 OpenAI 모델을 사용해 보정되었습니다. 다른 제공업체의 모델을 시험해 보고 싶다면 `model_id` 필드를 업데이트하여 다른 모델을 지정하면 됩니다. 예를 들어 Anthropic 모델을 사용하려면 다음과 같이 합니다:

```python lines
from weave.scorers import SummarizationScorer

# Anthropic의 Claude 모델로 전환
summarization_scorer = SummarizationScorer(
    model_id="anthropic/claude-3-5-sonnet-20240620"
)
```
