---
title: 로컬 모델
---

<div id="local-models">
  # 로컬 모델
</div>

많은 개발자들이 LLama-3, Mixtral, Gemma, Phi 등과 같은 오픈 소스 모델을 다운로드하여 로컬에서 실행합니다. 이러한 모델을 로컬에서 실행하는 방법은 여러 가지가 있으며, OpenAI SDK와 호환되기만 하면 Weave는 그중 일부를 기본적으로 지원합니다.

<div id="wrap-local-model-functions-with-weaveop">
  ## 로컬 모델 함수를 `@weave.op()`으로 감싸기
</div>

`weave.init('<your-project-name>')`로 Weave를 초기화한 다음 LLM 호출을 `weave.op()`으로 감싸기만 하면, 어떤 LLM이든 직접 Weave와 손쉽게 통합할 수 있습니다. 자세한 내용은 [tracing](/ko/weave/guides/tracking/tracing) 가이드를 참고하세요.

<div id="updating-your-openai-sdk-code-to-use-local-models">
  ## OpenAI SDK 코드를 로컬 모델을 사용하도록 업데이트하기
</div>

OpenAI SDK 호환을 지원하는 모든 프레임워크와 서비스에서는 몇 가지 작은 변경만 해주면 됩니다.

가장 먼저, 그리고 가장 중요한 것은 `openai.OpenAI()`를 초기화할 때 `base_url`을 변경하는 것입니다.

```python lines
client = openai.OpenAI(
    base_url="http://localhost:1234",
)
```

로컬 모델의 경우 `api_key`에는 아무 문자열이나 넣어도 되지만, 반드시 직접 지정해야 합니다. 그렇지 않으면 OpenAI가 환경 변수에서 키를 가져오려고 시도해 오류가 발생합니다.

<div id="openai-sdk-supported-local-model-runners">
  ## OpenAI SDK를 지원하는 로컬 모델 러너
</div>

다음은 Hugging Face에서 모델을 다운로드해 로컬 컴퓨터에서 실행할 수 있으며, OpenAI SDK 호환성을 지원하는 앱 목록입니다.

1. Nomic [GPT4All](https://www.nomic.ai/gpt4all) - 설정에서 Local Server를 통해 지원 ([FAQ](https://docs.gpt4all.io/gpt4all_help/faq.html))
2. [LMStudio](https://lmstudio.ai/) - Local Server OpenAI SDK 지원 [문서](https://lmstudio.ai/docs/local-server)
3. [Ollama](https://ollama.com/) - OpenAI SDK에 대한 [실험적 지원](https://github.com/ollama/ollama/blob/main/docs/openai.mdx)
4. [llama-cpp-python](https://llama-cpp-python.readthedocs.io/en/latest/server/) Python 패키지를 통한 llama.cpp
5. [llamafile](https://github.com/Mozilla-Ocho/llamafile#other-example-llamafiles) - `http://localhost:8080/v1`는 Llamafile 실행 시 OpenAI SDK를 자동으로 지원합니다