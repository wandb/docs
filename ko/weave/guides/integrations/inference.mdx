---
title: Inference
---

<div id="wb-inference">
  # W&amp;B Inference
</div>

&#95;W&amp;B Inference&#95;는 W&amp;B Weave 및 OpenAI 호환 API를 통해 주요 오픈 소스 파운데이션 모델에 액세스할 수 있도록 제공합니다. W&amp;B Inference를 사용하면 다음을 수행할 수 있습니다:

* 호스팅 서비스 제공업체에 가입하거나 모델을 직접 호스팅하지 않고도 AI 애플리케이션과 에이전트를 개발할 수 있습니다.
* 지원되는 모델을 W&amp;B Weave Playground에서 시험해 볼 수 있습니다.

<Warning>
  W&amp;B Inference 크레딧은 한시적으로 Free, Pro, Academic 플랜에 포함됩니다. Enterprise의 경우 제공 여부가 다를 수 있습니다. 크레딧을 모두 사용하면 다음이 적용됩니다:

  * Free 계정은 Inference를 계속 사용하려면 Pro 플랜으로 업그레이드해야 합니다.
  * Pro 플랜 사용자는 모델별 요금에 기반해 초과 사용량에 대해 월 단위로 청구됩니다.

  자세한 내용은 [요금 페이지](https://wandb.ai/site/pricing/)와 [W&amp;B Inference 모델 비용](https://wandb.ai/site/pricing/inference)을 참고하세요.
</Warning>

Weave를 사용하면 W&amp;B Inference로 구동되는 애플리케이션을 추적(trace), 평가, 모니터링하고 반복적으로 개선할 수 있습니다.

| Model            | Model ID (for API usage)                                     | Type(s)       | Context Window | Parameters                  | Description                                                                 |
|------------------|-----------------------------------------------|---------------|----------------|-----------------------------|-----------------------------------------------------------------------------|
| DeepSeek R1-0528 | deepseek-ai/DeepSeek-R1-0528                  | Text          | 161K           | 37B - 680B (Active - Total) | 복잡한 코딩, 수학, 구조화된 문서 분석 등 정밀한 추론 작업에 최적화된 모델입니다. |
| DeepSeek V3-0324 | deepseek-ai/DeepSeek-V3-0324                  | Text          | 161K           | 37B - 680B (Active - Total) | 고난도 언어 처리와 포괄적인 문서 분석을 위해 설계된 강력한 Mixture-of-Experts 모델입니다. |
| Llama 3.1 8B     | meta-llama/Llama-3.1-8B-Instruct              | Text          | 128K           | 8B (Total)                  | 반응성이 뛰어난 다국어 챗봇 상호작용에 최적화된 효율적인 대화형 모델입니다. |
| Llama 3.3 70B    | meta-llama/Llama-3.3-70B-Instruct             | Text          | 128K           | 70B (Total)                 | 대화형 작업, 상세한 지시사항 준수, 코딩에서 뛰어난 성능을 보이는 다국어 모델입니다. |
| Llama 4 Scout    | meta-llama/Llama-4-Scout-17B-16E-Instruct     | Text, Vision  | 64K            | 17B - 109B (Active - Total) | 텍스트와 이미지 이해를 통합한 멀티모달 모델로, 시각적 작업과 결합 분석에 적합합니다. |
| Phi 4 Mini       | microsoft/Phi-4-mini-instruct                | Text          | 128K           | 3.8B (Active - Total)       | 리소스가 제한된 환경에서 빠른 응답에 적합한 작고 효율적인 모델입니다. |

이 가이드는 다음과 같은 정보를 제공합니다:

* [사전 준비 사항](#prerequisites)
  * [Python을 통한 API 사용을 위한 추가 사전 준비 사항](#additional-prerequisites-for-using-the-api-via-python)
* [API 명세](#api-specification)
  * [엔드포인트](#endpoint)
  * [사용 가능한 메서드](#available-methods)
    * [Chat completions](#chat-completions)
    * [지원되는 모델 목록 가져오기](#list-supported-models)
* [사용 예시](#usage-examples)
* [UI](#ui)
  * [Inference 서비스에 액세스](#access-the-inference-service)
  * [Playground에서 모델 테스트](#try-a-model-in-the-playground)
  * [여러 모델 비교](#compare-multiple-models)
  * [결제 및 사용량 정보 보기](#view-billing-and-usage-information)
* [사용량 정보 및 한도](#usage-information-and-limits)
* [API 오류](#api-errors)

<div id="prerequisites">
  ## 사전 준비 사항
</div>

API 또는 W&amp;B Weave UI를 통해 W&amp;B Inference 서비스를 사용하려면 다음이 필요합니다.

1. W&amp;B 계정. [여기](https://app.wandb.ai/login?signup=true&_gl=1*1yze8dp*_ga*ODIxMjU5MTk3LjE3NDk0OTE2NDM.*_ga_GMYDGNGKDT*czE3NDk4NDYxMzgkbzEyJGcwJHQxNzQ5ODQ2MTM4JGo2MCRsMCRoMA..*_ga_JH1SJHJQXJ*czE3NDk4NDU2NTMkbzI1JGcxJHQxNzQ5ODQ2MTQ2JGo0NyRsMCRoMA..*_gcl_au*MTE4ODk1MzY1OC4xNzQ5NDkxNjQzLjk1ODA2MjQwNC4xNzQ5NTgyMTUzLjE3NDk1ODIxNTM.)에서 가입하세요.
2. W&amp;B API 키. [User Settings](https://wandb.ai/settings)에서 API 키를 생성합니다.
3. W&amp;B 프로젝트.
4. Python을 통해 Inference 서비스를 사용하는 경우 [Python을 통해 API를 사용하기 위한 추가 사전 준비 사항](#additional-prerequisites-for-using-the-api-via-python)을 참고하세요.

<div id="additional-prerequisites-for-using-the-api-via-python">
  ### Python을 통해 API를 사용하기 위한 추가 전제 조건
</div>

Python을 통해 Inference API를 사용하려면 먼저 일반 전제 조건을 완료하세요. 그런 다음 로컬 환경에 `openai` 및 `weave` 라이브러리를 설치하세요:

```bash
pip install openai weave
```

<Note>
  `weave` 라이브러리는 Weave로 LLM 애플리케이션을 추적하려는 경우에만 필요합니다. Weave 사용을 시작하려면 [Weave 빠른 시작](/ko/weave/quickstart)을 참조하세요.

  Weave와 함께 W&amp;B Inference 서비스를 사용하는 방법에 대한 예제는 [API 사용 예제](#usage-examples)를 참조하세요.
</Note>

<div id="api-specification">
  ## API 사양
</div>

다음 섹션에서는 API 사양과 사용 예제를 제공합니다.

* [엔드포인트](#endpoint)
* [사용 가능한 메서드](#available-methods)
* [사용 예제](#usage-examples)

<div id="endpoint">
  ### 엔드포인트
</div>

Inference 서비스는 다음 엔드포인트를 통해 호출할 수 있습니다:

```plaintext
https://api.inference.wandb.ai/v1
```

<Warning>
  이 엔드포인트에 액세스하려면 Inference 서비스 크레딧이 할당된 W&amp;B 계정, 유효한 W&amp;B API 키, 그리고 W&amp;B 엔터티(“team”이라고도 함)와 프로젝트가 있어야 합니다. 이 가이드의 코드 예시에서는 엔터티(team)와 프로젝트를 `<your-team>\<your-project>` 형식으로 사용합니다.
</Warning>

<div id="available-methods">
  ### 사용 가능한 메서드
</div>

Inference 서비스는 다음과 같은 API 메서드를 지원합니다.

* [채팅 완료 (Chat completions)](#chat-completions)
* [지원되는 모델 목록](#list-supported-models)

<div id="chat-completions">
  #### 채팅 컴플리션
</div>

사용할 수 있는 주요 API 메서드는 `/chat/completions`이며, 지원되는 모델에 메시지를 보내고 컴플리션을 받기 위한 OpenAI 호환 요청 형식을 지원합니다. Weave와 함께 W&amp;B Inference 서비스를 사용하는 방법에 대한 예시는 [API 사용 예제](#usage-examples)를 참고하세요.

채팅 컴플리션을 생성하려면 다음이 필요합니다:

* Inference 서비스의 기본 URL `https://api.inference.wandb.ai/v1`
* W&amp;B API 키 `<your-api-key>`
* W&amp;B 엔터티와 프로젝트 이름 `<your-team>/<your-project>`
* 사용하려는 모델의 ID (다음 중 하나):
  * `meta-llama/Llama-3.1-8B-Instruct`
  * `deepseek-ai/DeepSeek-V3-0324`
  * `meta-llama/Llama-3.3-70B-Instruct`
  * `deepseek-ai/DeepSeek-R1-0528`
  * `meta-llama/Llama-4-Scout-17B-16E-Instruct`
  * `microsoft/Phi-4-mini-instruct`

<Tabs>
  <Tab title="Bash">
    ```bash
    curl https://api.inference.wandb.ai/v1/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer <your-api-key>" \
      -H "OpenAI-Project: <your-team>/<your-project>" \
      -d '{
        "model": "<model-id>",
        "messages": [
          { "role": "system", "content": "You are a helpful assistant." },
          { "role": "user", "content": "Tell me a joke." }
        ]
      }'
    ```
  </Tab>

  <Tab title="Python">
    ```python lines
    import openai

    client = openai.OpenAI(
        # 커스텀 base URL은 W&B Inference를 가리킵니다
        base_url='https://api.inference.wandb.ai/v1',

        # https://wandb.ai/settings 에서 API 키를 생성하세요
        # 보안을 위해 환경 변수 OPENAI_API_KEY로 설정하는 것을 권장합니다
        api_key="<your-api-key>",

        # 사용량 추적을 위해 팀과 프로젝트가 필요합니다
        project="<your-team>/<your-project>",
    )

    # <model-id>를 다음 값 중 하나로 교체하세요:
    # meta-llama/Llama-3.1-8B-Instruct
    # deepseek-ai/DeepSeek-V3-0324
    # meta-llama/Llama-3.3-70B-Instruct
    # deepseek-ai/DeepSeek-R1-0528
    # meta-llama/Llama-4-Scout-17B-16E-Instruct
    # microsoft/Phi-4-mini-instruct

    response = client.chat.completions.create(
        model="<model-id>",
        messages=[
            {"role": "system", "content": "<your-system-prompt>"},
            {"role": "user", "content": "<your-prompt>"}
        ],
    )

    print(response.choices[0].message.content)
    ```
  </Tab>
</Tabs>

<div id="list-supported-models">
  #### 지원되는 모델 목록 조회
</div>

API를 사용해 현재 사용 가능한 모든 모델과 해당 ID를 조회하세요. 이는 모델을 동적으로 선택하거나 현재 환경에서 어떤 모델이 사용 가능한지 확인할 때 유용합니다.

<Tabs>
  <Tab title="Bash">
    ```bash
    curl https://api.inference.wandb.ai/v1/models \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer <your-api-key>" \
      -H "OpenAI-Project: <your-team>/<your-project>" \
    ```
  </Tab>

  <Tab title="Python">
    ```python lines
    import openai

    client = openai.OpenAI(
        base_url="https://api.inference.wandb.ai/v1",
        api_key="<your-api-key>",
        project="<your-team>/<your-project>"
    )

    response = client.models.list()

    for model in response.data:
        print(model.id)
    ```
  </Tab>
</Tabs>

<div id="usage-examples">
  ## 사용 예시
</div>

이 섹션에서는 Weave와 함께 W&amp;B Inference를 사용하는 방법을 보여주는 여러 예제를 제공합니다:

* [기본 예시: Weave로 Llama 3.1 8B 추적하기](#basic-example-trace-llama-31-8b-with-weave)
* [고급 예시: Weave Evaluations 및 Leaderboards를 Inference 서비스와 함께 사용하기](#advanced-example-use-weave-evaluations-and-leaderboards-with-the-inference-service)

<div id="basic-example-trace-llama-31-8b-with-weave">
  ### 기본 예시: Weave로 Llama 3.1 8B 추적하기
</div>

다음 Python 코드 예제는 W&amp;B Inference API를 사용해 **Llama 3.1 8B** 모델에 프롬프트를 보내고, 해당 호출을 Weave에서 추적하는 방법을 보여줍니다. 트레이싱을 사용하면 LLM 호출의 전체 입력/출력을 캡처하고, 성능을 모니터링하며, Weave UI에서 결과를 분석할 수 있습니다.

<Tip>
  [Weave에서 트레이싱하는 방법](../tracking/tracing.mdx)에 대해 자세히 알아보세요.
</Tip>

이 예시에서는 다음을 수행합니다:

* OpenAI 호환 클라이언트를 사용해 챗 컴플리션 요청을 보내는 `run_chat` 함수를 정의하고, 이 함수에 `@weave.op()` 데코레이터를 지정합니다.
* 생성된 트레이스는 사용자의 W&amp;B 엔터티와 프로젝트 `project="<your-team>/<your-project>`에 기록되어 연결됩니다.
* 이 함수는 Weave에 의해 자동으로 추적되므로 입력, 출력, 지연 시간(latency), 모델 ID와 같은 메타데이터가 모두 로그됩니다.
* 결과는 터미널에 출력되며, 지정한 프로젝트의 [https://wandb.ai](https://wandb.ai) **Traces** 탭에 해당 트레이스가 표시됩니다.

이 예제를 사용하려면 먼저 [일반 사전 요구 사항](#prerequisites)과 [Python을 통해 API를 사용할 때의 추가 사전 요구 사항](#additional-prerequisites-for-using-the-api-via-python)을 모두 충족해야 합니다.

```python lines
import weave
import openai

# 추적을 위한 Weave 팀 및 프로젝트 설정
weave.init("<your-team>/<your-project>")

client = openai.OpenAI(
    base_url='https://api.inference.wandb.ai/v1',

    # https://wandb.ai/settings 에서 API 키 생성
    api_key="<your-api-key>",

    # W&B 추론 사용량 추적에 필요
    project="wandb/inference-demo",
)

# Weave에서 모델 호출 추적
@weave.op()
def run_chat():
    response = client.chat.completions.create(
        model="meta-llama/Llama-3.1-8B-Instruct",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Tell me a joke."}
        ],
    )
    return response.choices[0].message.content

# 추적된 호출 실행 및 로깅
output = run_chat()
print(output)
```

코드 예제를 실행하면 터미널에 출력된 링크(예: `https://wandb.ai/<your-team>/<your-project>/r/call/01977f8f-839d-7dda-b0c2-27292ef0e04g`)를 클릭해 Weave에서 trace를 확인하거나, 다음을 수행합니다:

1. [https://wandb.ai](https://wandb.ai)로 이동합니다.
2. **Traces** 탭을 선택해 Weave trace를 확인합니다.

이제 [고급 예제](#advanced-example-use-weave-evaluations-and-leaderboards-with-the-inference-service)를 시도해 보세요.

<Frame>
  ![Traces display](/weave/guides/integrations/imgs/image.png)
</Frame>

<div id="advanced-example-use-weave-evaluations-and-leaderboards-with-the-inference-service">
  ### 고급 예제: Inference 서비스에서 Weave Evaluation과 리더보드를 사용하는 방법
</div>

Inference 서비스에서 Weave를 사용해 [모델 호출을 트레이싱](../tracking/tracing.mdx)할 뿐만 아니라, [성능을 평가](../core-types/evaluations.mdx)하고 [리더보드를 게시](../core-types/leaderboards.mdx)할 수도 있습니다. 아래 Python 코드 예제는 간단한 질의응답 데이터셋에서 두 개의 모델을 비교합니다.

이 예제를 사용하려면 [일반 사전 준비 사항](#prerequisites)과 [Python을 통해 API를 사용하기 위한 추가 사전 준비 사항](#additional-prerequisites-for-using-the-api-via-python)을 모두 완료해야 합니다.

```python lines
import os
import asyncio
import openai
import weave
from weave.flow import leaderboard
from weave.trace.ref_util import get_ref

# 추적을 위한 Weave 팀 및 프로젝트 설정
weave.init("<your-team>/<your-project>")

dataset = [
    {"input": "What is 2 + 2?", "target": "4"},
    {"input": "Name a primary color.", "target": "red"},
]

@weave.op
def exact_match(target: str, output: str) -> float:
    return float(target.strip().lower() == output.strip().lower())

class WBInferenceModel(weave.Model):
    model: str

    @weave.op
    def predict(self, prompt: str) -> str:
        client = openai.OpenAI(
            base_url="https://api.inference.wandb.ai/v1",
            # https://wandb.ai/settings 에서 API 키 생성
            api_key="<your-api-key>",
            # W&B 추론 사용량 추적에 필요
            project="<your-team>/<your-project>",
        )
        resp = client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
        )
        return resp.choices[0].message.content

llama = WBInferenceModel(model="meta-llama/Llama-3.1-8B-Instruct")
deepseek = WBInferenceModel(model="deepseek-ai/DeepSeek-V3-0324")

def preprocess_model_input(example):
    return {"prompt": example["input"]}

evaluation = weave.Evaluation(
    name="QA",
    dataset=dataset,
    scorers=[exact_match],
    preprocess_model_input=preprocess_model_input,
)

async def run_eval():
    await evaluation.evaluate(llama)
    await evaluation.evaluate(deepseek)

asyncio.run(run_eval())

spec = leaderboard.Leaderboard(
    name="Inference Leaderboard",
    description="Compare models on a QA dataset",
    columns=[
        leaderboard.LeaderboardColumn(
            evaluation_object_ref=get_ref(evaluation).uri(),
            scorer_name="exact_match",
            summary_metric_path="mean",
        )
    ],
)

weave.publish(spec)
```

다음 코드 샘플을 실행한 뒤 [https://wandb.ai/](https://wandb.ai/)의 W&amp;B 계정으로 이동해 다음을 확인하세요:

* **Traces** 탭으로 이동해 [traces를 확인하세요](../tracking/tracing.mdx)
* **Evals** 탭으로 이동해 [모델 평가 결과를 확인하세요](../core-types/evaluations.mdx)
* **Leaders** 탭으로 이동해 [생성된 리더보드를 확인하세요](../core-types/leaderboards.mdx)

<Frame>
  ![모델 평가 결과 확인](/weave/guides/integrations/imgs/inference-advanced-evals.png)
</Frame>

<Frame>
  ![traces 확인](/weave/guides/integrations/imgs/inference-advanced-leaderboard.png)
</Frame>

<div id="ui">
  ## UI
</div>

이 섹션에서는 W&amp;B UI에서 Inference 서비스를 사용하는 방법을 설명합니다. UI를 통해 Inference 서비스에 접근하기 전에 [사전 요구사항](#prerequisites)을 먼저 완료해야 합니다.

<div id="access-the-inference-service">
  ### Inference 서비스에 접근하기
</div>

Inference 서비스에는 Weave UI에서 두 가지 방법으로 접근할 수 있습니다:

* [직접 링크](#direct-link)
* [Inference 탭에서](#from-the-inference-tab)
* [Playground 탭에서](#from-the-playground-tab)

<div id="direct-link">
  #### 직접 링크
</div>

[https://wandb.ai/inference](https://wandb.ai/inference)로 이동하세요.

<div id="from-the-inference-tab">
  #### Inference 탭에서
</div>

1. [https://wandb.ai/](https://wandb.ai/)에서 W&amp;B 계정에 로그인합니다.
2. 왼쪽 사이드바에서 **Inference**를 선택합니다. 사용 가능한 모델과 모델 정보가 표시된 페이지가 나타납니다.

<Frame>
  ![The Inference tab](/weave/guides/integrations/imgs/inference-ui.png)
</Frame>

<div id="from-the-playground-tab">
  #### Playground 탭에서
</div>

1. 왼쪽 사이드바에서 **Playground**를 선택합니다. Playground 채팅 UI가 표시됩니다.
2. LLM 드롭다운 목록에서 **W&amp;B Inference** 위로 마우스를 가져갑니다. 오른쪽에 사용 가능한 W&amp;B Inference 모델이 표시된 드롭다운이 나타납니다.
3. W&amp;B Inference 모델 드롭다운에서 다음을 할 수 있습니다:
   * 사용 가능한 모델 이름을 클릭하여 [Playground에서 사용해 봅니다](#try-a-model-in-the-playground).
   * [Playground에서 하나 이상의 모델을 비교합니다](#compare-multiple-models)

<Frame>
  ![Playground의 Inference 모델 드롭다운](/weave/guides/integrations/imgs/inference-playground.png)
</Frame>

<div id="try-a-model-in-the-playground">
  ### Playground에서 모델 시험해 보기
</div>

[액세스 옵션 중 하나를 사용해 모델을 선택](#access-the-inference-service)한 후에는 Playground에서 해당 모델을 시험해 볼 수 있습니다. 다음 작업을 수행할 수 있습니다:

* [모델 설정과 매개변수 사용자 지정](../tools/playground#customize-settings)
* [메시지 추가, 재시도, 편집, 삭제](../tools/playground#message-controls)
* [사용자 지정 설정이 적용된 모델 저장 및 재사용](../tools/playground#saved-models)
* [여러 모델 비교](#compare-multiple-models)

<Frame>
  ![Playground에서 Inference 모델 사용하기](/weave/guides/integrations/imgs/inference-playground-single.png)
</Frame>

<div id="compare-multiple-models">
  ### 여러 모델 비교
</div>

Playground에서 여러 Inference 모델을 비교할 수 있습니다. Compare 뷰는 다음 두 위치에서 열 수 있습니다:

* [Inference 탭에서 Compare 뷰 열기](#access-the-compare-view-from-the-inference-tab)
* [Playground 탭에서 Compare 뷰 열기](#access-the-compare-view-from-the-playground-tab)

<div id="access-the-compare-view-from-the-inference-tab">
  #### Inference 탭에서 Compare 뷰 열기
</div>

1. 왼쪽 사이드바에서 **Inference**를 선택합니다. 사용 가능한 모델과 모델 정보가 표시된 페이지가 열립니다.
2. 비교할 모델을 선택하려면 모델 이름을 제외하고 모델 카드의 아무 곳이나 클릭합니다. 선택되면 모델 카드의 테두리가 파란색으로 강조 표시됩니다.
3. 비교하려는 각 모델에 대해 2단계를 반복합니다.
4. 선택된 카드 중 하나에서 **Compare N models in the Playground** 버튼을 클릭합니다 (`N`은 비교 중인 모델의 개수입니다. 예를 들어, 3개의 모델이 선택된 경우 버튼은 **Compare 3 models in the Playground**로 표시됩니다). 비교 뷰가 열립니다.

이제 Playground에서 모델을 비교하고, [Try a model in the Playground](#try-a-model-in-the-playground)에 설명된 기능을 모두 사용할 수 있습니다.

<Frame>
  ![Playground에서 비교할 여러 모델 선택](/weave/guides/integrations/imgs/inference-playground-compare.png)
</Frame>

<div id="access-the-compare-view-from-the-playground-tab">
  #### Playground 탭에서 Compare 뷰로 이동하기
</div>

1. 왼쪽 사이드바에서 **Playground**를 선택합니다. Playground 채팅 UI가 표시됩니다.
2. LLM 드롭다운 목록에서 **W&amp;B Inference** 위로 마우스를 가져갑니다. 오른쪽에 사용 가능한 W&amp;B Inference 모델이 포함된 드롭다운이 표시됩니다.
3. 드롭다운에서 **Compare**를 선택합니다. **Inference** 탭이 표시됩니다.
4. 비교할 모델을 선택하려면 모델 카드에서 모델 이름을 제외한 아무 곳이나 클릭합니다. 선택되었음을 나타내기 위해 모델 카드의 테두리가 파란색으로 강조 표시됩니다.
5. 비교하려는 각 모델에 대해 4단계를 반복합니다.
6. 선택된 카드 중 하나에서 **Compare N models in the Playground** 버튼을 클릭합니다 (`N`은 비교 중인 모델 수입니다. 예를 들어 3개의 모델이 선택된 경우 버튼에는 **Compare 3 models in the Playground**가 표시됩니다). 비교 뷰가 열립니다.

이제 Playground에서 모델을 비교할 수 있으며, [Playground에서 모델 사용해 보기](#try-a-model-in-the-playground)에 설명된 모든 기능을 사용할 수 있습니다.

<div id="view-billing-and-usage-information">
  ### 청구 및 사용량 정보 보기
</div>

조직 관리자라면 W&amp;B UI에서 현재 Inference 크레딧 잔액, 사용 이력, 예정 청구 내역(해당되는 경우)을 직접 확인할 수 있습니다:

1. W&amp;B UI에서 W&amp;B **Billing** 페이지로 이동합니다.
2. 오른쪽 아래에 Inference 청구 정보 카드가 표시됩니다. 여기에서 다음을 수행할 수 있습니다:

* Inference 청구 정보 카드에서 **View usage** 버튼을 클릭하여 시간 경과에 따른 사용량을 확인합니다.
* 유료 요금제를 사용하는 경우, 예정된 Inference 청구 금액을 확인합니다.

<Tip>
  모델별 과금 기준을 확인하려면 [Inference 가격 페이지](https://wandb.ai/site/pricing/inference)를 방문하세요.
</Tip>

<div id="usage-information-and-limits">
  ## 사용 정보 및 제한 사항
</div>

다음 섹션에서는 중요한 사용 정보와 제한 사항을 설명합니다. 서비스를 사용하기 전에 반드시 확인하세요.

<div id="geographic-restrictions">
  ### 지리적 제한
</div>

Inference 서비스는 지원되는 지리적 지역에서만 이용할 수 있습니다. 자세한 내용은 [이용 약관](https://docs.coreweave.com/docs/policies/terms-of-service/terms-of-use#geographic-restrictions)을 참조하세요.

<div id="concurrency-limits">
  ### 동시성 제한
</div>

공정한 사용과 안정적인 성능을 보장하기 위해 W&amp;B Inference API는 사용자 및 프로젝트 수준에서 동시성 한도를 적용합니다. 이러한 한도는 다음에 도움이 됩니다:

* 오용을 방지하고 API 안정성을 보호합니다
* 모든 사용자가 접근할 수 있도록 보장합니다
* 인프라 부하를 효과적으로 관리합니다

동시성 한도를 초과하면 API는 `429 Concurrency limit reached for requests` 응답을 반환합니다. 이 오류를 해결하려면 동시에 보내는 요청 수를 줄이십시오.

<div id="pricing">
  ### 요금
</div>

모델 요금 정보는 [https://wandb.ai/site/pricing/inference](https://wandb.ai/site/pricing/inference)에서 확인할 수 있습니다.

<div id="api-errors">
  ## API errors
</div>

| Error Code | Message                                                                     | Cause                                           | Solution                                                                               |
| ---------- | --------------------------------------------------------------------------- | ----------------------------------------------- | -------------------------------------------------------------------------------------- |
| 401        | Invalid Authentication                                                      | 인증 자격 증명이 올바르지 않거나 W&amp;B 프로젝트 엔터티 및/또는 이름이 잘못되었습니다.              | 올바른 API 키를 사용하고 있는지, 그리고 W&amp;B 프로젝트 이름과 엔터티가 정확한지 확인하십시오.                                              |
| 403        | Country, region, or territory not supported                                 | 지원되지 않는 지역에서 API에 액세스하고 있습니다. | [Geographic restrictions](#geographic-restrictions)를 참조하십시오.                                       |
| 429        | Concurrency limit reached for requests                                      | 동시에 처리되는 요청이 너무 많습니다.                   | 동시에 처리되는 요청 수를 줄이십시오.               |
| 429        | You exceeded your current quota, please check your plan and billing details | 현재 할당량을 초과했거나 월별 지출 한도에 도달했습니다. | 크레딧을 추가로 구매하거나 사용 한도를 늘리십시오.                       |
| 500        | The server had an error while processing your request                       | 내부 서버 오류입니다.                          | 잠시 후 다시 시도하고, 문제가 지속되면 지원팀에 문의하십시오. |
| 503        | The engine is currently overloaded, please try again later                  | 서버에 트래픽이 과도하게 몰리고 있습니다.            | 잠시 대기한 후 요청을 다시 시도하십시오.                                                |