# W&B Inference

_W&B Inference_는 W&B Weave 및 OpenAI 호환 API를 통해 주요 오픈 소스 파운데이션 모델에 대한 엑세스를 제공합니다. W&B Inference를 사용하면 다음을 수행할 수 있습니다:

- 호스팅 제공업체에 가입하거나 모델을 직접 호스팅할 필요 없이 AI 애플리케이션과 에이전트를 개발할 수 있습니다.
- W&B Weave Playground에서 지원되는 모델들을 사용해 볼 수 있습니다.

<Warning>
W&B Inference 크레딧은 한시적으로 Free, Pro, Academic 플랜에 포함됩니다. Enterprise의 경우 가용성이 다를 수 있습니다. 크레딧이 소진되면:

- Free 계정은 Inference를 계속 사용하기 위해 Pro 플랜으로 업그레이드해야 합니다.
- Pro 플랜 사용자는 모델별 가격에 따라 월별로 Inference 초과 사용분에 대해 과금됩니다.

자세한 내용은 [pricing page](https://wandb.ai/site/pricing/) 및 [W&B Inference model costs](https://wandb.ai/site/pricing/inference)를 참조하세요.
</Warning>

Weave를 사용하여 W&B Inference 기반 애플리케이션을 trace, evaluate, monitor 및 iterate할 수 있습니다.

| 모델 | 모델 ID (API 사용 시) | 유형 | 컨텍스트 윈도우 | 파라미터 | 설명 |
|------------------|-----------------------------------------------|---------------|----------------|-----------------------------|-----------------------------------------------------------------------------|
| DeepSeek R1-0528 | deepseek-ai/DeepSeek-R1-0528                  | Text          | 161K           | 37B - 680B (Active - Total) | 복잡한 코딩, 수학, 구조화된 문서 분석을 포함한 정밀한 추론 작업에 최적화되었습니다. |
| DeepSeek V3-0324 | deepseek-ai/DeepSeek-V3-0324                  | Text          | 161K           | 37B - 680B (Active - Total) | 고난도 언어 처리 및 포괄적인 문서 분석에 맞춤화된 강력한 Mixture-of-Experts 모델입니다. |
| Llama 3.1 8B     | meta-llama/Llama-3.1-8B-Instruct              | Text          | 128K           | 8B (Total)                  | 응답성이 뛰어난 다국어 챗봇 상호작용에 최적화된 효율적인 대화형 모델입니다. |
| Llama 3.3 70B    | meta-llama/Llama-3.3-70B-Instruct             | Text          | 128K           | 70B (Total)                 | 대화형 작업, 상세한 지시 이행 및 코딩에 탁월한 다국어 모델입니다. |
| Llama 4 Scout    | meta-llama/Llama-4-Scout-17B-16E-Instruct     | Text, Vision  | 64K            | 17B - 109B (Active - Total) | 텍스트와 이미지 이해를 통합하여 시각적 작업과 결합 분석에 이상적인 멀티모달 모델입니다. |
| Phi 4 Mini       | microsoft/Phi-4-mini-instruct                | Text          | 128K           | 3.8B (Active - Total)       | 자원이 제한된 환경에서 빠른 응답에 이상적인 작고 효율적인 모델입니다. |

이 가이드는 다음 정보를 제공합니다:

- [필수 사항](#prerequisites)
  - [Python을 통한 API 사용을 위한 추가 필수 사항](#additional-prerequisites-for-using-the-api-via-python)
- [API 사양](#api-specification)
  - [엔드포인트](#endpoint)
  - [사용 가능한 메소드](#available-methods)
      - [Chat completions](#chat-completions)
      - [지원 모델 목록 조회](#list-supported-models)
- [사용 예시](#usage-examples)
- [UI](#ui)
  - [Inference 서비스 엑세스](#access-the-inference-service)
  - [Playground에서 모델 사용해 보기](#try-a-model-in-the-playground)
  - [여러 모델 비교하기](#compare-multiple-models)
  - [결제 및 사용량 정보 확인](#view-billing-and-usage-information)
- [사용 정보 및 제한 사항](#usage-information-and-limits)
- [API 에러](#api-errors)

## 필수 사항

API 또는 W&B Weave UI를 통해 W&B Inference 서비스에 엑세스하려면 다음 필수 사항이 필요합니다.

1. W&B 계정. [여기](https://app.wandb.ai/login?signup=true&_gl=1*1yze8dp*_ga*ODIxMjU5MTk3LjE3NDk0OTE2NDM.*_ga_GMYDGNGKDT*czE3NDk4NDYxMzgkbzEyJGcwJHQxNzQ5ODQ2MTM4JGo2MCRsMCRoMA..*_ga_JH1SJHJQXJ*czE3NDk4NDU2NTMkbzI1JGcxJHQxNzQ5ODQ2MTQ2JGo0NyRsMCRoMA..*_gcl_au*MTE4ODk1MzY1OC4xNzQ5NDkxNjQzLjk1ODA2MjQwNC4xNzQ5NTgyMTUzLjE3NDk1ODIxNTM.)에서 가입하세요.
2. W&B API 키. [User Settings](https://wandb.ai/settings)에서 API 키를 생성하세요.
3. W&B Project. 
4. Python을 통해 Inference 서비스를 사용하는 경우, [Python을 통한 API 사용을 위한 추가 필수 사항](#additional-prerequisites-for-using-the-api-via-python)을 참조하세요.

### Python을 통한 API 사용을 위한 추가 필수 사항

Python을 통해 Inference API를 사용하려면 먼저 일반 필수 사항을 완료하십시오. 그런 다음 로컬 환경에 `openai` 및 `weave` 라이브러리를 설치합니다.

```bash
pip install openai weave
```

<Note>
`weave` 라이브러리는 Weave를 사용하여 LLM 애플리케이션을 trace하려는 경우에만 필요합니다. Weave 시작에 대한 자세한 내용은 [Weave Quickstart](/weave/quickstart)를 참조하세요. 

Weave와 함께 W&B Inference 서비스를 사용하는 방법을 보여주는 사용 예시는 [API 사용 예시](#usage-examples)를 참조하세요.
</Note>

## API 사양

다음 섹션에서는 API 사양 정보와 API 사용 예시를 제공합니다. 

- [엔드포인트](#endpoint)
- [사용 가능한 메소드](#available-methods)
- [사용 예시](#usage-examples)

### 엔드포인트

Inference 서비스는 다음 엔드포인트를 통해 엑세스할 수 있습니다:

```plaintext
https://api.inference.wandb.ai/v1
```

<Warning>
이 엔드포인트에 엑세스하려면 Inference 서비스 크레딧이 할당된 W&B 계정, 유효한 W&B API 키, 그리고 W&B Entity (또는 "Team") 및 Project가 있어야 합니다. 이 가이드의 코드 샘플에서 Entity (Team) 및 Project는 `<your-team>\<your-project>`로 표기됩니다.
</Warning>

### 사용 가능한 메소드

Inference 서비스는 다음 API 메소드를 지원합니다:

- [Chat completions](#chat-completions)
- [지원 모델 목록 조회](#list-supported-models)

#### Chat completions

사용 가능한 주요 API 메소드는 `/chat/completions`이며, 지원되는 모델에 메시지를 보내고 답변을 받기 위한 OpenAI 호환 요청 형식을 지원합니다. Weave와 함께 W&B Inference 서비스를 사용하는 방법을 보여주는 사용 예시는 [API 사용 예시](#usage-examples)를 참조하세요.

Chat completion을 생성하려면 다음이 필요합니다:

- Inference 서비스 베이스 URL `https://api.inference.wandb.ai/v1`
- W&B API 키 `<your-api-key>`
- W&B Entity 및 Project 이름 `<your-team>/<your-project>`
- 사용하려는 모델의 ID (다음 중 하나):
  - `meta-llama/Llama-3.1-8B-Instruct`
  - `deepseek-ai/DeepSeek-V3-0324`
  - `meta-llama/Llama-3.3-70B-Instruct`
  - `deepseek-ai/DeepSeek-R1-0528`
  - `meta-llama/Llama-4-Scout-17B-16E-Instruct`
  - `microsoft/Phi-4-mini-instruct`

<Tabs>
  <Tab title="Bash">
    ```bash
    curl https://api.inference.wandb.ai/v1/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer <your-api-key>" \
      -H "OpenAI-Project: <your-team>/<your-project>" \
      -d '{
        "model": "<model-id>",
        "messages": [
          { "role": "system", "content": "You are a helpful assistant." },
          { "role": "user", "content": "Tell me a joke." }
        ]
      }'
    ```
  </Tab>
  <Tab title="Python">
    ```python lines
    import openai

    client = openai.OpenAI(
        # 커스텀 베이스 URL은 W&B Inference를 가리킵니다.
        base_url='https://api.inference.wandb.ai/v1',

        # https://wandb.ai/settings 에서 API 키를 생성하세요.
        # 보안을 위해 환경 변수 OPENAI_API_KEY에 설정하는 것을 권장합니다.
        api_key="<your-api-key>",

        # 사용량 추적을 위해 Team과 Project가 필요합니다.
        project="<your-team>/<your-project>",
    )

    # <model-id>를 다음 값 중 하나로 대체하세요:
    # meta-llama/Llama-3.1-8B-Instruct
    # deepseek-ai/DeepSeek-V3-0324
    # meta-llama/Llama-3.3-70B-Instruct
    # deepseek-ai/DeepSeek-R1-0528
    # meta-llama/Llama-4-Scout-17B-16E-Instruct
    # microsoft/Phi-4-mini-instruct

    response = client.chat.completions.create(
        model="<model-id>",
        messages=[
            {"role": "system", "content": "<your-system-prompt>"},
            {"role": "user", "content": "<your-prompt>"}
        ],
    )

    print(response.choices[0].message.content)
    ```
  </Tab>
</Tabs>

#### 지원 모델 목록 조회

API를 사용하여 현재 사용 가능한 모든 모델과 해당 ID를 조회합니다. 이는 모델을 동적으로 선택하거나 환경에서 사용 가능한 대상을 확인하는 데 유용합니다.

<Tabs>
  <Tab title="Bash">
    ```bash
    curl https://api.inference.wandb.ai/v1/models \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer <your-api-key>" \
      -H "OpenAI-Project: <your-team>/<your-project>" \
    ```
  </Tab>
  <Tab title="Python">
    ```python lines
    import openai

    client = openai.OpenAI(
        base_url="https://api.inference.wandb.ai/v1",
        api_key="<your-api-key>",
        project="<your-team>/<your-project>"
    )

    response = client.models.list()

    for model in response.data:
        print(model.id)
    ```
  </Tab>
</Tabs>

## 사용 예시

이 섹션에서는 Weave와 함께 W&B Inference를 사용하는 여러 예시를 제공합니다:

- [기본 예시: Weave로 Llama 3.1 8B Trace하기](#basic-example-trace-llama-31-8b-with-weave)
- [심화 예시: Inference 서비스와 함께 Weave Evaluations 및 Leaderboards 사용하기](#advanced-example-use-weave-evaluations-and-leaderboards-with-the-inference-service) 

### 기본 예시: Weave로 Llama 3.1 8B Trace하기

다음 Python 코드 샘플은 W&B Inference API를 사용하여 **Llama 3.1 8B** 모델에 프롬프트를 보내고 Weave에서 호출을 trace하는 방법을 보여줍니다. Tracing을 통해 LLM 호출의 전체 입출력을 캡처하고, 성능을 모니터링하며, Weave UI에서 결과를 분석할 수 있습니다.

<Tip>
[Weave의 tracing](../tracking/tracing.mdx)에 대해 자세히 알아보세요.
</Tip>

이 예시에서는:

- `@weave.op()` 데코레이터가 적용된 함수 `run_chat`을 정의하며, 이 함수는 OpenAI 호환 클라이언트를 사용하여 chat completion 요청을 수행합니다.
- Trace는 지정된 W&B Entity 및 Project(`project="<your-team>/<your-project>`)와 연결되어 기록됩니다.
- 함수는 Weave에 의해 자동으로 trace되므로 입력, 출력, 지연 시간 및 메타데이터(모델 ID 등)가 로그에 남습니다.
- 결과는 터미널에 출력되며, trace는 지정된 프로젝트 아래 [https://wandb.ai](https://wandb.ai)의 **Traces** 탭에 나타납니다.

이 예시를 사용하려면 [일반 필수 사항](#prerequisites) 및 [Python을 통한 API 사용을 위한 추가 필수 사항](#additional-prerequisites-for-using-the-api-via-python)을 완료해야 합니다.

```python lines
import weave
import openai

# Tracing을 위한 Weave team과 project 설정
weave.init("<your-team>/<your-project>")

client = openai.OpenAI(
    base_url='https://api.inference.wandb.ai/v1',

    # https://wandb.ai/settings 에서 API 키를 생성하세요.
    api_key="<your-api-key>",

    # W&B inference 사용량 추적을 위해 필요합니다.
    project="wandb/inference-demo",
)

# Weave에서 모델 호출 trace
@weave.op()
def run_chat():
    response = client.chat.completions.create(
        model="meta-llama/Llama-3.1-8B-Instruct",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Tell me a joke."}
        ],
    )
    return response.choices[0].message.content

# Trace된 호출 실행 및 로그 기록
output = run_chat()
print(output)
```

코드 샘플을 실행한 후 터미널에 출력된 링크(예: `https://wandb.ai/<your-team>/<your-project>/r/call/01977f8f-839d-7dda-b0c2-27292ef0e04g`)를 클릭하거나 다음 단계를 통해 Weave에서 trace를 볼 수 있습니다:

1. [https://wandb.ai](https://wandb.ai)로 이동합니다.
2. **Traces** 탭을 선택하여 Weave trace를 확인합니다.

다음으로, [심화 예시](#advanced-example-use-weave-evaluations-and-leaderboards-with-the-inference-service)를 시도해 보세요.

<Frame>
![Traces 표시](/weave/guides/integrations/imgs/image.png)
</Frame>

### 심화 예시: Inference 서비스와 함께 Weave Evaluations 및 Leaderboards 사용하기

Inference 서비스와 함께 Weave를 사용하여 [모델 호출을 trace](../tracking/tracing.mdx)하는 것 외에도, [성능을 evaluate](../core-types/evaluations.mdx)하고 [leaderboard를 게시](../core-types/leaderboards.mdx)할 수 있습니다. 다음 Python 코드 샘플은 간단한 질의응답 데이터셋에서 두 모델을 비교합니다.

이 예시를 사용하려면 [일반 필수 사항](#prerequisites) 및 [Python을 통한 API 사용을 위한 추가 필수 사항](#additional-prerequisites-for-using-the-api-via-python)을 완료해야 합니다.

```python lines
import os
import asyncio
import openai
import weave
from weave.flow import leaderboard
from weave.trace.ref_util import get_ref

# Tracing을 위한 Weave team과 project 설정
weave.init("<your-team>/<your-project>")

dataset = [
    {"input": "What is 2 + 2?", "target": "4"},
    {"input": "Name a primary color.", "target": "red"},
]

@weave.op
def exact_match(target: str, output: str) -> float:
    return float(target.strip().lower() == output.strip().lower())

class WBInferenceModel(weave.Model):
    model: str

    @weave.op
    def predict(self, prompt: str) -> str:
        client = openai.OpenAI(
            base_url="https://api.inference.wandb.ai/v1",
            # https://wandb.ai/settings 에서 API 키를 생성하세요.
            api_key="<your-api-key>",
            # W&B inference 사용량 추적을 위해 필요합니다.
            project="<your-team>/<your-project>",
        )
        resp = client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
        )
        return resp.choices[0].message.content

llama = WBInferenceModel(model="meta-llama/Llama-3.1-8B-Instruct")
deepseek = WBInferenceModel(model="deepseek-ai/DeepSeek-V3-0324")

def preprocess_model_input(example):
    return {"prompt": example["input"]}

evaluation = weave.Evaluation(
    name="QA",
    dataset=dataset,
    scorers=[exact_match],
    preprocess_model_input=preprocess_model_input,
)

async def run_eval():
    await evaluation.evaluate(llama)
    await evaluation.evaluate(deepseek)

asyncio.run(run_eval())

spec = leaderboard.Leaderboard(
    name="Inference Leaderboard",
    description="Compare models on a QA dataset",
    columns=[
        leaderboard.LeaderboardColumn(
            evaluation_object_ref=get_ref(evaluation).uri(),
            scorer_name="exact_match",
            summary_metric_path="mean",
        )
    ],
)

weave.publish(spec)
```

위의 코드 샘플을 실행한 후 [https://wandb.ai/](https://wandb.ai/)의 W&B 계정으로 이동하여 다음을 수행합니다:

- **Traces** 탭으로 이동하여 [trace 확인](../tracking/tracing.mdx)
- **Evals** 탭으로 이동하여 [모델 evaluation 확인](../core-types/evaluations.mdx)
- **Leaders** 탭으로 이동하여 [생성된 leaderboard 확인](../core-types/leaderboards.mdx)

<Frame>
![모델 evaluation 확인](/weave/guides/integrations/imgs/inference-advanced-evals.png)
</Frame>
<Frame>
![trace 확인](/weave/guides/integrations/imgs/inference-advanced-leaderboard.png)
</Frame>

## UI

다음 섹션에서는 W&B UI에서 Inference 서비스를 사용하는 방법을 설명합니다. UI를 통해 Inference 서비스에 엑세스하기 전에 [필수 사항](#prerequisites)을 완료하십시오.

### Inference 서비스 엑세스

Weave UI의 두 가지 위치에서 Inference 서비스에 엑세스할 수 있습니다:

- [직접 링크](#direct-link)
- [Inference 탭에서](#from-the-inference-tab)
- [Playground 탭에서](#from-the-playground-tab)

#### 직접 링크

[https://wandb.ai/inference](https://wandb.ai/inference)로 이동합니다.

#### Inference 탭에서

1. [https://wandb.ai/](https://wandb.ai/)의 W&B 계정으로 이동합니다.
2. 왼쪽 사이드바에서 **Inference**를 선택합니다. 사용 가능한 모델 및 모델 정보 페이지가 표시됩니다.

<Frame>
![Inference 탭](/weave/guides/integrations/imgs/inference-ui.png)
</Frame>

#### Playground 탭에서

1. 왼쪽 사이드바에서 **Playground**를 선택합니다. Playground 채팅 UI가 표시됩니다.
2. LLM 드롭다운 목록에서 **W&B Inference** 위로 마우스를 가져갑니다. 오른쪽에 사용 가능한 W&B Inference 모델 드롭다운이 표시됩니다.
3. W&B Inference 모델 드롭다운에서 다음을 수행할 수 있습니다:
   - 사용 가능한 모델의 이름을 클릭하여 [Playground에서 시도해 보기](#try-a-model-in-the-playground).
   - [Playground에서 하나 이상의 모델 비교하기](#compare-multiple-models).

<Frame>
![Playground의 Inference 모델 드롭다운](/weave/guides/integrations/imgs/inference-playground.png)
</Frame>

### Playground에서 모델 사용해 보기

[엑세스 옵션 중 하나를 사용하여 모델을 선택](#access-the-inference-service)하면 Playground에서 모델을 사용해 볼 수 있습니다. 다음 기능을 사용할 수 있습니다:

- [모델 설정 및 파라미터 커스텀](../tools/playground#customize-settings)
- [메시지 추가, 재시도, 편집 및 삭제](../tools/playground#message-controls) 
- [커스텀 설정으로 모델 저장 및 재사용](../tools/playground#saved-models)
- [여러 모델 비교하기](#compare-multiple-models)

<Frame>
![Playground에서 Inference 모델 사용하기](/weave/guides/integrations/imgs/inference-playground-single.png)
</Frame>

### 여러 모델 비교하기

Playground에서 여러 Inference 모델을 비교할 수 있습니다. Compare 뷰는 두 가지 위치에서 엑세스할 수 있습니다:

- [Inference 탭에서 Compare 뷰 엑세스](#access-the-compare-view-from-the-inference-tab)
- [Playground 탭에서 Compare 뷰 엑세스](#access-the-compare-view-from-the-playground-tab)

#### Inference 탭에서 Compare 뷰 엑세스

1. 왼쪽 사이드바에서 **Inference**를 선택합니다. 사용 가능한 모델 및 모델 정보 페이지가 표시됩니다.
2. 비교할 모델을 선택하려면 모델 카드의 아무 곳이나(모델 이름 제외) 클릭합니다. 선택을 나타내기 위해 모델 카드의 테두리가 파란색으로 강조됩니다.
3. 비교하려는 각 모델에 대해 2단계를 반복합니다.
4. 선택된 카드 중 하나에서 **Compare N models in the Playground** 버튼을 클릭합니다 (`N`은 비교 중인 모델의 수입니다. 예를 들어 3개의 모델이 선택되면 버튼에 **Compare 3 models in the Playground**라고 표시됩니다). 비교 뷰가 열립니다. 

이제 Playground에서 모델을 비교하고 [Playground에서 모델 사용해 보기](#try-a-model-in-the-playground)에서 설명한 기능을 사용할 수 있습니다.

<Frame>
![Playground에서 비교할 여러 모델 선택](/weave/guides/integrations/imgs/inference-playground-compare.png)
</Frame>

#### Playground 탭에서 Compare 뷰 엑세스

1. 왼쪽 사이드바에서 **Playground**를 선택합니다. Playground 채팅 UI가 표시됩니다.
2. LLM 드롭다운 목록에서 **W&B Inference** 위로 마우스를 가져갑니다. 오른쪽에 사용 가능한 W&B Inference 모델 드롭다운이 표시됩니다.
3. 드롭다운에서 **Compare**를 선택합니다. **Inference** 탭이 표시됩니다.
4. 비교할 모델을 선택하려면 모델 카드의 아무 곳이나(모델 이름 제외) 클릭합니다. 선택을 나타내기 위해 모델 카드의 테두리가 파란색으로 강조됩니다.
5. 비교하려는 각 모델에 대해 4단계를 반복합니다.
6. 선택된 카드 중 하나에서 **Compare N models in the Playground** 버튼을 클릭합니다 (`N`은 비교 중인 모델의 수입니다. 예를 들어 3개의 모델이 선택되면 버튼에 **Compare 3 models in the Playground**라고 표시됩니다). 비교 뷰가 열립니다. 

이제 Playground에서 모델을 비교하고 [Playground에서 모델 사용해 보기](#try-a-model-in-the-playground)에서 설명한 기능을 사용할 수 있습니다.

### 결제 및 사용량 정보 확인

조직 관리자는 W&B UI에서 직접 현재 Inference 크레딧 잔액, 사용 내역 및 향후 청구 예정 금액(해당하는 경우)을 추적할 수 있습니다.

1. W&B UI에서 W&B **Billing** 페이지로 이동합니다.
2. 우측 하단에 Inference 결제 정보 카드가 표시됩니다. 여기에서 다음을 수행할 수 있습니다:
- Inference 결제 정보 카드의 **View usage** 버튼을 클릭하여 시간 경과에 따른 사용량을 확인합니다.
- 유료 플랜인 경우, 예정된 inference 요금을 확인합니다.

<Tip>
[Inference pricing page](https://wandb.ai/site/pricing/inference)를 방문하여 모델별 가격 세부 사항을 확인하세요.
</Tip>

## 사용 정보 및 제한 사항 

다음 섹션에서는 중요한 사용 정보 및 제한 사항을 설명합니다. 서비스를 사용하기 전에 이 정보를 숙지하시기 바랍니다.

### 지리적 제한 사항

Inference 서비스는 지원되는 지리적 위치에서만 엑세스할 수 있습니다. 자세한 내용은 [Terms of Service](https://docs.coreweave.com/docs/policies/terms-of-service/terms-of-use#geographic-restrictions)를 참조하세요.

### 동시성 제한 사항

공정한 사용과 안정적인 성능을 보장하기 위해 W&B Inference API는 사용자 및 프로젝트 수준에서 속도 제한(rate limits)을 적용합니다. 이러한 제한은 다음을 돕습니다:

- 오용 방지 및 API 안정성 보호
- 모든 사용자의 엑세스 보장
- 인프라 부하의 효과적인 관리

속도 제한을 초과하면 API는 `429 Concurrency limit reached for requests` 응답을 반환합니다. 이 에러를 해결하려면 동시 요청 수를 줄이십시오. 

### 가격

모델 가격 정보는 [https://wandb.ai/site/pricing/inference](https://wandb.ai/site/pricing/inference)를 방문하세요.

## API 에러

| 에러 코드 | 메시지 | 원인 | 해결 방법 |
| ---------- | --------------------------------------------------------------------------- | ----------------------------------------------- | -------------------------------------------------------------------------------------- |
| 401        | Invalid Authentication                                                      | 인증 정보가 잘못되었거나 W&B Project Entity 또는 이름이 올바르지 않습니다. | 올바른 API 키를 사용하고 있는지, W&B Project 이름과 Entity가 맞는지 확인하세요. |
| 403        | Country, region, or territory not supported                                 | 지원되지 않는 위치에서 API에 엑세스하고 있습니다. | [지리적 제한 사항](#geographic-restrictions)을 확인하세요. |
| 429        | Concurrency limit reached for requests                                      | 동시 요청이 너무 많습니다. | 동시 요청 수를 줄이십시오. |
| 429        | You exceeded your current quota, please check your plan and billing details | 크레딧이 소진되었거나 월간 지출 한도에 도달했습니다. | 크레딧을 추가 구매하거나 한도를 늘리십시오. |
| 500        | The server had an error while processing your request                       | 내부 서버 에러입니다. | 잠시 후 다시 시도하고 문제가 지속되면 고객 지원에 문의하세요. |
| 503        | The engine is currently overloaded, please try again later                  | 서버에 트래픽이 몰리고 있습니다. | 잠시 후 요청을 다시 시도하세요. |