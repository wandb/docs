---
title: Inference
---

<div id="wb-inference">
  # W&amp;B Inference
</div>

*W&amp;B Inference* 는 W&amp;B Weave 및 OpenAI 호환 API를 통해 선도적인 오픈 소스 파운데이션 모델에 액세스를 제공합니다. W&amp;B Inference를 사용하면 다음을 수행할 수 있습니다:

* 호스팅 제공업체에 가입하거나 모델을 직접 호스팅하지 않고도 AI 애플리케이션과 에이전트를 개발할 수 있습니다.
* 지원되는 모델을 W&amp;B Weave Playground에서 시험해 볼 수 있습니다.

<Warning>
  W&amp;B Inference 크레딧은 한시적으로 Free, Pro, Academic 요금제에 포함되어 있습니다. Enterprise의 경우 제공 여부가 상이할 수 있습니다. 크레딧을 모두 사용하면:

  * Free 계정은 Inference를 계속 사용하려면 Pro 요금제로 업그레이드해야 합니다.
  * Pro 요금제 사용자는 모델별 가격에 따라 Inference 초과 사용량에 대해 매월 청구됩니다.

  자세한 내용은 [pricing page](https://wandb.ai/site/pricing/) 및 [W&amp;B Inference model costs](https://wandb.ai/site/pricing/inference)를 참고하세요.
</Warning>

Weave를 사용하면 W&amp;B Inference로 구동되는 애플리케이션을 추적(trace)하고, 평가하고, 모니터링하며, 반복(iteration)하여 개선할 수 있습니다.

| Model            | Model ID (for API usage)                                     | Type(s)       | Context Window | Parameters                  | Description                                                                 |
|------------------|-----------------------------------------------|---------------|----------------|-----------------------------|-----------------------------------------------------------------------------|
| DeepSeek R1-0528 | deepseek-ai/DeepSeek-R1-0528                  | Text          | 161K           | 37B - 680B (Active - Total) | 복잡한 코딩, 수학, 구조화된 문서 분석을 포함한 정밀 추론 작업에 최적화된 모델입니다. |
| DeepSeek V3-0324 | deepseek-ai/DeepSeek-V3-0324                  | Text          | 161K           | 37B - 680B (Active - Total) | 높은 복잡도의 언어 처리와 포괄적인 문서 분석에 특화된 견고한 Mixture-of-Experts 모델입니다. |
| Llama 3.1 8B     | meta-llama/Llama-3.1-8B-Instruct              | Text          | 128K           | 8B (Total)                  | 반응성이 뛰어난 다국어 챗봇 상호작용에 최적화된 효율적인 대화형 모델입니다. |
| Llama 3.3 70B    | meta-llama/Llama-3.3-70B-Instruct             | Text          | 128K           | 70B (Total)                 | 대화형 작업, 상세한 지시 사항 준수, 코딩에 뛰어난 다국어 모델입니다. |
| Llama 4 Scout    | meta-llama/Llama-4-Scout-17B-16E-Instruct     | Text, Vision  | 64K            | 17B - 109B (Active - Total) | 텍스트와 이미지 이해를 통합한 멀티모달 모델로, 시각적 작업과 복합 분석에 적합합니다. |
| Phi 4 Mini       | microsoft/Phi-4-mini-instruct                | Text          | 128K           | 3.8B (Active - Total)       | 리소스가 제한된 환경에서 빠른 응답에 적합한 작고 효율적인 모델입니다. |

이 가이드는 다음 정보를 제공합니다:

* [사전 준비 사항](#prerequisites)
  * [Python을 통한 API 사용을 위한 추가 사전 준비 사항](#additional-prerequisites-for-using-the-api-via-python)
* [API 사양](#api-specification)
  * [엔드포인트](#endpoint)
  * [사용 가능한 메서드](#available-methods)
    * [Chat completions](#chat-completions)
    * [지원되는 모델 나열](#list-supported-models)
* [사용 예시](#usage-examples)
* [UI](#ui)
  * [Inference 서비스에 액세스](#access-the-inference-service)
  * [Playground에서 모델 시험해 보기](#try-a-model-in-the-playground)
  * [여러 모델 비교](#compare-multiple-models)
  * [결제 및 사용량 정보 보기](#view-billing-and-usage-information)
* [사용 정보 및 제한](#usage-information-and-limits)
* [API 오류](#api-errors)

<div id="prerequisites">
  ## 사전 준비 사항
</div>

API 또는 W&amp;B Weave UI를 통해 W&amp;B Inference 서비스에 액세스하려면 다음 사전 준비 사항이 필요합니다.

1. W&amp;B 계정. [여기](https://app.wandb.ai/login?signup=true&_gl=1*1yze8dp*_ga*ODIxMjU5MTk3LjE3NDk0OTE2NDM.*_ga_GMYDGNGKDT*czE3NDk4NDYxMzgkbzEyJGcwJHQxNzQ5ODQ2MTM4JGo2MCRsMCRoMA..*_ga_JH1SJHJQXJ*czE3NDk4NDU2NTMkbzI1JGcxJHQxNzQ5ODQ2MTQ2JGo0NyRsMCRoMA..*_gcl_au*MTE4ODk1MzY1OC4xNzQ5NDkxNjQzLjk1ODA2MjQwNC4xNzQ5NTgyMTUzLjE3NDk1ODIxNTM.)에서 가입하세요.
2. W&amp;B API 키. [User Settings](https://wandb.ai/settings)에서 API 키를 생성하세요.
3. W&amp;B 프로젝트.
4. Python으로 Inference 서비스를 사용하는 경우 [Python을 통해 API를 사용하기 위한 추가 사전 준비 사항](#additional-prerequisites-for-using-the-api-via-python)을 참조하세요.

<div id="additional-prerequisites-for-using-the-api-via-python">
  ### Python을 통한 API 사용을 위한 추가 사전 준비 사항
</div>

Python을 통해 Inference API를 사용하려면 먼저 일반 사전 준비 사항을 완료하세요. 그런 다음 로컬 환경에 `openai` 및 `weave` 라이브러리를 설치하세요.

```bash
pip install openai weave
```

<Note>
  `weave` 라이브러리는 Weave를 사용해 LLM 애플리케이션을 트레이싱하는 경우에만 필요합니다. Weave를 시작하는 방법은 [Weave 퀵스타트](/ko/weave/quickstart)를 참조하세요.

  Weave와 함께 W&amp;B Inference 서비스를 사용하는 방법을 보여 주는 사용 예시는 [API 사용 예시](#usage-examples)를 참조하세요.
</Note>

<div id="api-specification">
  ## API 사양
</div>

다음 섹션에서는 API 사양과 API 사용 예제를 제공합니다.

* [엔드포인트](#endpoint)
* [사용 가능한 메서드](#available-methods)
* [사용 예제](#usage-examples)

<div id="endpoint">
  ### 엔드포인트
</div>

Inference 서비스에는 다음 엔드포인트를 통해 접근할 수 있습니다.

```plaintext
https://api.inference.wandb.ai/v1
```

<Warning>
  이 엔드포인트에 액세스하려면 Inference 서비스 크레딧이 할당된 W&amp;B 계정, 유효한 W&amp;B API 키, 그리고 W&amp;B entity(“team”이라고도 함)와 프로젝트가 필요합니다. 이 가이드의 코드 예제에서는 entity(team)와 프로젝트를 `<your-team>\<your-project>`로 표기합니다.
</Warning>

<div id="available-methods">
  ### 사용 가능한 메서드
</div>

Inference 서비스는 다음 API 메서드를 지원합니다.

* [Chat completions](#chat-completions)
* [지원되는 모델 목록](#list-supported-models)

<div id="chat-completions">
  #### Chat completions
</div>

제공되는 기본 API 메서드는 `/chat/completions`이며, 지원되는 모델에 메시지를 보내고 completion을 받기 위한 OpenAI 호환 요청 형식을 지원합니다. Weave와 함께 W&amp;B Inference 서비스를 사용하는 방법에 대한 예시는 [API 사용 예제](#usage-examples)를 참고하세요.

chat completion을 생성하려면 다음이 필요합니다:

* Inference 서비스 기본 URL `https://api.inference.wandb.ai/v1`
* W&amp;B API 키 `<your-api-key>`
* W&amp;B entity 및 프로젝트 이름 `<your-team>/<your-project>`
* 사용하려는 모델의 ID (다음 중 하나):
  * `meta-llama/Llama-3.1-8B-Instruct`
  * `deepseek-ai/DeepSeek-V3-0324`
  * `meta-llama/Llama-3.3-70B-Instruct`
  * `deepseek-ai/DeepSeek-R1-0528`
  * `meta-llama/Llama-4-Scout-17B-16E-Instruct`
  * `microsoft/Phi-4-mini-instruct`

<Tabs>
  <Tab title="Bash">
    ```bash
    curl https://api.inference.wandb.ai/v1/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer <your-api-key>" \
      -H "OpenAI-Project: <your-team>/<your-project>" \
      -d '{
        "model": "<model-id>",
        "messages": [
          { "role": "system", "content": "You are a helpful assistant." },
          { "role": "user", "content": "Tell me a joke." }
        ]
      }'
    ```
  </Tab>

  <Tab title="Python">
    ```python lines
    import openai

    client = openai.OpenAI(
        # 사용자 정의 base URL은 W&B Inference를 가리킵니다
        base_url='https://api.inference.wandb.ai/v1',

        # https://wandb.ai/settings 에서 API 키를 생성하세요
        # 보안을 위해 환경 변수 OPENAI_API_KEY로 설정하는 것을 고려하세요
        api_key="<your-api-key>",

        # 사용량 추적을 위해 팀과 프로젝트가 필요합니다
        project="<your-team>/<your-project>",
    )

    # <model-id>를 다음 값 중 하나로 교체하세요:
    # meta-llama/Llama-3.1-8B-Instruct
    # deepseek-ai/DeepSeek-V3-0324
    # meta-llama/Llama-3.3-70B-Instruct
    # deepseek-ai/DeepSeek-R1-0528
    # meta-llama/Llama-4-Scout-17B-16E-Instruct
    # microsoft/Phi-4-mini-instruct

    response = client.chat.completions.create(
        model="<model-id>",
        messages=[
            {"role": "system", "content": "<your-system-prompt>"},
            {"role": "user", "content": "<your-prompt>"}
        ],
    )

    print(response.choices[0].message.content)
    ```
  </Tab>
</Tabs>

<div id="list-supported-models">
  #### 지원되는 모델 목록 조회
</div>

API를 사용해 현재 사용 가능한 모든 모델과 해당 ID를 조회할 수 있습니다. 이는 환경에서 어떤 모델을 사용할 수 있는지 확인하거나 모델을 동적으로 선택할 때 유용합니다.

<Tabs>
  <Tab title="Bash">
    ```bash
    curl https://api.inference.wandb.ai/v1/models \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer <your-api-key>" \
      -H "OpenAI-Project: <your-team>/<your-project>" \
    ```
  </Tab>

  <Tab title="Python">
    ```python lines
    import openai

    client = openai.OpenAI(
        base_url="https://api.inference.wandb.ai/v1",
        api_key="<your-api-key>",
        project="<your-team>/<your-project>"
    )

    response = client.models.list()

    for model in response.data:
        print(model.id)
    ```
  </Tab>
</Tabs>

<div id="usage-examples">
  ## 사용 예시
</div>

이 섹션에서는 Weave와 함께 W&amp;B Inference를 사용하는 방법을 여러 가지 예제로 보여줍니다:

* [기본 예시: Weave로 Llama 3.1 8B를 트레이스하기](#basic-example-trace-llama-31-8b-with-weave)
* [고급 예시: Inference 서비스와 함께 Weave Evaluations 및 Leaderboards를 사용하기](#advanced-example-use-weave-evaluations-and-leaderboards-with-the-inference-service)

<div id="basic-example-trace-llama-31-8b-with-weave">
  ### 기본 예시: Weave로 Llama 3.1 8B 트레이싱하기
</div>

다음 Python 코드 예시는 W&amp;B Inference API를 사용해 **Llama 3.1 8B** 모델에 프롬프트를 보내고, Weave에서 해당 호출을 트레이싱하는 방법을 보여줍니다. 트레이싱을 사용하면 LLM 호출의 전체 입력/출력을 캡처하고, 성능을 모니터링하며, Weave UI에서 결과를 분석할 수 있습니다.

<Tip>
  [Weave에서 트레이싱하는 방법](../tracking/tracing.mdx)에 대해 더 알아보세요.
</Tip>

이 예시에서는:

* OpenAI 호환 클라이언트를 사용해 chat completion 요청을 보내는 `run_chat` 함수를 정의하고, 이 함수에 `@weave.op()` 데코레이터를 적용합니다.
* 트레이스는 W&amp;B entity와 프로젝트 `project="<your-team>/<your-project>`에 연결되어 기록됩니다.
* 이 함수는 Weave에 의해 자동으로 트레이싱되므로, 입력, 출력, 지연 시간, 모델 ID와 같은 메타데이터가 로그로 남습니다.
* 결과는 터미널에 출력되며, 지정한 프로젝트의 [https://wandb.ai](https://wandb.ai) 내 **Traces** 탭에서 해당 트레이스를 확인할 수 있습니다.

이 예시를 사용하려면 [일반 사전 준비 사항](#prerequisites)과 [Python을 통해 API를 사용할 때의 추가 사전 준비 사항](#additional-prerequisites-for-using-the-api-via-python)을 모두 완료해야 합니다.

```python lines
import weave
import openai

# 트레이싱을 위한 Weave 팀 및 프로젝트 설정
weave.init("<your-team>/<your-project>")

client = openai.OpenAI(
    base_url='https://api.inference.wandb.ai/v1',

    # https://wandb.ai/settings 에서 API 키 생성
    api_key="<your-api-key>",

    # W&B 추론 사용량 추적에 필요
    project="wandb/inference-demo",
)

# Weave에서 모델 호출 트레이싱
@weave.op()
def run_chat():
    response = client.chat.completions.create(
        model="meta-llama/Llama-3.1-8B-Instruct",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Tell me a joke."}
        ],
    )
    return response.choices[0].message.content

# 트레이싱된 호출 실행 및 로그 기록
output = run_chat()
print(output)
```

코드 예제를 실행하면 터미널에 출력되는 링크를 클릭하여 Weave에서 트레이스를 볼 수 있습니다(예: `https://wandb.ai/<your-team>/<your-project>/r/call/01977f8f-839d-7dda-b0c2-27292ef0e04g`). 또는 다음과 같이 진행합니다:

1. [https://wandb.ai](https://wandb.ai)로 이동합니다.
2. **Traces** 탭을 선택하여 Weave 트레이스를 확인합니다.

이제 [고급 예제](#advanced-example-use-weave-evaluations-and-leaderboards-with-the-inference-service)를 시도해 보세요.

<Frame>
  ![Traces 화면](/weave/guides/integrations/imgs/image.png)
</Frame>

<div id="advanced-example-use-weave-evaluations-and-leaderboards-with-the-inference-service">
  ### 고급 예시: Inference 서비스에서 Weave Evaluations 및 Leaderboards 사용하기
</div>

Inference 서비스와 함께 Weave를 사용해 [모델 호출을 추적](../tracking/tracing.mdx)할 뿐만 아니라, [성능을 평가](../core-types/evaluations.mdx)하고 [리더보드를 게시](../core-types/leaderboards.mdx)할 수도 있습니다. 아래 Python 코드 예시는 간단한 질문–답변 데이터셋에서 두 개의 모델을 비교합니다.

이 예시를 사용하려면 먼저 [일반 필수 조건](#prerequisites)과 [Python을 통한 API 사용을 위한 추가 필수 조건](#additional-prerequisites-for-using-the-api-via-python)을 모두 완료해야 합니다.

```python lines
import os
import asyncio
import openai
import weave
from weave.flow import leaderboard
from weave.trace.ref_util import get_ref

# 트레이싱을 위한 Weave 팀 및 프로젝트 설정
weave.init("<your-team>/<your-project>")

dataset = [
    {"input": "What is 2 + 2?", "target": "4"},
    {"input": "Name a primary color.", "target": "red"},
]

@weave.op
def exact_match(target: str, output: str) -> float:
    return float(target.strip().lower() == output.strip().lower())

class WBInferenceModel(weave.Model):
    model: str

    @weave.op
    def predict(self, prompt: str) -> str:
        client = openai.OpenAI(
            base_url="https://api.inference.wandb.ai/v1",
            # https://wandb.ai/settings 에서 API 키 생성
            api_key="<your-api-key>",
            # W&B inference 사용량 추적에 필요
            project="<your-team>/<your-project>",
        )
        resp = client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
        )
        return resp.choices[0].message.content

llama = WBInferenceModel(model="meta-llama/Llama-3.1-8B-Instruct")
deepseek = WBInferenceModel(model="deepseek-ai/DeepSeek-V3-0324")

def preprocess_model_input(example):
    return {"prompt": example["input"]}

evaluation = weave.Evaluation(
    name="QA",
    dataset=dataset,
    scorers=[exact_match],
    preprocess_model_input=preprocess_model_input,
)

async def run_eval():
    await evaluation.evaluate(llama)
    await evaluation.evaluate(deepseek)

asyncio.run(run_eval())

spec = leaderboard.Leaderboard(
    name="Inference Leaderboard",
    description="Compare models on a QA dataset",
    columns=[
        leaderboard.LeaderboardColumn(
            evaluation_object_ref=get_ref(evaluation).uri(),
            scorer_name="exact_match",
            summary_metric_path="mean",
        )
    ],
)

weave.publish(spec)
```

다음 코드 예제를 실행한 후 [https://wandb.ai/](https://wandb.ai/)에서 W&amp;B 계정에 접속한 뒤 다음을 수행하세요.

* **Traces** 탭으로 이동하여 [트레이스를 확인](../tracking/tracing.mdx)합니다.
* **Evals** 탭으로 이동하여 [모델 평가 결과를 확인](../core-types/evaluations.mdx)합니다.
* **Leaders** 탭으로 이동하여 [생성된 리더보드를 확인](../core-types/leaderboards.mdx)합니다.

<Frame>
  ![모델 평가 결과 보기](/weave/guides/integrations/imgs/inference-advanced-evals.png)
</Frame>

<Frame>
  ![트레이스 보기](/weave/guides/integrations/imgs/inference-advanced-leaderboard.png)
</Frame>

<div id="ui">
  ## UI
</div>

다음 섹션에서는 W&amp;B UI를 통해 Inference 서비스를 사용하는 방법을 설명합니다. UI에서 Inference 서비스에 접근하기 전에 [사전 요구 사항](#prerequisites)을 완료해야 합니다.

<div id="access-the-inference-service">
  ### Inference 서비스에 접근하기
</div>

Weave UI에서 다음 두 위치에서 Inference 서비스에 접근할 수 있습니다.

* [직접 링크](#direct-link)
* [Inference 탭에서](#from-the-inference-tab)
* [Playground 탭에서](#from-the-playground-tab)

<div id="direct-link">
  #### 직접 링크
</div>

[https://wandb.ai/inference](https://wandb.ai/inference)로 이동하세요.

<div id="from-the-inference-tab">
  #### Inference 탭에서
</div>

1. [https://wandb.ai/](https://wandb.ai/)에서 W&amp;B 계정 페이지로 이동합니다.
2. 왼쪽 사이드바에서 **Inference**를 선택합니다. 사용 가능한 모델과 해당 모델 정보가 표시된 페이지가 열립니다.

<Frame>
  ![The Inference tab](/weave/guides/integrations/imgs/inference-ui.png)
</Frame>

<div id="from-the-playground-tab">
  #### Playground 탭에서
</div>

1. 왼쪽 사이드바에서 **Playground**를 선택합니다. Playground 채팅 UI가 표시됩니다.
2. LLM 드롭다운 목록에서 **W&amp;B Inference** 위에 마우스를 올립니다. 사용 가능한 W&amp;B Inference 모델이 오른쪽에 드롭다운으로 표시됩니다.
3. W&amp;B Inference 모델 드롭다운에서 다음을 수행할 수 있습니다.
   * 사용 가능한 아무 모델 이름이나 클릭하여 [Playground에서 사용해 보기](#try-a-model-in-the-playground)를 진행합니다.
   * [Playground에서 하나 이상의 모델 비교하기](#compare-multiple-models)를 진행합니다.

<Frame>
  ![Playground의 Inference 모델 드롭다운](/weave/guides/integrations/imgs/inference-playground.png)
</Frame>

<div id="try-a-model-in-the-playground">
  ### Playground에서 모델 테스트하기
</div>

[액세스 옵션 중 하나를 사용해 모델을 선택](#access-the-inference-service)한 후에는 Playground에서 해당 모델을 테스트할 수 있습니다. 다음 작업을 할 수 있습니다:

* [모델 설정과 파라미터 맞춤 설정](../tools/playground#customize-settings)
* [메시지 추가, 재시도, 편집 및 삭제](../tools/playground#message-controls)
* [사용자 정의 설정이 적용된 모델 저장 및 재사용](../tools/playground#saved-models)
* [여러 모델 비교](#compare-multiple-models)

<Frame>
  ![Playground에서 Inference 모델 사용하기](/weave/guides/integrations/imgs/inference-playground-single.png)
</Frame>

<div id="compare-multiple-models">
  ### 여러 모델 비교하기
</div>

Playground에서 여러 Inference 모델을 비교할 수 있습니다. Compare 뷰는 다음 두 곳에서 이동할 수 있습니다:

* [Inference 탭에서 Compare 뷰로 이동](#access-the-compare-view-from-the-inference-tab)
* [Playground 탭에서 Compare 뷰로 이동](#access-the-compare-view-from-the-playground-tab)

<div id="access-the-compare-view-from-the-inference-tab">
  #### Inference 탭에서 Compare 뷰 열기
</div>

1. 왼쪽 사이드바에서 **Inference**를 선택합니다. 사용 가능한 모델과 모델 정보가 표시된 페이지가 열립니다.
2. 비교할 모델을 선택하려면 모델 이름을 제외한 모델 카드의 아무 곳이나 클릭합니다. 선택된 모델 카드의 테두리가 파란색으로 강조 표시됩니다.
3. 비교하려는 각 모델에 대해 2단계를 반복합니다.
4. 선택된 카드 중 하나에서 **Compare N models in the Playground** 버튼을 클릭합니다(`N`은 비교 중인 모델의 개수입니다. 예를 들어, 모델 3개를 선택한 경우 버튼에는 **Compare 3 models in the Playground**가 표시됩니다). 그러면 비교 뷰가 열립니다.

이제 Playground에서 모델을 비교하고, [Try a model in the Playground](#try-a-model-in-the-playground)에 설명된 모든 기능을 사용할 수 있습니다.

<Frame>
  ![Playground에서 비교할 여러 모델 선택](/weave/guides/integrations/imgs/inference-playground-compare.png)
</Frame>

<div id="access-the-compare-view-from-the-playground-tab">
  #### Playground 탭에서 Compare 뷰로 이동하기
</div>

1. 왼쪽 사이드바에서 **Playground**를 선택합니다. Playground 채팅 UI가 표시됩니다.
2. LLM 드롭다운 목록에서 **W&amp;B Inference** 위에 마우스를 올립니다. 사용 가능한 W&amp;B Inference 모델이 오른쪽에 드롭다운으로 표시됩니다.
3. 드롭다운에서 **Compare**를 선택합니다. **Inference** 탭이 표시됩니다.
4. 비교할 모델을 선택하려면 모델 이름을 제외한 모델 카드의 아무 곳이나 클릭합니다. 선택되었음을 나타내기 위해 모델 카드의 테두리가 파란색으로 강조 표시됩니다.
5. 비교하려는 각 모델에 대해 4단계를 반복합니다.
6. 선택된 카드 중 아무 카드에서나 **Compare N models in the Playground** 버튼을 클릭합니다 (`N`은 비교 중인 모델의 개수입니다. 예를 들어, 모델 3개가 선택된 경우 버튼에는 **Compare 3 models in the Playground**로 표시됩니다). 비교 뷰가 열립니다.

이제 Playground에서 모델을 비교할 수 있으며, [Try a model in the Playground](#try-a-model-in-the-playground)에 설명된 모든 기능을 사용할 수 있습니다.

<div id="view-billing-and-usage-information">
  ### 결제 및 사용량 정보 확인
</div>

조직 관리자는 W&amp;B UI에서 현재 Inference 크레딧 잔액, 사용 이력, 그리고 예정된 결제(해당되는 경우)를 직접 확인할 수 있습니다.

1. W&amp;B UI에서 W&amp;B **Billing** 페이지로 이동합니다.
2. 오른쪽 하단에 Inference 결제 정보 카드가 표시됩니다. 여기에서 다음 작업을 수행할 수 있습니다.

* Inference 결제 정보 카드에서 **View usage** 버튼을 클릭해 시간에 따른 사용량을 확인합니다.
* 유료 플랜을 사용하는 경우, 예정된 Inference 요금을 확인합니다.

<Tip>
  [Inference 가격 페이지에서 모델별 요금 체계를 확인하세요](https://wandb.ai/site/pricing/inference)
</Tip>

<div id="usage-information-and-limits">
  ## 사용 정보 및 제한 사항
</div>

다음 섹션에서는 중요한 사용 정보와 제한 사항을 설명합니다. 서비스를 사용하기 전에 이 정보를 숙지하세요.

<div id="geographic-restrictions">
  ### 지역 제한
</div>

Inference 서비스는 지원되는 지역에서만 이용할 수 있습니다. 자세한 내용은 [이용 약관](https://docs.coreweave.com/docs/policies/terms-of-service/terms-of-use#geographic-restrictions)을 참조하세요.

<div id="concurrency-limits">
  ### 동시성 한도
</div>

공정한 사용과 안정적인 성능을 보장하기 위해 W&amp;B Inference API는 사용자 및 프로젝트 수준에서 요청 속도 제한(레이트 리밋)을 적용합니다. 이러한 제한은 다음과 같은 데 도움이 됩니다.

* 오남용을 방지하고 API 안정성을 보호
* 모든 사용자에 대한 접근 권한 보장
* 인프라 부하를 효과적으로 관리

요청 속도 제한을 초과하면 API는 `429 Concurrency limit reached for requests` 응답을 반환합니다. 이 오류를 해결하려면 동시에 보내는 요청 수를 줄이십시오.

<div id="pricing">
  ### 가격
</div>

모델 가격 정보는 [https://wandb.ai/site/pricing/inference](https://wandb.ai/site/pricing/inference) 페이지를 방문하세요.

<div id="api-errors">
  ## API 오류
</div>

| Error Code | Message                                                                     | Cause                                           | Solution                                                                               |
| ---------- | --------------------------------------------------------------------------- | ----------------------------------------------- | -------------------------------------------------------------------------------------- |
| 401        | Invalid Authentication                                                      | 인증 자격 증명이 유효하지 않거나 W&amp;B 프로젝트 entity 및/또는 이름이 올바르지 않습니다.              | 올바른 API 키를 사용하고 있는지, 그리고 W&amp;B 프로젝트 이름과 entity가 정확한지 확인하세요.                                              |
| 403        | Country, region, or territory not supported                                 | 지원되지 않는 지역에서 API에 액세스했습니다. | [Geographic restrictions](#geographic-restrictions)를 참조하세요.                                       |
| 429        | Concurrency limit reached for requests                                      | 동시에 처리되는 요청이 너무 많습니다.                   | 동시 요청 수를 줄이세요.               |
| 429        | You exceeded your current quota, please check your plan and billing details | 크레딧이 소진되었거나 월간 사용 한도에 도달했습니다. | 추가 크레딧을 구매하거나 한도를 늘리세요.                       |
| 500        | The server had an error while processing your request                       | 내부 서버 오류입니다.                          | 잠시 후 다시 시도하고, 문제가 지속되면 지원팀에 문의하세요. |
| 503        | The engine is currently overloaded, please try again later                  | 서버에 트래픽이 과도하게 몰리고 있습니다.            | 잠시 기다린 후 요청을 다시 시도하세요.                                                |