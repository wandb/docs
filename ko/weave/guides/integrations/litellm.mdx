---
title: "LiteLLM"
description: "LiteLLM을 통해 이루어지는 LLM 호출을 자동으로 추적하고 기록합니다"
---

<a target="_blank" href="https://colab.research.google.com/github/wandb/examples/blob/master/weave/docs/quickstart_litellm.ipynb" aria-label="Google Colab에서 열기">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab에서 열기"/>
</a>

Weave는 `weave.init()`을 호출한 이후 LiteLLM을 통해 이루어지는 LLM 호출을 자동으로 추적하고 기록합니다.

<div id="traces">
  ## 트레이스
</div>

LLM 애플리케이션의 트레이스를 개발 중과 프로덕션 환경 모두에서 중앙 데이터베이스에 저장하는 것은 중요합니다. 이 트레이스는 디버깅에 사용하고, 애플리케이션을 개선하는 데 도움을 주는 데이터셋으로 활용하게 됩니다.

> **Note:** LiteLLM을 사용할 때는 `from litellm import completion` 대신 `import litellm`으로 라이브러리를 임포트하고, `litellm.completion`으로 completion 함수를 호출해야 합니다. 이렇게 해야 모든 함수와 파라미터가 올바르게 참조됩니다.

Weave는 LiteLLM에 대한 트레이스를 자동으로 기록합니다. 평소처럼 라이브러리를 사용하면 되며, 먼저 `weave.init()`을 호출하세요:

```python lines {4}
import litellm
import weave

weave.init("weave_litellm_integration")

openai_response = litellm.completion(
    model="gpt-3.5-turbo", 
    messages=[{"role": "user", "content": "Translate 'Hello, how are you?' to French"}],
    max_tokens=1024
)
print(openai_response.choices[0].message.content)

claude_response = litellm.completion(
    model="claude-3-5-sonnet-20240620", 
    messages=[{"role": "user", "content": "Translate 'Hello, how are you?' to French"}],
    max_tokens=1024
)
print(claude_response.choices[0].message.content)
```

이제 Weave는 LiteLLM을 통해 이루어지는 모든 LLM 호출을 추적하고 로그로 기록합니다. Weave 웹 인터페이스에서 트레이스를 확인할 수 있습니다.


<div id="wrapping-with-your-own-ops">
  ## 사용자 정의 op으로 래핑하기
</div>

Weave op은 실험하는 동안 코드를 자동으로 버전 관리하고, 입력과 출력을 캡처하여 결과를 재현 가능하게 만듭니다. LiteLLM의 completion 함수를 호출하는 함수를 `@weave.op()` 데코레이터로 정의하기만 하면 Weave가 입력과 출력을 추적해 줍니다. 예시는 다음과 같습니다:

```python lines {4,6}
import litellm
import weave

weave.init("weave_litellm_integration")

@weave.op()
def translate(text: str, target_language: str, model: str) -> str:
    response = litellm.completion(
        model=model,
        messages=[{"role": "user", "content": f"Translate '{text}' to {target_language}"}],
        max_tokens=1024
    )
    return response.choices[0].message.content

print(translate("Hello, how are you?", "French", "gpt-3.5-turbo"))
print(translate("Hello, how are you?", "Spanish", "claude-3-5-sonnet-20240620"))
```


<div id="create-a-model-for-easier-experimentation">
  ## 실험을 더 쉽게 하기 위한 `Model` 생성
</div>

구성 요소가 많을수록 실험을 체계적으로 관리하기가 어렵습니다. `Model` 클래스를 사용하면 시스템 프롬프트나 사용 중인 모델처럼 앱의 실험 세부 정보를 캡처하고 정리할 수 있습니다. 이를 통해 앱의 여러 iteration을 체계적으로 관리하고 서로 비교할 수 있습니다.

코드 버저닝과 입출력 캡처뿐 아니라, Model은 애플리케이션의 동작을 제어하는 구조화된 파라미터를 캡처하여 어떤 파라미터 조합이 가장 잘 동작했는지 쉽게 찾을 수 있게 해줍니다. 또한 Weave Model을 `serve` 및 Evaluation과 함께 사용할 수도 있습니다.

아래 예시에서는 서로 다른 모델과 temperature 값을 바꿔가며 실험해 볼 수 있습니다:

```python lines {4,6,10}
import litellm
import weave

weave.init('weave_litellm_integration')

class TranslatorModel(weave.Model):
    model: str
    temperature: float
  
    @weave.op()
    def predict(self, text: str, target_language: str):
        response = litellm.completion(
            model=self.model,
            messages=[
                {"role": "system", "content": f"You are a translator. Translate the given text to {target_language}."},
                {"role": "user", "content": text}
            ],
            max_tokens=1024,
            temperature=self.temperature
        )
        return response.choices[0].message.content

# 다양한 모델로 인스턴스 생성
gpt_translator = TranslatorModel(model="gpt-3.5-turbo", temperature=0.3)
claude_translator = TranslatorModel(model="claude-3-5-sonnet-20240620", temperature=0.1)

# 번역에 다양한 모델 사용
english_text = "Hello, how are you today?"

print("GPT-3.5 프랑스어 번역:")
print(gpt_translator.predict(english_text, "French"))

print("\nClaude-3.5 Sonnet 스페인어 번역:")
print(claude_translator.predict(english_text, "Spanish"))
```


<div id="function-calling">
  ## 함수 호출(Function Calling)
</div>

LiteLLM은 호환되는 모델에서 함수 호출(Function Calling)을 지원합니다. Weave는 이러한 함수 호출을 자동으로 추적합니다.

```python lines {4}
import litellm
import weave

weave.init("weave_litellm_integration")

response = litellm.completion(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Translate 'Hello, how are you?' to French"}],
    functions=[
        {
            "name": "translate",
            "description": "Translate text to a specified language",
            "parameters": {
                "type": "object",
                "properties": {
                    "text": {
                        "type": "string",
                        "description": "The text to translate",
                    },
                    "target_language": {
                        "type": "string",
                        "description": "The language to translate to",
                    }
                },
                "required": ["text", "target_language"],
            },
        },
    ],
)

print(response)
```

프롬프트에서 사용한 함수를 자동으로 수집하여 버전으로 관리합니다.

[![litellm.gif](/weave/guides/integrations/imgs/litellm.gif)](https://wandb.ai/a-sh0ts/weave_litellm_integration/weave/calls)
