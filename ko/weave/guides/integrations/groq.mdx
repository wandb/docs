---
title: Groq
description: "Weave에서 Groq의 초고속 LPU™ 추론을 추적하고 모니터링하여, Groq의 특수 하드웨어 가속을 사용하는 고속 LLM 애플리케이션의 모델 호출, 성능 지표, 함수 체인을 캡처합니다."
---

<a target="_blank" href="https://colab.research.google.com/github/wandb/examples/blob/master/weave/docs/quickstart_groq.ipynb" aria-label="Google Colab에서 열기">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Colab에서 열기" />
</a>

<Note>
  별도의 설정 없이 Weave에서 Groq 모델을 실험해 보고 싶나요? [LLM Playground](../tools/playground)를 사용해 보세요.
</Note>

[Groq](https://groq.com/)는 고속 AI 추론을 제공하는 AI 인프라 기업입니다. Groq의 LPU™ Inference Engine은 뛰어난 연산 속도, 품질, 에너지 효율성을 제공하는 하드웨어 및 소프트웨어 플랫폼입니다. Weave는 Groq의 chat completion 호출을 자동으로 추적하고 로그로 남깁니다.

<div id="tracing">
  ## 트레이싱
</div>

언어 모델 애플리케이션의 트레이스를 개발 환경과 프로덕션 환경 모두에서 중앙화된 위치에 저장해 두는 것은 중요합니다. 이러한 트레이스는 디버깅에 유용할 뿐만 아니라, 애플리케이션을 개선하는 데 도움이 되는 데이터셋으로도 활용할 수 있습니다.

Weave는 [Groq](https://groq.com/)에 대한 트레이스를 자동으로 수집합니다. 추적을 시작하려면 `weave.init(project_name="<YOUR-WANDB-PROJECT-NAME>")`를 호출한 뒤 평소처럼 라이브러리를 사용하면 됩니다.

```python lines
import os
import weave
from groq import Groq

weave.init(project_name="groq-project")

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Explain the importance of fast language models",
        }
    ],
    model="llama3-8b-8192",
)
```

| ![Groq Weave 대시보드 화면으로, 추적된 LLM 호출, 성능 메트릭 및 트레이스 정보가 표시되어 있습니다](/weave/guides/integrations/imgs/groq/groq_weave_dasboard.png) |
| ------------------------------------------------------------------------------------------------------------------------------ |
| 이제 Weave가 Groq 라이브러리를 통해 이루어지는 모든 LLM 호출을 추적하고 로그로 남깁니다. Weave 웹 인터페이스에서 이러한 트레이스를 확인할 수 있습니다.                                 |

<div id="track-your-own-ops">
  ## 나만의 op 추적하기
</div>

함수에 `@weave.op` 데코레이터를 추가하면 입력, 출력, 그리고 앱 로직이 캡처되어 데이터가 앱 안에서 어떻게 흐르는지 디버깅할 수 있습니다. op를 깊이 중첩해 추적하려는 함수들로 트리를 구성할 수 있습니다. 이 과정에서 실험하는 동안 git에 커밋되지 않은 즉석 변경 사항까지 포착할 수 있도록 코드를 자동으로 버전 관리하기도 합니다.

[`@weave.op`](/ko/weave/guides/tracking/ops) 데코레이터를 사용해 함수를 하나 만들기만 하면 됩니다.

아래 예시에서는 `recommend_places_to_visit`라는 함수가 있으며, 이는 `@weave.op`으로 감싼 함수로, 특정 도시에서 방문할 만한 장소를 추천합니다.

```python lines
import os
import weave
from groq import Groq


weave.init(project_name="groq-test")

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

@weave.op()
def recommend_places_to_visit(city: str, model: str="llama3-8b-8192"):
    chat_completion = client.chat.completions.create(
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant meant to suggest places to visit in a city",
            },
            {
                "role": "user",
                "content": city,
            }
        ],
        model="llama3-8b-8192",
    )
    return chat_completion.choices[0].message.content


recommend_places_to_visit("New York")
recommend_places_to_visit("Paris")
recommend_places_to_visit("Kolkata")
```

| ![op 데코레이터와 함수 호출 계층 구조, 트레이스 세부 정보를 보여주는 Groq Weave 트레이싱 인터페이스](/weave/guides/integrations/imgs/groq/groq_weave_tracing.png) |
| ----------------------------------------------------------------------------------------------------------------------------- |
| `recommend_places_to_visit` 함수에 `@weave.op` 데코레이터를 적용하면 이 함수의 입력, 출력, 그리고 함수 내부에서 수행되는 모든 LM 호출이 트레이싱됩니다.                     |

<div id="create-a-model-for-easier-experimentation">
  ## 더 쉬운 실험을 위한 `Model` 생성
</div>

여러 요소가 동시에 변하는 상황에서는 실험을 체계적으로 정리하기가 어렵습니다. [`Model`](../core-types/models) 클래스를 사용하면 시스템 프롬프트나 사용 중인 모델처럼 앱의 실험 세부 정보를 캡처하고 정리할 수 있습니다. 이를 통해 앱의 다양한 반복 버전을 정리하고 비교하기가 쉬워집니다.

코드 버저닝과 입력/출력 캡처에 더해, [`Model`](../core-types/models)은 애플리케이션 동작을 제어하는 구조화된 파라미터를 캡처하므로, 어떤 파라미터 조합이 가장 잘 동작했는지 쉽게 찾을 수 있습니다. 또한 Weave Model을 `serve` 및 [`Evaluation`](../core-types/evaluations)과 함께 사용할 수 있습니다.

아래 예시에서는 `GroqCityVisitRecommender`로 실험을 진행합니다. 이 중 하나를 변경할 때마다 `GroqCityVisitRecommender`의 새로운 *버전*이 생성됩니다.

```python lines
import os
from groq import Groq
import weave


class GroqCityVisitRecommender(weave.Model):
    model: str
    groq_client: Groq

    @weave.op()
    def predict(self, city: str) -> str:
        system_message = {
            "role": "system",
            "content": """
당신은 도시에서 방문할 장소를 추천해 주는 유용한 어시스턴트입니다
""",
        }
        user_message = {"role": "user", "content": city}
        chat_completion = self.groq_client.chat.completions.create(
            messages=[system_message, user_message],
            model=self.model,
        )
        return chat_completion.choices[0].message.content


weave.init(project_name="groq-test")
city_recommender = GroqCityVisitRecommender(
    model="llama3-8b-8192", groq_client=Groq(api_key=os.environ.get("GROQ_API_KEY"))
)
print(city_recommender.predict("New York"))
print(city_recommender.predict("San Francisco"))
print(city_recommender.predict("Los Angeles"))
```

| ![모델 버전, 추적 기록, 성능 지표를 포함한 Groq Weave Model 추적 및 버전 관리 인터페이스](/weave/guides/integrations/imgs/groq/groq_weave_model.png) |
| ------------------------------------------------------------------------------------------------------------------------ |
| [`Model`](../core-types/models)을 사용해 호출을 추적하고 버전 관리하기                                                                    |

<div id="serving-a-weave-model">
  ### Weave 모델 서빙하기
</div>

어떤 `weave.Model` 객체에 대한 weave reference가 있다면, FastAPI 서버를 실행해 [serve](https://docs.wandb.ai/weave/guides/tools/serve) 할 수 있습니다.

| [![dspy\_weave\_model\_serve.png](/weave/guides/integrations/imgs/groq/groq_weave_model_version.png)](https://wandb.ai/geekyrakshit/groq-test/weave/objects/GroqCityVisitRecommender/versions/6O1xPTJ9yFx8uuCjJAlI7KgcVYxXKn7JxfmVD9AQT5Q) |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| UI에서 해당 모델로 이동한 뒤, 거기서 복사하면 어떤 WeaveModel이든 weave reference를 확인할 수 있습니다.                                                                                                                                                                   |

터미널에서 다음 명령어를 사용해 모델을 서빙할 수 있습니다:

```shell
weave serve weave://your_entity/project-name/YourModel:<hash>
```
