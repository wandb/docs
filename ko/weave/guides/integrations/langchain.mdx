---
title: "LangChain"
description: "Weave를 사용하여 LangChain Python 라이브러리를 통해 이루어지는 모든 호출을 추적하고 로깅합니다"
---

<a target="_blank" href="https://colab.research.google.com/github/wandb/examples/blob/master/weave/docs/quickstart_langchain.ipynb" aria-label="Open in Google Colab">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
</a>

Weave는 [LangChain Python 라이브러리](https://github.com/langchain-ai/langchain)를 통해 이루어지는 모든 호출을 손쉽게 추적하고 로깅할 수 있도록 설계되었습니다.

LLM을 다룰 때 디버깅은 불가피합니다. 모델 호출이 실패하거나 출력 형식이 잘못되었거나, 중첩된 모델 호출로 인해 혼란이 생기는 등 문제의 원인을 정확히 찾기 어려운 경우가 많습니다. LangChain 애플리케이션은 여러 단계와 LLM 호출로 구성되는 경우가 많기 때문에, 체인과 에이전트의 내부 동작을 이해하는 것이 중요합니다.

Weave는 [LangChain](https://docs.langchain.com/oss/python/langchain/overview) 애플리케이션에 대한 트레이스(trace)를 자동으로 수집해 이 과정을 단순화합니다. 이를 통해 애플리케이션의 성능을 모니터링하고 분석할 수 있으므로, LLM 워크플로우를 더 쉽게 디버깅하고 최적화할 수 있습니다.

<div id="getting-started">
  ## 시작하기
</div>

시작하려면 스크립트의 맨 앞에서 `weave.init()`을 호출하세요. `weave.init()`의 인수는 트레이스를 정리하는 데 도움이 되는 프로젝트 이름입니다.

```python lines {6}
import weave
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# 프로젝트 이름으로 Weave 초기화
weave.init("langchain_demo")

llm = ChatOpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

llm_chain = prompt | llm

output = llm_chain.invoke({"number": 2})

print(output)
```

<div id="tracking-call-metadata">
  ## 호출 메타데이터 추적
</div>

LangChain 호출의 메타데이터를 추적하려면 [`weave.attributes`](/ko/weave/reference/python-sdk#function-attributes) 컨텍스트 매니저를 사용할 수 있습니다. 이 컨텍스트 매니저를 사용하면 체인 또는 단일 요청과 같은 특정 코드 블록에 대해 사용자 지정 메타데이터를 설정할 수 있습니다.

```python lines {6,13}
import weave
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# 프로젝트 이름으로 Weave 초기화
weave.init("langchain_demo")

llm = ChatOpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

llm_chain = prompt | llm

with weave.attributes({"my_awesome_attribute": "value"}):
    output = llm_chain.invoke()

print(output)
```

Weave은 LangChain 호출 trace와 연결된 메타데이터를 자동으로 기록합니다. 아래와 같이 Weave 웹 인터페이스에서 메타데이터를 확인할 수 있습니다.

[![langchain\_attributes.png](/weave/guides/integrations/imgs/langchain_attributes.png)](https://wandb.ai/parambharat/langchain_demo/weave/traces?cols=%7B%22attributes.weave.client_version%22%3Afalse%2C%22attributes.weave.os_name%22%3Afalse%2C%22attributes.weave.os_release%22%3Afalse%2C%22attributes.weave.os_version%22%3Afalse%2C%22attributes.weave.source%22%3Afalse%2C%22attributes.weave.sys_version%22%3Afalse%7D)

<div id="traces">
  ## 트레이스
</div>

LLM 애플리케이션의 트레이스를 중앙 데이터베이스에 저장하는 것은 개발 및 프로덕션 환경 모두에서 중요합니다. 이러한 트레이스는 애플리케이션을 디버깅하고 개선하는 데 필요한, 가치 있는 데이터셋을 제공합니다.

Weave는 LangChain 애플리케이션의 트레이스를 자동으로 캡처합니다. LangChain 라이브러리를 통해 발생하는 프롬프트 템플릿, 체인, LLM 호출, 도구, 에이전트 스텝 등의 모든 호출을 추적하고 로그로 남깁니다. Weave 웹 인터페이스에서 이러한 트레이스를 확인할 수 있습니다.

[![langchain\_trace.png](/weave/guides/integrations/imgs/langchain_trace.png)](https://wandb.ai/parambharat/langchain_demo/weave/calls)

<div id="manually-tracing-calls">
  ## 호출 수동 추적
</div>

자동 추적 외에도 `WeaveTracer` 콜백이나 `weave_tracing_enabled` 컨텍스트 매니저를 사용해 호출을 수동으로 추적할 수 있습니다. 이러한 방식은 LangChain 애플리케이션의 개별 부분에서 요청 콜백을 사용하는 것과 유사합니다.

**참고:** Weave는 기본적으로 LangChain Runnable을 추적하며, 이는 `weave.init()`를 호출할 때 활성화됩니다. `weave.init()`를 호출하기 전에 환경 변수 `WEAVE_TRACE_LANGCHAIN`을 `"false"`로 설정하여 이 동작을 비활성화할 수 있습니다. 이를 통해 애플리케이션에서 특정 체인이나 개별 요청의 추적 동작을 세밀하게 제어할 수 있습니다.

<div id="using-weavetracer">
  ### `WeaveTracer` 사용하기
</div>

특정 요청을 추적하기 위해 개별 LangChain 컴포넌트에 `WeaveTracer` 콜백을 전달할 수 있습니다.

```python lines {11,13,15,22}
import os

os.environ["WEAVE_TRACE_LANGCHAIN"] = "false" # <- 전역 추적을 명시적으로 비활성화합니다.

from weave.integrations.langchain import WeaveTracer
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import weave

# 프로젝트 이름으로 Weave를 초기화합니다
weave.init("langchain_demo")  # <-- 환경 변수가 `false`로 명시적으로 설정되어 있으므로 여기서는 추적을 활성화하지 않습니다

weave_tracer = WeaveTracer()

config = {"callbacks": [weave_tracer]}

llm = ChatOpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

llm_chain = prompt | llm

output = llm_chain.invoke({"number": 2}, config=config) # <-- 이 chain 호출에 대해서만 추적을 활성화합니다.

llm_chain.invoke({"number": 4})  # <-- langchain 호출에 대한 추적은 비활성화되지만 openai 호출은 계속 추적됩니다
```

<div id="using-weave_tracing_enabled-context-manager">
  ### `weave_tracing_enabled` 컨텍스트 매니저 사용
</div>

또는 `weave_tracing_enabled` 컨텍스트 매니저를 사용하여 특정 코드 블록에서만 트레이싱을 활성화할 수 있습니다.

```python lines {11,18}
import os

os.environ["WEAVE_TRACE_LANGCHAIN"] = "false" # <- 전역 추적을 명시적으로 비활성화합니다.

from weave.integrations.langchain import weave_tracing_enabled
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import weave

# 프로젝트 이름으로 Weave를 초기화합니다
weave.init("langchain_demo")  # <-- 환경 변수가 명시적으로 `false`로 설정되어 있으므로 여기서는 추적을 활성화하지 않습니다

llm = ChatOpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

llm_chain = prompt | llm

with weave_tracing_enabled():  # <-- 이 체인 호출에 대해서만 추적을 활성화합니다.
    output = llm_chain.invoke({"number": 2})


llm_chain.invoke({"number": 4})  # <-- langchain 호출에 대해서는 추적이 활성화되지 않지만 openai 호출은 계속 추적됩니다
```

<div id="configuration">
  ## 구성
</div>

`weave.init`을 호출하면 환경 변수 `WEAVE_TRACE_LANGCHAIN`이 `"true"`로 설정되어 트레이싱이 활성화됩니다. 이렇게 하면 Weave가 LangChain 애플리케이션의 트레이스를 자동으로 캡처할 수 있습니다. 이 동작을 비활성화하려면 해당 환경 변수를 `"false"`로 설정하면 됩니다.

<div id="relation-to-langchain-callbacks">
  ## LangChain 콜백과의 연관성
</div>

<div id="auto-logging">
  ### 자동 로깅
</div>

`weave.init()`이 제공하는 자동 로깅은 LangChain 애플리케이션의 모든 컴포넌트에 생성자 콜백을 전달하는 것과 비슷합니다. 이는 프롬프트 템플릿, 체인, LLM 호출, 툴, 에이전트 단계 등을 포함한 모든 상호작용이 애플리케이션 전역에서 추적된다는 뜻입니다.

<div id="manual-logging">
  ### 수동 로깅
</div>

수동 로깅 메서드(`WeaveTracer` 및 `weave_tracing_enabled`)는 LangChain 애플리케이션의 개별 부분에서 요청 콜백을 사용하는 방식과 유사합니다. 이 메서드들은 애플리케이션의 어느 부분을 추적할지 보다 정밀하게 제어할 수 있게 해줍니다:

* **생성자 콜백:** 전체 체인 또는 컴포넌트에 적용되어, 모든 상호작용을 일관되게 로깅합니다.
* **요청 콜백:** 특정 요청에 적용되어, 개별 호출에 대한 상세한 추적을 가능하게 합니다.

Weave를 LangChain과 통합하면 LLM 애플리케이션에 대해 포괄적인 로깅과 모니터링을 수행할 수 있으며, 이를 통해 디버깅과 성능 최적화를 더 쉽게 할 수 있습니다.

자세한 내용은 [LangChain 문서](https://python.langchain.com/v0.2/docs/how_to/debugging/#tracing)를 참조하세요.

<div id="models-and-evaluations">
  ## 모델과 Evaluation
</div>

여러 가지 사용 사례에 맞춰 애플리케이션에서 LLM을 구성하고 평가하는 작업은 프롬프트, 모델 설정, 추론 파라미터와 같은 여러 구성 요소가 얽혀 있어 쉽지 않습니다. [`weave.Model`](/ko/weave/guides/core-types/models)을(를) 사용하면 시스템 프롬프트나 사용하는 모델과 같은 실험 세부 정보를 캡처하고 정리하여, 서로 다른 반복 결과를 더 쉽게 비교할 수 있습니다.

다음 예시는 LangChain 체인을 `WeaveModel`로 래핑하는 방법을 보여줍니다:

```python lines {10,12,16}
import json
import asyncio

import weave

from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# 프로젝트 이름으로 Weave 초기화
weave.init("langchain_demo")

class ExtractFruitsModel(weave.Model):
    model_name: str
    prompt_template: str

    @weave.op()
    async def predict(self, sentence: str) -> dict:
        llm = ChatOpenAI(model=self.model_name, temperature=0.0)
        prompt = PromptTemplate.from_template(self.prompt_template)

        llm_chain = prompt | llm
        response = llm_chain.invoke({"sentence": sentence})
        result = response.content

        if result is None:
            raise ValueError("No response from model")
        parsed = json.loads(result)
        return parsed

model = ExtractFruitsModel(
    model_name="gpt-3.5-turbo-1106",
    prompt_template='Extract fields ("fruit": <str>, "color": <str>, "flavor": <str>) from the following text, as json: {sentence}',
)
sentence = "There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy."

prediction = asyncio.run(model.predict(sentence))

# Jupyter Notebook에서 실행하는 경우:
# prediction = await model.predict(sentence)

print(prediction)
```

이 코드는 Weave UI에서 시각화할 수 있는 모델을 만듭니다:

[![langchain\_model.png](/weave/guides/integrations/imgs/langchain_model.png)](https://wandb.ai/parambharat/langchain_demo/weave/object-versions?filter=%7B%22baseObjectClass%22%3A%22Model%22%7D\&peekPath=%2Fparambharat%2Flangchain_demo%2Fobjects%2FExtractFruitsModel%2Fversions%2FBeoL6WuCH8wgjy6HfmuBMyKzArETg1oAFpYaXZSq1hw%3F%26)

또한 Weave 모델은 `serve` 및 [`Evaluations`](/ko/weave/guides/core-types/evaluations)와 함께 사용할 수도 있습니다.

<div id="evaluations">
  ### Evaluations
</div>

Evaluations를 활용하면 모델의 성능을 측정할 수 있습니다. [`weave.Evaluation`](/ko/weave/guides/core-types/evaluations) 클래스를 사용하면 모델이 특정 작업이나 데이터셋에서 얼마나 잘 동작하는지를 기록하여, 서로 다른 모델과 애플리케이션의 여러 버전을 보다 쉽게 비교할 수 있습니다. 아래 예시는 앞에서 만든 모델을 어떻게 평가하는지 보여줍니다:

```python lines

from weave.scorers import MultiTaskBinaryClassificationF1

sentences = [
    "There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.",
    "Pounits are a bright green color and are more savory than sweet.",
    "Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.",
]
labels = [
    {"fruit": "neoskizzles", "color": "purple", "flavor": "candy"},
    {"fruit": "pounits", "color": "bright green", "flavor": "savory"},
    {"fruit": "glowls", "color": "pale orange", "flavor": "sour and bitter"},
]
examples = [
    {"id": "0", "sentence": sentences[0], "target": labels[0]},
    {"id": "1", "sentence": sentences[1], "target": labels[1]},
    {"id": "2", "sentence": sentences[2], "target": labels[2]},
]

@weave.op()
def fruit_name_score(target: dict, output: dict) -> dict:
    return {"correct": target["fruit"] == output["fruit"]}


evaluation = weave.Evaluation(
    dataset=examples,
    scorers=[
        MultiTaskBinaryClassificationF1(class_names=["fruit", "color", "flavor"]),
        fruit_name_score,
    ],
)
scores = asyncio.run(evaluation.evaluate(model)))
# Jupyter Notebook에서 실행하는 경우:
# scores = await evaluation.evaluate(model)

print(scores)
```

이 코드는 Weave UI에서 시각화할 수 있는 evaluation 트레이스를 생성합니다:

[![langchain\_evaluation.png](/weave/guides/integrations/imgs/langchain_eval.png)](https://wandb.ai/parambharat/langchain_demo/weave/calls?filter=%7B%22traceRootsOnly%22%3Atrue%7D\&peekPath=%2Fparambharat%2Flangchain_demo%2Fcalls%2F44c3f26c-d9d3-423e-b434-651ea5174be3)

Weave를 LangChain과 통합하면 LLM 애플리케이션을 포괄적으로 로깅하고 모니터링할 수 있어 디버깅과 성능 최적화가 더 쉬워집니다.

<div id="known-issues">
  ## 알려진 이슈
</div>

* **비동기 호출 추적(Tracing Async Calls)** - Langchain의 `AsyncCallbackManager` 구현에 존재하는 버그 때문에 비동기 호출이 올바른 순서로 추적되지 않습니다. 이 문제를 해결하기 위해 [PR](https://github.com/langchain-ai/langchain/pull/23909)을 제출했습니다. 따라서 Langchain Runnables에서 `ainvoke`, `astream`, `abatch` 메서드를 사용할 때 트레이스에서 호출 순서가 정확하지 않을 수 있습니다.