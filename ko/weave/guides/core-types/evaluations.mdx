---
title: "Evaluations 개요"
description: "평가 기반 LLM 애플리케이션 개발로 애플리케이션을 체계적으로 개선하기"
---

&#95;평가 기반 LLM 애플리케이션 개발&#95;은 일관되고 선별된 예제를 사용해 LLM 애플리케이션의 동작을 체계적으로 측정함으로써 애플리케이션을 체계적으로 개선할 수 있게 해줍니다.

<Tabs>
  <Tab title="Python">
    Weave에서 워크플로의 핵심은 &#95;`Evaluation` 객체&#95;이며, 다음을 정의합니다:

    * 테스트 예제용 [`Dataset`](../core-types/datasets) 또는 딕셔너리 리스트
    * 하나 이상의 [scoring function](../evaluation/scorers)
    * [입력 전처리](#format-dataset-rows-before-evaluating)와 같은 선택적 설정

    `Evaluation`을 정의한 후에는 [`Model`](../core-types/models) 객체나 LLM 애플리케이션 로직을 포함하는 임의의 사용자 정의 함수에 대해 이를 실행할 수 있습니다. `.evaluate()`를 호출할 때마다 &#95;evaluation run&#95;이 트리거됩니다. `Evaluation` 객체를 설계도로, 각 run을 해당 설정에서 애플리케이션 성능을 측정하는 과정으로 생각하면 됩니다.
  </Tab>

  <Tab title="TypeScript">
    Weave에서 워크플로의 핵심은 &#95;`Evaluation` 객체&#95;이며, 다음을 정의합니다:

    * 테스트 예제용 [`Dataset`](../core-types/datasets) 또는 객체 배열
    * 하나 이상의 [scoring function](../evaluation/scorers)

    `Evaluation`을 정의한 후에는 `weave.op`으로 래핑된 임의의 함수에 대해 이를 실행할 수 있습니다. `.evaluate()`를 호출할 때마다 &#95;evaluation run&#95;이 트리거됩니다. `Evaluation` 객체를 설계도로, 각 run을 해당 설정에서 애플리케이션 성능을 측정하는 과정으로 생각하면 됩니다.

    <Note>
      TypeScript SDK는 함수 기반 모델과 scorer를 사용합니다. 클래스 기반 `Model` 및 `Scorer` 타입은 TypeScript에서 아직 사용할 수 없습니다.
    </Note>
  </Tab>
</Tabs>

평가를 시작하려면 다음 단계를 수행합니다:

1. [`Evaluation` 객체 만들기](#1-create-an-evaluation-object)
2. [테스트 예제 데이터셋 정의](#2-define-a-dataset-of-test-examples)
3. [scoring function 정의](#3-define-scoring-functions)
4. [평가할 모델 또는 함수 정의](#4-define-a-model-or-function-to-evaluate)
5. [평가 실행](#5-run-the-evaluation)

완전한 Evaluation 코드 예시는 [여기](#full-evaluation-code-sample)에서 확인할 수 있습니다. 또한 [Saved views](#saved-views) 및 [명령형 평가 (`EvaluationLogger` 사용)](#imperative-evaluations-evaluationlogger)와 같은 [고급 평가 기능](#advanced-evaluation-usage)에 대해서도 더 자세히 알아볼 수 있습니다.

<div id="1-create-an-evaluation-object">
  ## 1. `Evaluation` 객체 생성
</div>

`Evaluation` 객체를 생성하는 것은 평가 설정을 구성하는 첫 번째 단계입니다. 하나의 `Evaluation`은 예제 데이터, 스코어링 로직, 그리고 선택적인 전처리로 구성됩니다. 이후 이 객체를 사용해 하나 이상의 평가를 실행합니다.

Weave는 각 예제를 애플리케이션을 통해 처리한 후, 여러 개의 커스텀 스코어링 함수로 출력값을 평가합니다. 이를 통해 애플리케이션의 성능을 한눈에 파악할 수 있고, 개별 출력과 점수를 자세히 탐색할 수 있는 풍부한 UI를 활용할 수 있습니다.

<div id="optional-custom-naming">
  ### (선택 사항) 사용자 지정 이름
</div>

<Tabs>
  <Tab title="Python">
    평가 플로우에서는 사용자 지정 가능한 이름 유형이 두 가지 있습니다.

    * [*Evaluation object name* (`evaluation_name`)](#name-the-evaluation-object): 구성한 `Evaluation` 객체에 대한 영구적인 레이블입니다.
    * [*Evaluation run display name* (`__weave["display_name"]`)](#name-individual-evaluation-runs): UI에 표시되는, 특정 평가 실행에 대한 레이블입니다.

    #### `Evaluation` 객체 이름 지정

    `Evaluation` 객체 자체에 이름을 지정하려면 `Evaluation` 클래스에 `evaluation_name` 파라미터를 전달합니다. 이 이름은 코드와 UI 목록에서 Evaluation을 식별하는 데 도움이 됩니다.

    ```python lines
    evaluation = Evaluation(
        dataset=examples, scorers=[match_score1], evaluation_name="My Evaluation"
    )
    ```

    #### 개별 evaluation run 이름 지정

    특정 evaluation run(`evaluate()` 호출)에 이름을 지정하려면 `display_name`을 포함한 `__weave` 사전을 사용합니다. 이는 해당 run이 UI에 어떻게 표시되는지에 영향을 줍니다.

    ```python lines
    evaluation = Evaluation(
        dataset=examples, scorers=[match_score1]
    )
    evaluation.evaluate(model, __weave={"display_name": "My Evaluation Run"})
    ```
  </Tab>

  <Tab title="TypeScript">
    `Evaluation` 객체에 이름을 지정하려면 `Evaluation` 생성자에 `id` 파라미터를 전달합니다. 이 이름은 코드와 UI 목록에서 Evaluation을 식별하는 데 도움이 됩니다.

    ```typescript lines
    const evaluation = new weave.Evaluation({
      id: 'my-evaluation',
      dataset: dataset,
      scorers: [matchScore],
    });
    ```
  </Tab>
</Tabs>

<div id="2-define-a-dataset-of-test-examples">
  ## 2. 테스트 예제용 데이터셋 정의하기
</div>

먼저 평가할 예제 모음으로 구성된 [Dataset](../core-types/datasets) 객체 또는 예제 목록을 정의합니다. 이러한 예제들은 대개 테스트 대상이 되는 실패 사례로, 테스트 주도 개발(TDD)의 단위 테스트와 비슷한 역할을 합니다.

<Tabs>
  <Tab title="Python">
    다음 예제는 딕셔너리 목록으로 정의된 데이터셋을 보여줍니다.

    ```python lines
    examples = [
        {"question": "What is the capital of France?", "expected": "Paris"},
        {"question": "Who wrote 'To Kill a Mockingbird'?", "expected": "Harper Lee"},
        {"question": "What is the square root of 64?", "expected": "8"},
    ]
    ```
  </Tab>

  <Tab title="TypeScript">
    다음 예제는 행(row) 배열을 가진 `Dataset` 객체로 정의된 데이터셋을 보여줍니다.

    ```typescript lines
    const dataset = new weave.Dataset({
      id: 'my-dataset',
      rows: [
        {question: 'What is the capital of France?', expected: 'Paris'},
        {question: 'Who wrote "To Kill a Mockingbird"?', expected: 'Harper Lee'},
        {question: 'What is the square root of 64?', expected: '8'},
      ],
    });
    ```
  </Tab>
</Tabs>

<div id="3-define-scoring-functions">
  ## 3. 점수 함수 정의
</div>

그런 다음 하나 이상의 [scoring functions](../evaluation/scorers)을 생성합니다. 이 함수들은 `Dataset`의 각 예시를 점수화하는 데 사용됩니다.

<Tabs>
  <Tab title="Python">
    각 점수 함수는 `output` 매개변수를 포함해야 하며, 점수가 담긴 사전을 반환해야 합니다. 선택적으로, 예시에 포함된 다른 입력들도 인자로 받을 수 있습니다.

    점수 함수는 `output` 키워드 인자를 가져야 하지만, 나머지 인자들은 사용자 정의이며 데이터셋 예시에서 가져옵니다. 함수는 인자 이름과 동일한 사전 키만 사용하여 필요한 값만 가져옵니다.

    <Tip>
      스코어러에서 `output` 인자를 기대하는데 전달되지 않는다면, 예전 방식의 `model_output` 키를 사용하고 있는지 확인하세요. 이를 수정하려면, 스코어러 함수가 키워드 인자로 `output`을 받도록 업데이트하세요.
    </Tip>

    다음 예제 스코어러 함수 `match_score1`은 점수 계산을 위해 `examples` 사전에서 `expected` 값을 사용합니다.

    ```python lines
    import weave

    # 예시를 수집합니다
    examples = [
        {"question": "What is the capital of France?", "expected": "Paris"},
        {"question": "Who wrote 'To Kill a Mockingbird'?", "expected": "Harper Lee"},
        {"question": "What is the square root of 64?", "expected": "8"},
    ]

    # 사용자 정의 점수 함수를 정의합니다
    @weave.op()
    def match_score1(expected: str, output: dict) -> dict:
        # 여기에서 모델 출력에 점수를 매기는 로직을 정의합니다
        return {'match': expected == output['generated_text']}
    ```

    ### (선택 사항) 사용자 정의 `Scorer` 클래스 정의하기

    일부 애플리케이션에서는 사용자 정의 `Scorer` 클래스를 만들고자 할 수 있습니다. 예를 들어, 특정 파라미터(예: 채팅 모델, 프롬프트), 각 행에 대한 특정 점수 계산, 집계 점수 계산 방식으로 표준화된 `LLMJudge` 클래스를 생성해야 할 수 있습니다.

    자세한 내용은 [RAG 애플리케이션의 모델 기반 평가](/ko/weave/tutorial-rag#optional-defining-a-scorer-class) 튜토리얼의 `Scorer` 클래스 정의 섹션을 참고하세요.
  </Tab>

  <Tab title="TypeScript">
    각 점수 함수는 `weave.op`으로 래핑되며, `modelOutput`과 `datasetRow` 속성을 가진 객체를 전달받습니다.

    다음 예제 스코어러 함수 `matchScore`는 모델 출력을 데이터셋 행의 `expected` 값과 비교합니다.

    ```typescript lines
    import * as weave from 'weave';

    // 예시들을 하나의 데이터셋으로 수집합니다
    const dataset = new weave.Dataset({
      id: 'my-dataset',
      rows: [
        {question: 'What is the capital of France?', expected: 'Paris'},
        {question: 'Who wrote "To Kill a Mockingbird"?', expected: 'Harper Lee'},
        {question: 'What is the square root of 64?', expected: '8'},
      ],
    });

    // 사용자 정의 점수 함수를 정의합니다
    const matchScore = weave.op(
      ({modelOutput, datasetRow}) => {
        return {match: modelOutput === datasetRow.expected};
      },
      {name: 'matchScore'}
    );
    ```

    <Note>
      클래스 기반 `Scorer` 타입은 아직 TypeScript에서 사용할 수 없습니다. `weave.op`으로 래핑한 함수 기반 스코어러를 사용하세요.
    </Note>
  </Tab>
</Tabs>

<div id="4-define-a-model-or-function-to-evaluate">
  ## 4. 평가할 모델 또는 함수 정의하기
</div>

<Tabs>
  <Tab title="Python">
    `Model`을 평가하려면 `Evaluation`을 사용해 해당 모델 인스턴스에서 `evaluate`를 호출하세요. `Models`는 실험해 보고 Weave에 기록하고 싶은 파라미터가 있을 때 사용합니다.

    ```python lines
    from weave import Model, Evaluation
    import asyncio

    class MyModel(Model):
        prompt: str

        @weave.op()
        def predict(self, question: str):
            # 여기에서 LLM 호출을 추가하고 결과를 반환합니다
            return {'generated_text': 'Hello, ' + self.prompt}

    model = MyModel(prompt='World')

    evaluation = Evaluation(
        dataset=examples, scorers=[match_score1]
    )
    weave.init('intro-example') # weave로 결과 추적 시작
    asyncio.run(evaluation.evaluate(model))
    ```

    이렇게 하면 각 예제에 대해 `predict`를 실행하고, 각 스코어링 함수로 출력값을 점수화합니다.

    ### (선택 사항) 평가할 함수 정의하기

    또는 `@weave.op()`으로 추적되는 사용자 정의 함수를 평가할 수도 있습니다.

    ```python lines
    @weave.op
    def function_to_evaluate(question: str):
        # 여기에서 LLM 호출을 추가하고 결과를 반환합니다
        return  {'generated_text': 'some response'}

    asyncio.run(evaluation.evaluate(function_to_evaluate))
    ```
  </Tab>

  <Tab title="TypeScript">
    TypeScript에서는 `weave.op`으로 래핑된 함수를 평가합니다. 이 함수는 데이터셋의 행을 입력으로 받고 모델 출력을 반환합니다.

    ```typescript lines
    import * as weave from 'weave';

    // Weave 초기화
    await weave.init('intro-example');

    // 평가할 함수 정의
    const myModel = weave.op(
      async ({question}) => {
        // 여기에서 LLM 호출을 추가하고 결과를 반환합니다
        return 'Paris';
      },
      {name: 'myModel'}
    );

    // Evaluation 생성
    const evaluation = new weave.Evaluation({
      id: 'my-evaluation',
      dataset: dataset,
      scorers: [matchScore],
    });

    // Evaluation 실행
    const results = await evaluation.evaluate({model: myModel});
    ```

    이렇게 하면 각 예제에 대해 `myModel`을 실행하고, 각 스코어링 함수로 출력값을 점수화합니다.
  </Tab>
</Tabs>

<div id="5-run-the-evaluation">
  ## 5. 평가 실행하기
</div>

평가를 실행하려면 `Evaluation` 객체에서 `.evaluate()`를 호출합니다.

<Tabs>
  <Tab title="Python">
    `evaluation`이라는 `Evaluation` 객체와 평가할 `model` 객체가 있다고 가정하면, 아래 코드는 평가 run을 생성합니다.

    ```python lines
    asyncio.run(evaluation.evaluate(model))
    ```

    ### (선택 사항) 여러 번의 trial 실행

    각 예제를 여러 번 실행하려면 `Evaluation` 객체의 `trials` 파라미터를 설정하면 됩니다.

    ```python lines
    evaluation = Evaluation(
        dataset=examples,
        scorers=[match_score],
        trials=3
    )
    ```
  </Tab>

  <Tab title="TypeScript">
    `evaluation`이라는 `Evaluation` 객체와 `myModel`이라는 model 함수가 있다고 가정하면, 아래 코드는 평가를 실행합니다.

    ```typescript lines
    const results = await evaluation.evaluate({model: myModel});
    ```

    ### (선택 사항) 여러 번의 trial 실행

    각 예제를 여러 번 실행하려면 `evaluate()`를 호출할 때 `nTrials` 파라미터를 설정하면 됩니다.

    ```typescript lines
    const results = await evaluation.evaluate({
      model: myModel,
      nTrials: 3,
    });
    ```
  </Tab>
</Tabs>

이 평가 run은 각 예제를 모델에 세 번씩 전달하며, 각 실행(run)은 Weave에서 개별적으로 점수화되고 표시됩니다.

<div id="full-evaluation-code-example">
  ## 전체 평가 코드 예시
</div>

<Tabs>
  <Tab title="Python">
    다음 코드 예시는 평가를 처음부터 끝까지 한 번 완전히 실행하는 방법을 보여줍니다. `examples` 사전은 `prompt` 값이 주어졌을 때 `MyModel`을 평가하기 위해 `match_score1` 및 `match_score2` 스코어링 함수에서 사용되며, 사용자 정의 함수 `function_to_evaluate`를 평가하는 데에도 사용됩니다. `Model`과 함수에 대한 평가는 모두 `asyncio.run(evaluation.evaluate())`로 호출됩니다.

    ```python lines
    from weave import Evaluation, Model
    import weave
    import asyncio
    weave.init('intro-example')
    examples = [
        {"question": "What is the capital of France?", "expected": "Paris"},
        {"question": "Who wrote 'To Kill a Mockingbird'?", "expected": "Harper Lee"},
        {"question": "What is the square root of 64?", "expected": "8"},
    ]

    @weave.op()
    def match_score1(expected: str, output: dict) -> dict:
        return {'match': expected == output['generated_text']}

    @weave.op()
    def match_score2(expected: dict, output: dict) -> dict:
        return {'match': expected == output['generated_text']}

    class MyModel(Model):
        prompt: str

        @weave.op()
        def predict(self, question: str):
            # 여기에서 LLM 호출을 추가하고 출력을 반환합니다
            return {'generated_text': 'Hello, ' + question + self.prompt}

    model = MyModel(prompt='World')
    evaluation = Evaluation(dataset=examples, scorers=[match_score1, match_score2])

    asyncio.run(evaluation.evaluate(model))

    @weave.op()
    def function_to_evaluate(question: str):
        # 여기에서 LLM 호출을 추가하고 출력을 반환합니다
        return  {'generated_text': 'some response' + question}

    asyncio.run(evaluation.evaluate(function_to_evaluate("What is the capitol of France?")))
    ```
  </Tab>

  <Tab title="TypeScript">
    다음 코드 예시는 평가를 처음부터 끝까지 한 번 완전히 실행하는 방법을 보여줍니다. 데이터셋의 각 행은 `matchScore` 스코어링 함수에서 `myModel`을 평가하는 데 사용됩니다.

    ```typescript lines
    import * as weave from 'weave';

    // Weave 초기화
    await weave.init('intro-example');

    // 예시를 수집해 데이터셋으로 구성
    const dataset = new weave.Dataset({
      id: 'my-dataset',
      rows: [
        {question: 'What is the capital of France?', expected: 'Paris'},
        {question: 'Who wrote "To Kill a Mockingbird"?', expected: 'Harper Lee'},
        {question: 'What is the square root of 64?', expected: '8'},
      ],
    });

    // 스코어링 함수 정의
    const matchScore = weave.op(
      ({modelOutput, datasetRow}) => {
        return {match: modelOutput === datasetRow.expected};
      },
      {name: 'matchScore'}
    );

    // 평가할 함수 정의
    const myModel = weave.op(
      async ({question}) => {
        // 여기에서 LLM 호출을 추가하고 출력을 반환합니다
        return 'Paris';
      },
      {name: 'myModel'}
    );

    // 평가 생성 및 실행
    const evaluation = new weave.Evaluation({
      id: 'my-evaluation',
      dataset: dataset,
      scorers: [matchScore],
    });

    const results = await evaluation.evaluate({model: myModel});
    console.log('Evaluation results:', results);
    ```
  </Tab>
</Tabs>

<Frame>
  ![Evals hero](/images/evals-hero.png)
</Frame>

<div id="advanced-evaluation-usage">
  ## 고급 평가 활용
</div>

<div id="format-dataset-rows-before-evaluating">
  ### 평가 전에 데이터셋 행 형식 지정하기
</div>

<Tabs>
  <Tab title="Python">
    <Warning>
      `preprocess_model_input` 함수는 모델의 예측 함수에 전달되기 전 입력에만 적용됩니다. Scorer 함수는 항상 어떤 전처리도 적용되지 않은 원본 데이터셋 예제를 받습니다.
    </Warning>

    `preprocess_model_input` 파라미터를 사용하면 평가 함수에 전달되기 전에 데이터셋 예제를 변환할 수 있습니다. 다음과 같은 경우에 유용합니다.

    * 필드 이름을 모델이 기대하는 입력 이름에 맞게 변경해야 할 때
    * 데이터를 올바른 형식으로 변환해야 할 때
    * 필드를 추가하거나 제거해야 할 때
    * 각 예제에 대해 추가 데이터를 로드해야 할 때

    다음은 `preprocess_model_input`을 사용해 필드 이름을 변경하는 방법을 보여 주는 간단한 예제입니다.

    ```python lines
    import weave
    from weave import Evaluation
    import asyncio

    # 데이터셋에는 "input_text"가 있지만, 모델은 "question"을 기대합니다
    examples = [
        {"input_text": "What is the capital of France?", "expected": "Paris"},
        {"input_text": "Who wrote 'To Kill a Mockingbird'?", "expected": "Harper Lee"},
        {"input_text": "What is the square root of 64?", "expected": "8"},
    ]

    @weave.op()
    def preprocess_example(example):
        # input_text를 question으로 이름 변경
        return {
            "question": example["input_text"]
        }

    @weave.op()
    def match_score(expected: str, output: dict) -> dict:
        return {'match': expected == output['generated_text']}

    @weave.op()
    def function_to_evaluate(question: str):
        return {'generated_text': f'Answer to: {question}'}

    # 전처리를 포함해 Evaluation 생성
    evaluation = Evaluation(
        dataset=examples,
        scorers=[match_score],
        preprocess_model_input=preprocess_example
    )

    # 평가 실행
    weave.init('preprocessing-example')
    asyncio.run(evaluation.evaluate(function_to_evaluate))
    ```

    이 예제에서 데이터셋에는 `input_text` 필드를 가진 예제들이 있지만, 평가 함수는 `question` 파라미터를 기대합니다. `preprocess_example` 함수는 필드 이름을 변경해 각 예제를 변환하고, 이를 통해 평가가 올바르게 동작하도록 합니다.

    전처리 함수는 다음을 수행합니다.

    1. 데이터셋에서 원본 예제를 입력으로 받습니다.
    2. 모델이 기대하는 필드를 가진 사전을 반환합니다.
    3. 예제가 평가 함수에 전달되기 전에 각 예제에 적용됩니다.

    이는 외부 데이터셋을 사용할 때 특히 유용합니다. 외부 데이터셋은 모델이 기대하는 것과 다른 필드 이름이나 구조를 가질 수 있습니다.
  </Tab>

  <Tab title="TypeScript">
    TypeScript에서는 `Evaluation` 객체의 `columnMapping`을 사용해 데이터셋 컬럼 이름을 scorer가 기대하는 이름에 매핑할 수 있습니다. 데이터셋의 필드 이름이 scorer 함수가 기대하는 이름과 다를 때 유용합니다.

    다음 예제는 `expectedOutputTimesTwo` 컬럼을 `expected` 컬럼에 매핑합니다.

    ```typescript lines
    const myScorer = weave.op(
      ({modelOutput, datasetRow}) => {
        return modelOutput * 2 === datasetRow.expectedOutputTimesTwo;
      },
      {name: 'myScorer'}
    );

    const evaluation = new weave.Evaluation({
      id: 'my-evaluation',
      dataset: [{expected: 2}],
      scorers: [myScorer],
      columnMapping: {expectedOutputTimesTwo: 'expected'},
    });
    ```

    <Note>
      `preprocess_model_input` 파라미터는 아직 TypeScript에서 사용할 수 없습니다. 데이터셋 필드를 scorer가 기대하는 필드에 매핑하려면 `columnMapping`을 사용하세요.
    </Note>
  </Tab>
</Tabs>

<div id="use-huggingface-datasets-with-evaluations">
  ### HuggingFace 데이터셋을 평가에 활용하기
</div>

<Tabs>
  <Tab title="Python">
    우리는 서드파티 서비스 및 라이브러리와의 인테그레이션을 지속적으로 개선하고 있습니다.

    더 원활한 인테그레이션을 제공하기 전까지는, 임시 방편으로 `preprocess_model_input`을 사용하여 Weave 평가에서 HuggingFace 데이터셋을 사용할 수 있습니다.

    자세한 내용은 [평가에서 HuggingFace 데이터셋 사용하기 쿡북](/ko/weave/cookbooks/hf_dataset_evals)을 참고하세요.
  </Tab>

  <Tab title="TypeScript">
    ```plaintext
    이 기능은 현재 TypeScript에서는 사용할 수 없습니다.
    ```
  </Tab>
</Tabs>

<div id="saved-views">
  ### 저장된 뷰
</div>

Evals 테이블 설정, 필터, 정렬을 *저장된 뷰* 로 저장해 두면 선호하는 구성을 빠르게 다시 불러와 사용할 수 있습니다. UI와 Python SDK 모두에서 저장된 뷰를 설정하고 사용할 수 있습니다. 자세한 내용은 [Saved Views](/ko/weave/guides/tools/saved-views)를 참조하세요.

<div id="imperative-evaluations-evaluationlogger">
  ### 명령형 평가 (`EvaluationLogger`)
</div>

더 유연한 평가 프레임워크를 선호한다면 Weave의 [`EvaluationLogger`](../evaluation/evaluation_logger)를 살펴보세요. `EvaluationLogger`는 Python과 TypeScript에서 모두 사용할 수 있으며, 복잡한 워크플로에 더 높은 유연성을 제공하고, 표준 평가 프레임워크는 더 많은 구조와 안내를 제공합니다.