---
title: "모델로 애플리케이션 버전 추적하기"
description: "데이터와 코드를 결합한 구조화된 모델로 애플리케이션 버전을 추적합니다."
---

`Model`은 설정, 학습된 모델 가중치 또는 기타 정보를 포함할 수 있는 데이터와 모델이 어떻게 동작하는지 정의하는 코드의 조합입니다. 코드를 이 API와 호환되도록 구조화하면, 애플리케이션의 버전을 구조화된 방식으로 관리할 수 있어 Experiments를 더 체계적으로 추적할 수 있습니다.

<Tabs>
  <Tab title="Python">
    Weave에서 모델을 만들려면 다음이 필요합니다.

    * `weave.Model`을 상속하는 클래스
    * 모든 파라미터에 대한 타입 정의
    * `@weave.op()` 데코레이터가 적용된 타입 지정 `predict` 함수

    ```python lines
    from weave import Model
    import weave

    class YourModel(Model):
        attribute1: str
        attribute2: int

        @weave.op()
        def predict(self, input_data: str) -> dict:
            # 모델 로직을 여기에 작성합니다
            prediction = self.attribute1 + ' ' + input_data
            return {'pred': prediction}
    ```

    일반적인 방식으로 다음과 같이 모델을 호출할 수 있습니다.

    ```python lines
    import weave
    weave.init('intro-example')

    model = YourModel(attribute1='hello', attribute2=5)
    model.predict('world')
    ```

    이렇게 하면 `predict`를 호출할 때마다 입력과 출력과 함께 모델 설정이 추적됩니다.

    ## 모델 자동 버전 관리

    모델을 정의하는 파라미터나 코드를 변경하면 이러한 변경 사항이 로깅되고 버전이 업데이트됩니다.
    이를 통해 모델의 서로 다른 버전 간 예측 결과를 비교할 수 있습니다. 이를 활용해 프롬프트를 반복적으로 개선하거나 최신 LLM을 시도하고, 다양한 설정에서 예측 결과를 비교해 보세요.

    예를 들어, 여기서는 새로운 모델을 생성합니다.

    ```python lines
    import weave
    weave.init('intro-example')

    model = YourModel(attribute1='howdy', attribute2=10)
    model.predict('world')
    ```

    이 코드를 실행한 후 UI에서 서로 다른 호출이 추적된 두 개의 Model 버전을 확인할 수 있습니다.

    ## 모델 서빙

    모델을 서빙하려면 다음을 호출해 FastAPI 서버를 간단히 구동할 수 있습니다.

    ```bash
    weave serve <your model ref>
    ```

    추가 안내는 [serve](/ko/weave/guides/tools/serve)를 참고하세요.

    ## 프로덕션 호출 추적

    프로덕션 호출을 분리하려면, UI나 API에서 쉽게 필터링할 수 있도록 예측 결과에 추가 속성을 포함시킬 수 있습니다.

    ```python lines
    with weave.attributes({'env': 'production'}):
        model.predict('world')
    ```
  </Tab>

  <Tab title="TypeScript">
    ```plaintext
    이 기능은 아직 TypeScript에서는 사용할 수 없습니다. 추후 업데이트를 기다려 주세요!
    ```
  </Tab>
</Tabs>