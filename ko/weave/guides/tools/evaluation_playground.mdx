---
title: "Evaluation Playground을 사용하여 모델 성능 비교하기"
description: "코드 없이 Weave의 인터랙티브 플레이그라운드를 사용해 모델 성능을 비교·평가하고, 시스템 프롬프트·모델·스코어링 기준을 테스트하기 위해 사용자 정의 데이터셋과 LLM 판정기를 활용해 Evaluation을 시각적 인터페이스에서 실행합니다."
---

<div id="evaluation-playground">
  # Evaluation Playground
</div>

Evaluation Playground을 사용하면 기존 모델에 접근하고, 평가 데이터세트와 LLM 채점 심사자를 활용해 성능을 비교할 수 있습니다. 이를 통해 별도의 코드를 작성하지 않고도 모델을 실험하고 비교할 수 있습니다. 또한 Playground에서 만든 모델, 채점기, 데이터세트를 저장해 두었다가 나중에 추가 개발이나 배포에 활용할 수 있습니다.

예를 들어 Evaluation Playground을 열고, 이전에 저장해 둔 두 개의 모델을 추가한 다음, 새로 만든 또는 이전에 저장해 둔 질문-응답 형식의 평가 데이터세트를 기준으로 두 모델의 성능을 평가할 수 있습니다. 그런 다음 인터페이스에서 새로운 모델을 추가하고 시스템 프롬프트를 설정한 뒤, 세 모델 모두에 대해 새로운 평가를 실행하여 서로의 성능을 비교할 수 있습니다.

<Frame>
  ![Evaluation Playground Interface](/weave/guides/evaluation/img/eval-playground-ui.png)
</Frame>

<div id="set-up-an-evaluation-in-the-playground">
  ## Playground에서 평가 설정하기
</div>

Evaluation Playground에서 평가를 설정하려면 다음을 수행합니다:

1. [Weave UI](https://wandb.ai/wandb/quickstart_playground/weave/playground)를 연 다음, 평가를 수행할 프로젝트를 엽니다. 그러면 Traces 페이지가 열립니다.
2. Traces 페이지에서 왼쪽 메뉴에서 **Playground** 아이콘을 클릭한 다음, Playground 페이지에서 **Evaluate** 탭을 선택합니다. Evaluate 페이지에서는 다음 중 하나를 선택할 수 있습니다:
   * **Load a demo example**: 미리 정의된 구성을 불러와 MoonshotAI Kimi K2 모델을 기대 출력에 대해 평가하고, LLM 평가자를 사용해 정답 여부를 판단합니다. 이 구성을 사용해 인터페이스를 실험해 볼 수 있습니다.
   * **Start from scratch**: 빈 구성을 불러와 처음부터 직접 설정을 구성할 수 있습니다.
3. **Start from scratch**를 선택한 경우, **Title** 및 **Description** 필드에 평가에 대한 제목과 설명을 입력합니다.

다음 섹션의 안내를 따라 데이터셋, 모델, 그리고 스코어러를 설정합니다.

<div id="add-a-dataset">
  ### 데이터셋 추가
</div>

[Datasets](../core-types/datasets)는 예시 사용자 입력과 모델이 내야 하는 예상 응답을 모아 정리해 둔 컬렉션입니다. 평가를 수행하는 동안 플레이그라운드는 각 테스트 입력을 모델에 넣고 출력값을 수집한 다음, 정답 여부와 같은 선택한 메트릭에 따라 출력을 점수화합니다. UI에서 새 데이터셋을 생성하거나, 이미 프로젝트에 저장된 기존 데이터셋을 추가하거나, 새 데이터셋을 업로드할 수 있습니다.

데이터셋은 다음 형식으로 업로드할 수 있습니다:

* `.csv`
* `.tsv`
* `.json`
* `.jsonl`

데이터셋을 어떻게 포맷하고 Weave에 저장하는지에 대한 자세한 내용은 Datasets 페이지를 참조하세요.

**Dataset** 섹션에서 데이터셋을 추가하려면:

1. 드롭다운 메뉴를 클릭한 다음, 다음 중 하나를 선택합니다:
   * **Start from scratch**를 선택해 UI에서 새 데이터셋을 생성합니다.
   * **Upload a file**을 선택해 로컬 머신에서 데이터셋 파일을 업로드합니다.
   * 프로젝트에 이미 저장된 기존 데이터셋을 선택합니다.
2. 선택 사항: 나중에 사용하기 위해 데이터셋을 프로젝트에 저장하려면 **Save**를 클릭합니다.

옵션을 선택하면 데이터셋이 UI의 오른쪽 패널에 표시되며, 각 필드를 클릭해 필요에 따라 수정할 수 있습니다. **Add row**를 클릭해 데이터셋에 새 행을 추가할 수도 있습니다.

<Note>
  새로 만든 데이터셋만 UI에서 편집할 수 있습니다.

  또한 스코어러가 데이터에 접근할 수 있도록 데이터셋의 열 이름을 `user_input`과 `expected_output`으로 적절히 지정하는 것이 중요합니다.
</Note>

<div id="add-a-model">
  ### 모델 추가하기
</div>

Weave의 컨텍스트에서 [Models](/ko/weave/guides/core-types/models)는 AI 모델(예: GPT)과 평가 동안 모델의 동작 방식을 정의하는 환경(여기서는 system prompt)의 조합을 의미합니다. 프로젝트에 있는 기존 모델을 선택하거나 새 모델을 만들어 평가할 수 있으며, 여러 모델을 한 번에 추가해 동일한 데이터셋과 scorer로 동시에 평가할 수 있습니다. **Playground 기능을 사용해 생성한 모델만 사용할 수 있습니다.**

Evaluation Playground의 **Models** 섹션에서 모델을 추가하려면:

1. **Add Model**을 클릭하고, 드롭다운 메뉴에서 **New Model** 또는 기존 모델을 선택합니다.

2. **New Model**을 선택했다면 다음 필드를 설정합니다:

   * **Name**: 새 모델에 대한 설명이 포함된 이름을 입력합니다.
   * **LLM Model**: OpenAI의 GPT-4와 같은 foundation model을 선택해 새 모델을 구성합니다. 이미 액세스를 설정해 둔 foundation model 목록에서 선택할 수 있으며, **Add AI provider**를 선택한 후 모델을 선택해 foundation model에 대한 액세스를 추가할 수도 있습니다. 프로바이더를 추가하면 해당 프로바이더에 대한 액세스 자격 증명을 입력하라는 메시지가 표시됩니다. API 키, endpoint, 그리고 Weave를 사용해 모델에 액세스하는 데 필요한 추가 설정 정보를 찾는 방법은 각 프로바이더의 문서를 참고하세요.
   * **System Prompt**: 모델이 어떻게 동작해야 하는지에 대한 지침을 입력합니다. 예: `You are a helpful assistant specializing in Python programming.` 데이터셋의 `user_input`은 후속 메시지에서 전송되므로 system prompt에 포함할 필요가 없습니다.

   기존 모델을 선택하면, 모델 이름 옆에 기존 모델의 버전을 선택할 수 있는 새 필드가 나타나며, 추가로 설정해야 할 필드는 없습니다. 평가 전후에 기존 모델을 변경하고 싶다면 [Prompt Playground](../tools/playground)를 사용하세요.

3. 선택 사항: 나중에 사용하기 위해 모델을 프로젝트에 저장하려면 **Save**를 클릭합니다.

4. 선택 사항: 동시에 평가할 추가 모델이 필요하면 **Add Model**을 다시 클릭해 필요한 만큼 다른 모델을 추가할 수 있습니다.

<div id="add-scorers">
  ### Scorer 추가
</div>

[Scorer](../evaluation/scorers)는 LLM judge를 사용해 AI 모델 출력의 품질을 측정하고 평가합니다. 프로젝트에서 기존 scorer를 선택하거나 새 scorer를 생성해 모델을 평가할 수 있습니다.

Evaluation Playground에서 scorer를 추가하려면:

1. **Add Scorer**를 클릭한 다음, 다음 필드를 설정합니다:
   * **Name**: scorer에 대한 설명적인 이름을 입력합니다.
   * **Type**: 점수를 boolean으로 출력할지, 숫자로 출력할지 선택합니다. Boolean scorer는 모델 출력이 설정한 판단 기준을 충족했는지에 따라 `True` 또는 `False`의 이진 값을 반환합니다. Number scorer는 `0`에서 `1` 사이의 점수를 출력하며, 모델 출력이 판단 기준을 얼마나 잘 충족했는지를 나타내는 총평 점수를 제공합니다.
   * **LLM-as-a-judge-model**: scorer의 judge로 사용할 foundation model을 선택합니다. **Models** 섹션의 LLM Model 필드와 마찬가지로, 이미 액세스 구성을 완료한 foundation model 중에서 선택하거나, 새 foundation model 액세스를 구성할 수 있습니다.
   * **Scoring Prompt**: LLM judge가 어떤 기준으로 출력을 채점해야 하는지에 대한 파라미터를 제공합니다. 예를 들어 환각(hallucination)을 검사하고 싶다면 다음과 유사한 scoring prompt를 입력할 수 있습니다:

     ```
     Given the following context and answer, determine if the answer contains any information not supported by the context.

     User input: {user_input}
     Expected output: {expected_output}
     Model Output: {output}

     Is the model output correct?
     ```

     `{user_input}`, `{expected_output}`, `{output}`처럼 scoring prompt에서 데이터셋과 응답의 필드를 변수로 사용할 수 있습니다. 사용 가능한 변수 목록을 보려면 UI에서 **Insert variable**을 클릭합니다.

2. 선택 사항: **Save**를 클릭하여 나중에 사용할 수 있도록 scorer를 프로젝트에 저장합니다.

<div id="run-the-evaluation">
  ### 평가 실행하기
</div>

데이터 세트, 모델, 그리고 스코어러 설정을 마쳤다면 평가를 실행할 수 있습니다.

* Evaluation Playground에서 평가를 실행하려면 **Run eval**을 클릭하세요.

Weave는 추가한 각 모델에 대해 개별 평가를 실행하고, 데이터 세트를 사용하는 각 요청에 대한 지표를 수집합니다. Weave는 이러한 각 평가를 나중에 검토할 수 있도록 **Evals** 섹션에 저장합니다.

<div id="review-evaluation-results">
  ### 평가 결과 검토
</div>

Evaluation이 완료되면 playground에서 모델에 대해 수행된 각 요청에서 수집된 다양한 지표를 보여 주는 리포트를 엽니다.

<Frame>
  ![Evals hero](/images/weave/evals-hero.png)
</Frame>

**Dataset results** 탭에는 입력, 기대 출력, 모델의 실제 출력, 지연 시간(latency), 토큰 사용량, 스코어링 결과가 표시됩니다. **Row** 열의 ID를 클릭하면 특정 요청 집합에 대한 세부 지표를 확인할 수 있습니다. 또한 탭 바로 아래에 있는 표시 형식 버튼을 사용해 리포트 셀의 표시 형식을 변경할 수 있습니다.

**Summary** 탭에서는 각 모델의 성능을 데이터의 시각적 표현과 함께 개괄적으로 보여 줍니다.

Evaluation을 열고 비교하는 방법에 대한 자세한 내용은 [Evaluations](../core-types/evaluations)을 참조하세요.