---
title: "Weave란 무엇인가요?"
description: "W&B Weave에 대해 알아보고, LLM 애플리케이션을 빌드·평가·개선하는 데 어떻게 도움이 되는지 살펴보세요"
---

W&amp;B Weave는 신뢰할 수 있는 LLM 애플리케이션을 구축하기 위한 관측(Observability) 및 평가 플랫폼입니다. Weave는 AI 애플리케이션이 무엇을 하고 있는지 이해하고, 얼마나 잘 동작하는지 측정하며, 시간에 따라 체계적으로 개선할 수 있도록 도와줍니다.

LLM 애플리케이션을 구축하는 일은 전통적인 소프트웨어 개발과 근본적으로 다릅니다. LLM 출력은 비결정적이기 때문에 디버깅이 더 어렵습니다. 품질은 주관적이며 맥락에 따라 달라집니다. 작은 프롬프트 변경만으로도 예기치 않은 동작 변화를 유발할 수 있습니다. 전통적인 테스트 접근 방식만으로는 충분하지 않습니다.

<div id="the-main-threads-of-weave">
  ## Weave의 주요 축
</div>

Weave는 다음과 같은 핵심 기능을 제공합니다:

* 애플리케이션에서 발생하는 모든 LLM 호출과 입력, 출력을 한눈에 볼 수 있는 **가시성**
* 큐레이션된 테스트 케이스를 기반으로 성능을 측정하는 **체계적인 평가**
* 무엇이 어떻게 변경되었는지 이해할 수 있도록 프롬프트, 모델, 데이터에 대한 **버전 추적**
* 서로 다른 프롬프트와 모델을 비교해 볼 수 있는 **실험 기능**
* 사람의 판단과 주석을 수집하는 **피드백 수집**
* LLM의 안전성과 품질을 위해 가드레일과 스코어러를 활용한 운영 환경 **모니터링**

<div id="traces">
  ### Traces
</div>

LLM 애플리케이션에서 데이터가 처음부터 끝까지 어떻게 흐르는지 추적합니다.

* 각 애플리케이션 사용 과정의 입력과 출력을 확인합니다.
* LLM 응답을 생성하는 데 사용된 소스 문서를 확인합니다.
* LLM 호출의 비용, 토큰 수, 레이턴시를 확인합니다.
* 특정 프롬프트와 답변이 어떻게 생성되는지까지 자세히 분석합니다.
* 사용자로부터 응답에 대한 피드백을 수집합니다.
* 코드에서 Weave [ops and calls](/ko/weave/guides/tracking/tracing)를 사용하여 함수에서 어떤 일이 일어나는지 추적할 수 있습니다.

[트레이싱 시작하기](/ko/weave/quickstart)

<div id="evaluations">
  ### 평가
</div>

LLM 애플리케이션의 성능을 체계적으로 벤치마크하여 프로덕션 배포 시 신뢰도를 높이세요.

* 어떤 모델/프롬프트 버전이 어떤 성능을 냈는지 쉽게 추적할 수 있습니다.
* 하나 이상의 스코어링 함수를 사용해 응답을 평가할 메트릭을 정의할 수 있습니다.
* 여러 메트릭에 걸쳐 두 개 이상의 서로 다른 평가를 비교할 수 있습니다. 개별 샘플의 성능을 서로 대비해 볼 수 있습니다.

[평가 파이프라인 구축하기](/ko/weave/tutorial-eval)

<div id="version-everything">
  ### 모든 것을 버전으로 관리하세요
</div>

Weave는 프롬프트, 데이터셋, 모델 설정의 버전을 추적합니다. 문제가 발생하면 정확히 어떤 부분이 변경되었는지 확인할 수 있습니다. 잘 동작한 경우에는 그대로 재현할 수 있습니다.

[버전 관리에 대해 알아보기](/ko/weave/guides/tracking/objects)

<div id="experiment-with-prompts-and-models">
  ### 프롬프트와 모델 실험하기
</div>

API 키를 입력해 Playground에서 프롬프트를 빠르게 테스트하고 다양한 상용 모델의 응답을 비교해 보세요.

[Weave Playground에서 실험하기](/ko/weave/guides/tools/playground)

<div id="collect-feedback">
  ### 피드백 수집
</div>

운영 환경에서 사람의 피드백, 어노테이션, 수정 사항을 수집하세요. 이 데이터를 사용해 더 나은 테스트 케이스를 만들고 애플리케이션을 개선하세요.

[피드백 수집](/ko/weave/guides/tracking/feedback)

<div id="monitor-production">
  ### 프로덕션 모니터링
</div>

평가에 사용할 때와 동일한 스코어러로 프로덕션 트래픽에 점수를 매기세요. 문제가 사용자에게 도달하기 전에 포착할 수 있도록 가드레일을 설정하세요.

[가드레일과 모니터를 설정하세요](/ko/weave/guides/evaluation/monitors)

<div id="get-started-using-weave">
  ## Weave 사용 시작하기
</div>

Weave는 Python과 TypeScript용 SDK를 제공합니다. 두 SDK 모두 트레이싱, 평가, 데이터셋, 그리고 핵심 Weave 기능을 지원합니다. 클래스 기반 Models와 Scorers와 같은 일부 고급 기능은 현재 Weave TypeScript SDK에서는 사용할 수 없습니다.

Weave를 사용하기 시작하려면:

1. [https://wandb.ai/site](https://wandb.ai/site/?utm_source=course\&utm_medium=course\&utm_campaign=weave)에서 Weights &amp; Biases 계정을 만들고, [https://wandb.ai/authorize](https://wandb.ai/authorize?utm_source=course\&utm_medium=course\&utm_campaign=weave)에서 API 키를 가져옵니다.
2. Weave를 설치합니다:

<CodeGroup>
  ```Python Python
  pip install weave
  ```

  ```Typescript Typescript
  npm install weave
  ```
</CodeGroup>

3. 스크립트에서 Weave를 임포트하고 프로젝트를 초기화합니다:

<CodeGroup>
  ```Python Python
  import weave
  client = weave.init('your-team/your-project-name')
  ```

  ```TypeScript Typescript
  import * as weave from 'weave';
  const client = await weave.init('your-team/your-project-name');
  ```
</CodeGroup>

이제 Weave를 사용할 준비가 되었습니다.
Weave는 널리 사용되는 LLM 제공업체와 프레임워크와 인테그레이션되어 있습니다. [지원되는 인테그레이션](/ko/weave/guides/integrations/)을 사용할 때 Weave는 추가 코드 변경 없이 LLM 호출을 자동으로 트레이싱합니다.

4. 지원되는 인테그레이션에만 의존하는 것뿐 아니라, 호출 함수에 한 줄만 추가해 사용자 정의 함수에 대한 트레이스를 Weave로 로깅할 수도 있습니다.

함수를 `@weave.op()`(Python)으로 데코레이트하거나 `weave.op()`(TypeScript)로 래핑하면, Weave가 해당 함수의 코드, 입력, 출력, 실행 메타데이터를 자동으로 캡처합니다.

<CodeGroup>
  ```python Python
      @weave.op
      async def my_function(){
        ...  }
  ```

  ```typescript Typescript
  function myFunction() {
      ...
  }

  const myFunctionOp = weave.op(myFunction)
  ```
</CodeGroup>

가이드 튜토리얼과 함께 직접 시도해 보려면 [트레이싱 시작하기](/ko/weave/quickstart)를 참조하세요.