---
title: "Weave란 무엇인가?"
description: "W&B Weave와 이를 통해 LLM 애플리케이션을 구축, 평가, 개선하는 방법을 알아보세요"
---

W&amp;B Weave는 신뢰할 수 있는 LLM 애플리케이션을 구축하기 위한 관찰 및 평가 플랫폼입니다. Weave는 AI 애플리케이션이 무엇을 하고 있는지 이해하고, 얼마나 잘 동작하는지 측정하며, 시간이 지남에 따라 체계적으로 개선할 수 있도록 도와줍니다.

LLM 애플리케이션을 구축하는 일은 기존 소프트웨어 개발과 근본적으로 다릅니다. LLM의 출력 결과는 비결정적이어서 디버깅을 더 어렵게 만듭니다. 품질은 주관적이고 상황에 따라 달라집니다. 작은 프롬프트 변경만으로도 예기치 않은 동작 변화를 일으킬 수 있습니다. 기존의 테스트 방식만으로는 충분하지 않습니다.

<div id="the-main-threads-of-weave">
  ## Weave의 주요 축
</div>

Weave는 다음과 같은 핵심 기능을 제공합니다:

* 애플리케이션 내 모든 LLM 호출, 입력, 출력에 대한 **가시성** 제공
* 선별된 테스트 케이스를 기준으로 성능을 측정하기 위한 **체계적인 평가**
* 어떤 변경이 있었는지 이해할 수 있도록 프롬프트, 모델, 데이터에 대한 **버전 관리**
* 서로 다른 프롬프트와 모델을 비교하는 **실험**
* 사람의 판단과 주석을 수집하기 위한 **피드백 수집**
* LLM 안전성과 품질을 위해 가드레일과 스코어러를 사용하는 프로덕션 환경에서의 **모니터링**

<div id="traces">
  ### 트레이스
</div>

LLM 애플리케이션에서 데이터가 처음부터 끝까지 어떻게 흐르는지 추적합니다.

* 각 애플리케이션 호출의 입력과 출력을 확인합니다.
* LLM 응답을 생성하는 데 사용된 원본 문서를 확인합니다.
* LLM 호출의 비용, 토큰 수, 지연 시간을 확인합니다.
* 특정 프롬프트와 답변이 어떻게 생성되었는지까지 자세히 살펴봅니다.
* 사용자가 응답에 대해 남긴 피드백을 수집합니다.
* 코드에서는 Weave [ops and calls](/ko/weave/guides/tracking/tracing)를 사용해 함수가 무엇을 하는지 추적할 수 있습니다.

[트레이싱 시작하기](/ko/weave/quickstart)

<div id="evaluations">
  ### Evaluations
</div>

프로덕션에 배포할 때 안심할 수 있도록 LLM 애플리케이션의 성능을 체계적으로 벤치마크하세요.

* 어떤 모델/프롬프트 버전이 어떤 성능을 보였는지 쉽게 추적할 수 있습니다.
* 하나 이상의 scoring function(점수 함수)으로 응답을 평가할 메트릭을 정의합니다.
* 다양한 메트릭에 걸쳐 둘 이상의 서로 다른 Evaluation을 비교합니다. 개별 샘플의 성능도 나란히 살펴볼 수 있습니다.

[Evaluation 파이프라인 구축하기](/ko/weave/tutorial-eval)

<div id="version-everything">
  ### 모든 것을 버전으로 관리하기
</div>

Weave는 프롬프트, 데이터셋, 모델 구성의 버전을 추적합니다. 문제가 생기면 무엇이 어떻게 바뀌었는지 정확히 확인할 수 있습니다. 잘 동작하는 구성이 확인되면 언제든 그대로 재현할 수 있습니다.

[버전 관리에 대해 더 알아보기](/ko/weave/guides/tracking/objects)

<div id="experiment-with-prompts-and-models">
  ### 프롬프트와 모델 실험하기
</div>

API 키를 입력하고 Playground에서 다양한 상용 모델의 응답을 빠르게 테스트하고 비교하세요.

[Weave Playground에서 실험하기](/ko/weave/guides/tools/playground)

<div id="collect-feedback">
  ### 피드백 수집
</div>

실제 서비스 환경에서 사용자 피드백, 주석, 수정 내용을 수집합니다. 이 데이터를 활용해 더 나은 테스트 케이스를 만들고 애플리케이션을 개선하세요.

[피드백 수집하기](/ko/weave/guides/tracking/feedback)

<div id="monitor-production">
  ### 프로덕션 모니터링
</div>

Evaluation에서 사용하는 것과 동일한 scorer로 프로덕션 트래픽에 점수를 매기세요. 문제가 사용자에게 도달하기 전에 감지할 수 있도록 guardrail을 설정하세요.

[guardrail과 모니터 설정](/ko/weave/guides/evaluation/monitors)

<div id="get-started-using-weave">
  ## Weave 사용 시작하기
</div>

Weave는 Python과 TypeScript용 SDK를 제공합니다. 두 SDK 모두 트레이싱, evaluation, 데이터셋 및 핵심 Weave 기능을 지원합니다. 클래스 기반 Models 및 Scorers 같은 일부 고급 기능은 현재 Weave TypeScript SDK에서는 사용할 수 없습니다.

Weave를 사용해 시작하려면:

1. [https://wandb.ai/site](https://wandb.ai/site/?utm_source=course\&utm_medium=course\&utm_campaign=weave)에서 Weights &amp; Biases 계정을 만들고, [https://wandb.ai/authorize](https://wandb.ai/authorize?utm_source=course\&utm_medium=course\&utm_campaign=weave)에서 API 키를 발급받습니다.
2. Weave를 설치합니다:

<CodeGroup>
  ```Python Python
  pip install weave
  ```

  ```Typescript Typescript
  npm install weave
  ```
</CodeGroup>

3. 스크립트에서 Weave를 가져오고 프로젝트를 초기화합니다:

<CodeGroup>
  ```Python Python
  import weave
  client = weave.init('your-team/your-project-name')
  ```

  ```TypeScript Typescript
  import * as weave from 'weave';
  const client = await weave.init('your-team/your-project-name');
  ```
</CodeGroup>

이제 Weave를 사용할 준비가 되었습니다.
Weave는 널리 사용되는 LLM 제공자 및 프레임워크와 통합됩니다. [지원되는 통합](/ko/weave/guides/integrations/)을 사용할 경우, Weave는 추가 코드 변경 없이 LLM 호출을 자동으로 트레이싱합니다.

4. 지원되는 통합에만 의존하지 않고, 호출 함수에 한 줄만 추가하여 사용자 정의 함수의 트레이스를 로깅하는 데에도 Weave를 사용할 수 있습니다.

함수에 `@weave.op()`(Python) 데코레이터를 적용하거나, `weave.op()`(TypeScript)로 래핑하면 Weave가 해당 함수의 코드, 입력, 출력, 실행 메타데이터를 자동으로 캡처합니다.

<CodeGroup>
  ```python Python
      @weave.op
      async def my_function(){
        ...  }
  ```

  ```typescript Typescript
  function myFunction() {
      ...
  }

  const myFunctionOp = weave.op(myFunction)
  ```
</CodeGroup>

가이드형 튜토리얼과 함께 직접 사용해 보려면 [트레이싱 시작하기](/ko/weave/quickstart)를 참고하세요.