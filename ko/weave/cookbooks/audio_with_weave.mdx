---
title: Weaveë¥¼ ì‚¬ìš©í•œ Audio
description: W&B Weaveë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì„¸ìš”.
---

<Note>
ì´ ë…¸íŠ¸ë¶ì€ ì¸í„°ë™í‹°ë¸Œ ë…¸íŠ¸ë¶ì…ë‹ˆë‹¤. ë¡œì»¬ì—ì„œ ì‹¤í–‰í•˜ê±°ë‚˜ ì•„ë˜ ë§í¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- [Google Colabì—ì„œ ì—´ê¸°](https://colab.research.google.com/github/wandb/docs/blob/main/weave/cookbooks/source/audio_with_weave.ipynb)
- [GitHubì—ì„œ ì†ŒìŠ¤ ë³´ê¸°](https://github.com/wandb/docs/blob/main/weave/cookbooks/source/audio_with_weave.ipynb)
</Note>

## 

# ì˜¤ë””ì˜¤ ë°ì´í„°ì— Weave ì‚¬ìš©í•˜ê¸°: OpenAI ì˜ˆì œ

ì´ ë°ëª¨ì—ì„œëŠ” OpenAI chat completions APIì™€ GPT 4o Audio Previewë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì˜¤ë””ì˜¤ ì‘ë‹µì„ ìƒì„±í•˜ê³  ì´ë¥¼ Weaveì—ì„œ ì¶”ì í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

<img src="https://i.imgur.com/OUfsZ2x.png"></img>

ì‹¬í™” ìœ ìŠ¤ ì¼€ì´ìŠ¤ë¡œëŠ” OpenAI Realtime APIë¥¼ í™œìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì˜¤ë””ì˜¤ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤. ë¹„ë””ì˜¤ ë°ëª¨ë¥¼ ë³´ë ¤ë©´ ë‹¤ìŒ ì¸ë„¤ì¼ì„ í´ë¦­í•˜ê±°ë‚˜ [ì—¬ê¸°](https://www.youtube.com/watch?v=lnnd73xDElw)ë¥¼ í´ë¦­í•˜ì„¸ìš”.

[![Everything Is AWESOME](https://img.youtube.com/vi/lnnd73xDElw/0.jpg)](https://www.youtube.com/watch?v=lnnd73xDElw "Everything Is AWESOME")

## ì„¤ì •

ë¨¼ì € OpenAI (`openai`)ì™€ Weave (`weave`) ì¢…ì†ì„±, ê·¸ë¦¬ê³  API í‚¤ ê´€ë¦¬ë¥¼ ìœ„í•œ `set-env`ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.

```python lines
%%capture
!pip install openai
!pip install weave
!pip install set-env-colab-kaggle-dotenv -q # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì„ ìœ„í•´ ì‚¬ìš©
python
%%capture
# openaiì˜ ë²„ê·¸ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì„ì‹œ ìš°íšŒì±…:
# TypeError: Client.__init__() got an unexpected keyword argument 'proxies'
# ì°¸ê³ : https://community.openai.com/t/error-with-openai-1-56-0-client-init-got-an-unexpected-keyword-argument-proxies/1040332/15
!pip install "httpx<0.28"
```

ë‹¤ìŒìœ¼ë¡œ OpenAIì™€ Weaveì— í•„ìš”í•œ API í‚¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” Google Colabì˜ secret keys managerì™€ í˜¸í™˜ë˜ëŠ” `set_env`ë¥¼ ì‚¬ìš©í•˜ë©°, ì´ëŠ” Colab ì „ìš©ì¸ `google.colab.userdata`ë¥¼ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ” ëŒ€ì•ˆì…ë‹ˆë‹¤. ì‚¬ìš©ë²•ì€ [ì—¬ê¸°](https://pypi.org/project/set-env-colab-kaggle-dotenv/)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

```python lines
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
from set_env import set_env

_ = set_env("OPENAI_API_KEY")
_ = set_env("WANDB_API_KEY")
```

ë§ˆì§€ë§‰ìœ¼ë¡œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.

```python lines
import base64
import os
import time
import wave

import numpy as np
from IPython.display import display
from openai import OpenAI

import weave
```

## ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ë° ì €ì¥ ì˜ˆì œ

ì´ì œ ì˜¤ë””ì˜¤ ëª¨ë‹¬ë¦¬í‹°ê°€ í™œì„±í™”ëœ OpenAI completions ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œì„ ì„¤ì •í•˜ê² ìŠµë‹ˆë‹¤. ë¨¼ì € OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ê³  Weave í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.

```python lines
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
weave.init("openai-audio-chat")
```

ì´ì œ OpenAI completions ìš”ì²­ì„ ì •ì˜í•˜ê³  Weave ë°ì½”ë ˆì´í„°(op)ë¥¼ ì¶”ê°€í•˜ê² ìŠµë‹ˆë‹¤.

ì—¬ê¸°ì„œëŠ” `prompt_endpont_and_log_trace` í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

1. í…ìŠ¤íŠ¸ ë° ì˜¤ë””ì˜¤ ì…ì¶œë ¥ì„ ì§€ì›í•˜ëŠ” `GPT 4o Audio Preview` ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ completion ì˜¤ë¸Œì íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

   - ëª¨ë¸ì´ ë‹¤ì–‘í•œ ì–µì–‘ìœ¼ë¡œ ì²œì²œíˆ 13ê¹Œì§€ ì„¸ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
   - completionì„ "stream"ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

2. ìŠ¤íŠ¸ë¦¬ë°ëœ ë°ì´í„°ê°€ ì²­í¬ë³„ë¡œ ê¸°ë¡ë  ìƒˆë¡œìš´ ì¶œë ¥ íŒŒì¼ì„ ì—½ë‹ˆë‹¤.

3. Weaveê°€ traceì— ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë¡œê·¸í•  ìˆ˜ ìˆë„ë¡ ì˜¤ë””ì˜¤ íŒŒì¼ì˜ íŒŒì¼ í•¸ë“¤ëŸ¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

```python lines
SAMPLE_RATE = 22050

@weave.op()
def prompt_endpoint_and_log_trace(system_prompt=None, user_prompt=None):
    if not system_prompt:
        system_prompt = "You're the fastest counter in the world"
    if not user_prompt:
        user_prompt = "Count to 13 super super slow, enunciate each number with a dramatic flair, changing up accents as you go along. British, French, German, Spanish, etc."
    # ì˜¤ë””ì˜¤ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ì‚¬ìš©í•œ OpenAI API ìš”ì²­
    completion = client.chat.completions.create(
        model="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "fable", "format": "pcm16"},
        stream=True,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )

    # ì“°ê¸°ìš© wave íŒŒì¼ ì—´ê¸°
    with wave.open("./output.wav", "wb") as wav_file:
        wav_file.setnchannels(1)  # ëª¨ë…¸
        wav_file.setsampwidth(2)  # 16-bit
        wav_file.setframerate(SAMPLE_RATE)  # ìƒ˜í”Œ ë ˆì´íŠ¸ (í•„ìš”ì‹œ ì¡°ì •)

        # APIì—ì„œ ìŠ¤íŠ¸ë¦¬ë°ë˜ëŠ” ì²­í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê¸°ë¡
        for chunk in completion:
            if (
                hasattr(chunk, "choices")
                and chunk.choices is not None
                and len(chunk.choices) > 0
                and hasattr(chunk.choices[0].delta, "audio")
                and chunk.choices[0].delta.audio.get("data") is not None
            ):
                # base64 ì˜¤ë””ì˜¤ ë°ì´í„° ë””ì½”ë”©
                audio_data = base64.b64decode(chunk.choices[0].delta.audio.get("data"))

                # í˜„ì¬ ì²­í¬ë¥¼ wave íŒŒì¼ì— ê¸°ë¡
                wav_file.writeframes(audio_data)

    # Weave opì— íŒŒì¼ ë°˜í™˜
    return wave.open("output.wav", "rb")
```

## í…ŒìŠ¤íŠ¸

ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”. ì‹œìŠ¤í…œ ë° ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì™€ ì¶œë ¥ ì˜¤ë””ì˜¤ê°€ Weave traceì— ì €ì¥ë©ë‹ˆë‹¤.
ì…€ ì‹¤í–‰ í›„, "ğŸ©" ì´ëª¨ì§€ ì˜†ì˜ ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ traceë¥¼ í™•ì¸í•˜ì„¸ìš”.

```python lines
from IPython.display import Audio

# ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ê¸°ë¡í•˜ëŠ” í•¨ìˆ˜ í˜¸ì¶œ
prompt_endpoint_and_log_trace(
    system_prompt="You're the fastest counter in the world",
    user_prompt="Count to 13 super super slow, enunciate each number with a dramatic flair, changing up accents as you go along. British, French, German, Spanish, etc.",
)

# ì—…ë°ì´íŠ¸ëœ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¬ìƒ
display(Audio("output.wav", rate=SAMPLE_RATE, autoplay=True))
```

# ì‹¬í™” ì‚¬ìš©ë²•: Weaveì™€ í•¨ê»˜ Realtime Audio API ì‚¬ìš©í•˜ê¸°

<img src="https://i.imgur.com/ZiW3IVu.png"/>
<details>
<summary> (ì‹¬í™”) Weaveì™€ í•¨ê»˜ Realtime Audio API ì‚¬ìš©í•˜ê¸° </summary>
OpenAIì˜ Realtime APIëŠ” ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ë° í…ìŠ¤íŠ¸ ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ê¸°ëŠ¥ì´ ë›°ì–´ë‚˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ëŒ€í™”í˜• APIì…ë‹ˆë‹¤.

ì°¸ê³  ì‚¬í•­:

- [ë§ˆì´í¬ ì„¤ì •](#microphone-configuration) ì„¹ì…˜ì˜ ì…€ì„ ê²€í† í•˜ì„¸ìš”.
- Google Colab ì‹¤í–‰ í™˜ê²½ì˜ ì œí•œìœ¼ë¡œ ì¸í•´, **ì´ ì½”ë“œëŠ” ë°˜ë“œì‹œ í˜¸ìŠ¤íŠ¸ ë¨¸ì‹ ì—ì„œ** Jupyter Notebookìœ¼ë¡œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì €ì—ì„œëŠ” ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
  - MacOSì—ì„œëŠ” Pyaudio ì‘ë™ì„ ìœ„í•´ Brewë¥¼ í†µí•´ `portaudio`ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤ ([ì—¬ê¸°](https://formulae.brew.sh/formula/portaudio) ì°¸ì¡°).
- OpenAIì˜ Python SDKëŠ” ì•„ì§ Realtime APIë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê°€ë…ì„±ì„ ìœ„í•´ Pydanticìœ¼ë¡œ ì „ì²´ OAI Realtime API ìŠ¤í‚¤ë§ˆë¥¼ êµ¬í˜„í–ˆìœ¼ë©°, ê³µì‹ ì§€ì›ì´ ì¶œì‹œë˜ë©´ ì§€ì›ì´ ì¤‘ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- `enable_audio_playback` í† ê¸€ì€ ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì¶œë ¥ ì˜¤ë””ì˜¤ ì¬ìƒ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. **ì´ ê¸°ëŠ¥ì„ ì¼œëŠ” ê²½ìš° ì—ì½” ê°ì§€ê°€ ë§¤ìš° ë³µì¡í•˜ë¯€ë¡œ ë°˜ë“œì‹œ í—¤ë“œí°ì„ ì°©ìš©í•´ì•¼ í•©ë‹ˆë‹¤**.

## ìš”êµ¬ ì‚¬í•­ ì„¤ì •

```python lines
%%capture
!pip install numpy==2.0
!pip install weave
!pip install pyaudio # Macì˜ ê²½ìš°, brew install portaudioë¡œ ë¨¼ì € portaudioë¥¼ ì„¤ì¹˜í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
!pip install websocket-client
!pip install set-env-colab-kaggle-dotenv -q # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ìš©
!pip install resampy
python
import io
import json
import os
import threading
from typing import Optional

import pyaudio
import resampy
import websocket
from set_env import set_env

import weave
python
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
# ì‚¬ìš©ë²•ì€ https://pypi.org/project/set-env-colab-kaggle-dotenv/ ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
_ = set_env("OPENAI_API_KEY")
_ = set_env("WANDB_API_KEY")
```

## ë§ˆì´í¬ ì„¤ì •

ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì˜¤ë””ì˜¤ ì¥ì¹˜ë¥¼ ì°¾ìœ¼ë ¤ë©´ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”. ê·¸ëŸ° ë‹¤ìŒ ë‚˜ì—´ëœ ì¥ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ `INPUT_DEVICE_INDEX`ì™€ `OUTPUT_DEVICE_INDEX`ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ì…ë ¥ ì¥ì¹˜ëŠ” ìµœì†Œ 1ê°œì˜ ì…ë ¥ ì±„ë„ì„ ê°€ì ¸ì•¼ í•˜ë©°, ì¶œë ¥ ì¥ì¹˜ëŠ” ìµœì†Œ 1ê°œì˜ ì¶œë ¥ ì±„ë„ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.

```python lines
# ë‹¤ìŒ ì…€ ì„¤ì •ì„ ìœ„í•´ pyaudioì—ì„œ ì¥ì¹˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
p = pyaudio.PyAudio()
devices_data = {i: p.get_device_info_by_index(i) for i in range(p.get_device_count())}
for i, device in devices_data.items():
    print(
        f"Found device @{i}: {device['name']} with sample rate: {device['defaultSampleRate']} and input channels: {device['maxInputChannels']} and output channels: {device['maxOutputChannels']}"
    )
python
INPUT_DEVICE_INDEX = 3  # @param                                                 # ìœ„ ì¥ì¹˜ ëª©ë¡ì—ì„œ ì„ íƒí•˜ì„¸ìš”. ì…ë ¥ ì±„ë„ì´ 0ë³´ë‹¤ í° ì¥ì¹˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
OUTPUT_DEVICE_INDEX = 12  # @param                                                # ìœ„ ì¥ì¹˜ ëª©ë¡ì—ì„œ ì„ íƒí•˜ì„¸ìš”. ì¶œë ¥ ì±„ë„ì´ 0ë³´ë‹¤ í° ì¥ì¹˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
enable_audio_playback = True  # @param {type:"boolean"}                           # ì–´ì‹œìŠ¤í„´íŠ¸ ì˜¤ë””ì˜¤ ì¬ìƒ í† ê¸€. í—¤ë“œí° ì‚¬ìš© í•„ìˆ˜.

# ì˜¤ë””ì˜¤ ë…¹ìŒ ë° ìŠ¤íŠ¸ë¦¬ë° íŒŒë¼ë¯¸í„°
INPUT_DEVICE_CHANNELS = devices_data[INPUT_DEVICE_INDEX][
    "maxInputChannels"
]  # ìœ„ ì¥ì¹˜ ëª©ë¡ì—ì„œ ê°€ì ¸ì˜´
SAMPLE_RATE = int(
    devices_data[INPUT_DEVICE_INDEX]["defaultSampleRate"]
)  # ìœ„ ì¥ì¹˜ ëª©ë¡ì—ì„œ ê°€ì ¸ì˜´
CHUNK = int(SAMPLE_RATE / 10)  # í”„ë ˆì„ë‹¹ ìƒ˜í”Œ ìˆ˜
SAMPLE_WIDTH = p.get_sample_size(pyaudio.paInt16)  # í¬ë§·ì— ëŒ€í•œ í”„ë ˆì„ë‹¹ ìƒ˜í”Œ ë„ˆë¹„
CHUNK_DURATION = 0.3  # OAI APIë¡œ ì „ì†¡ë˜ëŠ” ì²­í¬ë‹¹ ì˜¤ë””ì˜¤ ì‹œê°„(ì´ˆ)
OAI_SAMPLE_RATE = (
    24000  # OAI ìƒ˜í”Œ ë ˆì´íŠ¸ëŠ” 24kHzì´ë©°, ì–´ì‹œìŠ¤í„´íŠ¸ ì˜¤ë””ì˜¤ ì¬ìƒ ë˜ëŠ” ì €ì¥ì— í•„ìš”í•©ë‹ˆë‹¤.
)
OUTPUT_DEVICE_CHANNELS = 1  # ëª¨ë…¸ ì¶œë ¥ì„ ìœ„í•´ 1ë¡œ ì„¤ì •
```

## OpenAI Realtime API ìŠ¤í‚¤ë§ˆ êµ¬í˜„

OpenAI Python SDKëŠ” ì•„ì§ Realtime APIë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê°€ë…ì„±ì„ ìœ„í•´ Pydanticìœ¼ë¡œ ì „ì²´ OAI Realtime API ìŠ¤í‚¤ë§ˆë¥¼ êµ¬í˜„í–ˆìœ¼ë©°, ê³µì‹ ì§€ì›ì´ ì¶œì‹œë˜ë©´ ì§€ì›ì´ ì¤‘ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<details>
<summary> OpenAI Realtime APIë¥¼ ìœ„í•œ Pydantic ìŠ¤í‚¤ë§ˆ (OpenAI SDKì˜ Realtime API ì§€ì› ë¶€ì¡± ë³´ì™„) </summary>

```python lines
from enum import Enum
from typing import Any, Literal, Union

from pydantic import BaseModel, Field, ValidationError

class BaseEvent(BaseModel):
    type: Union["ClientEventTypes", "ServerEventTypes"]
    event_id: Optional[str] = None  # ëª¨ë“  ì´ë²¤íŠ¸ì— ì„ íƒì  í•„ë“œë¡œ event_id ì¶”ê°€

    # def model_dump_json(self, *args, **kwargs):
    #     # Noneì´ ì•„ë‹Œ í•„ë“œë§Œ í¬í•¨
    #     return super().model_dump_json(*args, exclude_none=True, **kwargs)

class ChatMessage(BaseModel):
    role: Literal["user", "assistant"]
    content: str
    timestamp: float

""" í´ë¼ì´ì–¸íŠ¸ ì´ë²¤íŠ¸ """

class ClientEventTypes(str, Enum):
    SESSION_UPDATE = "session.update"
    CONVERSATION_ITEM_CREATE = "conversation.item.create"
    CONVERSATION_ITEM_TRUNCATE = "conversation.item.truncate"
    CONVERSATION_ITEM_DELETE = "conversation.item.delete"
    RESPONSE_CREATE = "response.create"
    RESPONSE_CANCEL = "response.cancel"
    INPUT_AUDIO_BUFFER_APPEND = "input_audio_buffer.append"
    INPUT_AUDIO_BUFFER_COMMIT = "input_audio_buffer.commit"
    INPUT_AUDIO_BUFFER_CLEAR = "input_audio_buffer.clear"
    ERROR = "error"

#### ì„¸ì…˜ ì—…ë°ì´íŠ¸
class TurnDetection(BaseModel):
    type: Literal["server_vad"]
    threshold: float = Field(..., ge=0.0, le=1.0)
    prefix_padding_ms: int
    silence_duration_ms: int

class InputAudioTranscription(BaseModel):
    model: Optional[str] = None

class ToolParameterProperty(BaseModel):
    type: str

class ToolParameter(BaseModel):
    type: str
    properties: dict[str, ToolParameterProperty]
    required: list[str]

class Tool(BaseModel):
    type: Literal["function", "code_interpreter", "file_search"]
    name: Optional[str] = None
    description: Optional[str] = None
    parameters: Optional[ToolParameter] = None

class Session(BaseModel):
    modalities: Optional[list[str]] = None
    instructions: Optional[str] = None
    voice: Optional[str] = None
    input_audio_format: Optional[str] = None
    output_audio_format: Optional[str] = None
    input_audio_transcription: Optional[InputAudioTranscription] = None
    turn_detection: Optional[TurnDetection] = None
    tools: Optional[list[Tool]] = None
    tool_choice: Optional[str] = None
    temperature: Optional[float] = None
    max_output_tokens: Optional[int] = None

class SessionUpdate(BaseEvent):
    type: Literal[ClientEventTypes.SESSION_UPDATE] = ClientEventTypes.SESSION_UPDATE
    session: Session

#### ì˜¤ë””ì˜¤ ë²„í¼
class InputAudioBufferAppend(BaseEvent):
    type: Literal[ClientEventTypes.INPUT_AUDIO_BUFFER_APPEND] = (
        ClientEventTypes.INPUT_AUDIO_BUFFER_APPEND
    )
    audio: str

class InputAudioBufferCommit(BaseEvent):
    type: Literal[ClientEventTypes.INPUT_AUDIO_BUFFER_COMMIT] = (
        ClientEventTypes.INPUT_AUDIO_BUFFER_COMMIT
    )

class InputAudioBufferClear(BaseEvent):
    type: Literal[ClientEventTypes.INPUT_AUDIO_BUFFER_CLEAR] = (
        ClientEventTypes.INPUT_AUDIO_BUFFER_CLEAR
    )

#### ë©”ì‹œì§€
class MessageContent(BaseModel):
    type: Literal["input_audio"]
    audio: str

class ConversationItemContent(BaseModel):
    type: Literal["input_text", "input_audio", "text", "audio"]
    text: Optional[str] = None
    audio: Optional[str] = None
    transcript: Optional[str] = None

class FunctionCallContent(BaseModel):
    call_id: str
    name: str
    arguments: str

class FunctionCallOutputContent(BaseModel):
    output: str

class ConversationItem(BaseModel):
    id: Optional[str] = None
    type: Literal["message", "function_call", "function_call_output"]
    status: Optional[Literal["completed", "in_progress", "incomplete"]] = None
    role: Literal["user", "assistant", "system"]
    content: list[
        Union[ConversationItemContent, FunctionCallContent, FunctionCallOutputContent]
    ]
    call_id: Optional[str] = None
    name: Optional[str] = None
    arguments: Optional[str] = None
    output: Optional[str] = None

class ConversationItemCreate(BaseEvent):
    type: Literal[ClientEventTypes.CONVERSATION_ITEM_CREATE] = (
        ClientEventTypes.CONVERSATION_ITEM_CREATE
    )
    item: ConversationItem

class ConversationItemTruncate(BaseEvent):
    type: Literal[ClientEventTypes.CONVERSATION_ITEM_TRUNCATE] = (
        ClientEventTypes.CONVERSATION_ITEM_TRUNCATE
    )
    item_id: str
    content_index: int
    audio_end_ms: int

class ConversationItemDelete(BaseEvent):
    type: Literal[ClientEventTypes.CONVERSATION_ITEM_DELETE] = (
        ClientEventTypes.CONVERSATION_ITEM_DELETE
    )
    item_id: str

#### ì‘ë‹µ
class ResponseCreate(BaseEvent):
    type: Literal[ClientEventTypes.RESPONSE_CREATE] = ClientEventTypes.RESPONSE_CREATE

class ResponseCancel(BaseEvent):
    type: Literal[ClientEventTypes.RESPONSE_CANCEL] = ClientEventTypes.RESPONSE_CANCEL

# ëª¨ë“  ì´ë²¤íŠ¸ ìœ í˜•ì„ í¬í•¨í•˜ë„ë¡ Event ìœ ë‹ˆì˜¨ ì—…ë°ì´íŠ¸
ClientEvent = Union[
    SessionUpdate,
    InputAudioBufferAppend,
    InputAudioBufferCommit,
    InputAudioBufferClear,
    ConversationItemCreate,
    ConversationItemTruncate,
    ConversationItemDelete,
    ResponseCreate,
    ResponseCancel,
]

""" ì„œë²„ ì´ë²¤íŠ¸ """

class ServerEventTypes(str, Enum):
    ERROR = "error"
    RESPONSE_AUDIO_TRANSCRIPT_DONE = "response.audio_transcript.done"
    RESPONSE_AUDIO_TRANSCRIPT_DELTA = "response.audio_transcript.delta"
    RESPONSE_AUDIO_DELTA = "response.audio.delta"
    SESSION_CREATED = "session.created"
    SESSION_UPDATED = "session.updated"
    CONVERSATION_CREATED = "conversation.created"
    INPUT_AUDIO_BUFFER_COMMITTED = "input_audio_buffer.committed"
    INPUT_AUDIO_BUFFER_CLEARED = "input_audio_buffer.cleared"
    INPUT_AUDIO_BUFFER_SPEECH_STARTED = "input_audio_buffer.speech_started"
    INPUT_AUDIO_BUFFER_SPEECH_STOPPED = "input_audio_buffer.speech_stopped"
    CONVERSATION_ITEM_CREATED = "conversation.item.created"
    CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED = (
        "conversation.item.input_audio_transcription.completed"
    )
    CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED = (
        "conversation.item.input_audio_transcription.failed"
    )
    CONVERSATION_ITEM_TRUNCATED = "conversation.item.truncated"
    CONVERSATION_ITEM_DELETED = "conversation.item.deleted"
    RESPONSE_CREATED = "response.created"
    RESPONSE_DONE = "response.done"
    RESPONSE_OUTPUT_ITEM_ADDED = "response.output_item.added"
    RESPONSE_OUTPUT_ITEM_DONE = "response.output_item.done"
    RESPONSE_CONTENT_PART_ADDED = "response.content_part.added"
    RESPONSE_CONTENT_PART_DONE = "response.content_part.done"
    RESPONSE_TEXT_DELTA = "response.text.delta"
    RESPONSE_TEXT_DONE = "response.text.done"
    RESPONSE_AUDIO_DONE = "response.audio.done"
    RESPONSE_FUNCTION_CALL_ARGUMENTS_DELTA = "response.function_call_arguments.delta"
    RESPONSE_FUNCTION_CALL_ARGUMENTS_DONE = "response.function_call_arguments.done"
    RATE_LIMITS_UPDATED = "rate_limits.updated"

#### ì˜¤ë¥˜
class ErrorDetails(BaseModel):
    type: Optional[str] = None
    code: Optional[str] = None
    message: Optional[str] = None
    param: Optional[str] = None

class ErrorEvent(BaseEvent):
    type: Literal[ServerEventTypes.ERROR] = ServerEventTypes.ERROR
    error: ErrorDetails

#### ì„¸ì…˜
class SessionCreated(BaseEvent):
    type: Literal[ServerEventTypes.SESSION_CREATED] = ServerEventTypes.SESSION_CREATED
    session: Session

class SessionUpdated(BaseEvent):
    type: Literal[ServerEventTypes.SESSION_UPDATED] = ServerEventTypes.SESSION_UPDATED
    session: Session

#### ëŒ€í™”
class Conversation(BaseModel):
    id: str
    object: Literal["realtime.conversation"]

class ConversationCreated(BaseEvent):
    type: Literal[ServerEventTypes.CONVERSATION_CREATED] = (
        ServerEventTypes.CONVERSATION_CREATED
    )
    conversation: Conversation

class ConversationItemCreated(BaseEvent):
    type: Literal[ServerEventTypes.CONVERSATION_ITEM_CREATED] = (
        ServerEventTypes.CONVERSATION_ITEM_CREATED
    )
    previous_item_id: Optional[str] = None
    item: ConversationItem

class ConversationItemInputAudioTranscriptionCompleted(BaseEvent):
    type: Literal[
        ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED
    ] = ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED
    item_id: str
    content_index: int
    transcript: str

class ConversationItemInputAudioTranscriptionFailed(BaseEvent):
    type: Literal[
        ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED
    ] = ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED
    item_id: str
    content_index: int
    error: dict[str, Any]

class ConversationItemTruncated(BaseEvent):
    type: Literal[ServerEventTypes.CONVERSATION_ITEM_TRUNCATED] = (
        ServerEventTypes.CONVERSATION_ITEM_TRUNCATED
    )
    item_id: str
    content_index: int
    audio_end_ms: int

class ConversationItemDeleted(BaseEvent):
    type: Literal[ServerEventTypes.CONVERSATION_ITEM_DELETED] = (
        ServerEventTypes.CONVERSATION_ITEM_DELETED
    )
    item_id: str

#### ì‘ë‹µ
class ResponseUsage(BaseModel):
    total_tokens: int
    input_tokens: int
    output_tokens: int
    input_token_details: Optional[dict[str, int]] = None
    output_token_details: Optional[dict[str, int]] = None

class ResponseOutput(BaseModel):
    id: str
    object: Literal["realtime.item"]
    type: str
    status: str
    role: str
    content: list[dict[str, Any]]

class ResponseContentPart(BaseModel):
    type: str
    text: Optional[str] = None

class ResponseOutputItemContent(BaseModel):
    type: str
    text: Optional[str] = None

class ResponseStatusDetails(BaseModel):
    type: str
    reason: str

class ResponseOutputItem(BaseModel):
    id: str
    object: Literal["realtime.item"]
    type: str
    status: str
    role: str
    content: list[ResponseOutputItemContent]

class Response(BaseModel):
    id: str
    object: Literal["realtime.response"]
    status: str
    status_details: Optional[ResponseStatusDetails] = None
    output: list[ResponseOutput]
    usage: Optional[ResponseUsage]

class ResponseCreated(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_CREATED] = ServerEventTypes.RESPONSE_CREATED
    response: Response

class ResponseDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_DONE] = ServerEventTypes.RESPONSE_DONE
    response: Response

class ResponseOutputItemAdded(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_OUTPUT_ITEM_ADDED] = (
        ServerEventTypes.RESPONSE_OUTPUT_ITEM_ADDED
    )
    response_id: str
    output_index: int
    item: ResponseOutputItem

class ResponseOutputItemDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_OUTPUT_ITEM_DONE] = (
        ServerEventTypes.RESPONSE_OUTPUT_ITEM_DONE
    )
    response_id: str
    output_index: int
    item: ResponseOutputItem

class ResponseContentPartAdded(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_CONTENT_PART_ADDED] = (
        ServerEventTypes.RESPONSE_CONTENT_PART_ADDED
    )
    response_id: str
    item_id: str
    output_index: int
    content_index: int
    part: ResponseContentPart

class ResponseContentPartDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_CONTENT_PART_DONE] = (
        ServerEventTypes.RESPONSE_CONTENT_PART_DONE
    )
    response_id: str
    item_id: str
    output_index: int
    content_index: int
    part: ResponseContentPart

#### ì‘ë‹µ í…ìŠ¤íŠ¸
class ResponseTextDelta(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_TEXT_DELTA] = (
        ServerEventTypes.RESPONSE_TEXT_DELTA
    )
    response_id: str
    item_id: str
    output_index: int
    content_index: int
    delta: str

class ResponseTextDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_TEXT_DONE] = (
        ServerEventTypes.RESPONSE_TEXT_DONE
    )
    response_id: str
    item_id: str
    output_index: int
    content_index: int
    text: str

#### ì‘ë‹µ ì˜¤ë””ì˜¤
class ResponseAudioTranscriptDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE] = (
        ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE
    )
    transcript: str

class ResponseAudioTranscriptDelta(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DELTA] = (
        ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DELTA
    )
    delta: str

class ResponseAudioDelta(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_AUDIO_DELTA] = (
        ServerEventTypes.RESPONSE_AUDIO_DELTA
    )
    response_id: str
    item_id: str
    delta: str

class ResponseAudioDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_AUDIO_DONE] = (
        ServerEventTypes.RESPONSE_AUDIO_DONE
    )
    response_id: str
    item_id: str
    output_index: int
    content_index: int

class InputAudioBufferCommitted(BaseEvent):
    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_COMMITTED] = (
        ServerEventTypes.INPUT_AUDIO_BUFFER_COMMITTED
    )
    previous_item_id: Optional[str] = None
    item_id: Optional[str] = None
    event_id: Optional[str] = None

class InputAudioBufferCleared(BaseEvent):
    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_CLEARED] = (
        ServerEventTypes.INPUT_AUDIO_BUFFER_CLEARED
    )

class InputAudioBufferSpeechStarted(BaseEvent):
    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STARTED] = (
        ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STARTED
    )
    audio_start_ms: int
    item_id: str

class InputAudioBufferSpeechStopped(BaseEvent):
    type: Literal[ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STOPPED] = (
        ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STOPPED
    )
    audio_end_ms: int
    item_id: str

#### í•¨ìˆ˜ í˜¸ì¶œ
class ResponseFunctionCallArgumentsDelta(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DELTA] = (
        ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DELTA
    )
    response_id: str
    item_id: str
    output_index: int
    call_id: str
    delta: str

class ResponseFunctionCallArgumentsDone(BaseEvent):
    type: Literal[ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DONE] = (
        ServerEventTypes.RESPONSE_FUNCTION_CALL_ARGUMENTS_DONE
    )
    response_id: str
    item_id: str
    output_index: int
    call_id: str
    arguments: str

#### ì†ë„ ì œí•œ (Rate Limits)
class RateLimit(BaseModel):
    name: str
    limit: int
    remaining: int
    reset_seconds: float

class RateLimitsUpdated(BaseEvent):
    type: Literal[ServerEventTypes.RATE_LIMITS_UPDATED] = (
        ServerEventTypes.RATE_LIMITS_UPDATED
    )
    rate_limits: list[RateLimit]

ServerEvent = Union[
    ErrorEvent,
    ConversationCreated,
    ResponseAudioTranscriptDone,
    ResponseAudioTranscriptDelta,
    ResponseAudioDelta,
    ResponseCreated,
    ResponseDone,
    ResponseOutputItemAdded,
    ResponseOutputItemDone,
    ResponseContentPartAdded,
    ResponseContentPartDone,
    ResponseTextDelta,
    ResponseTextDone,
    ResponseAudioDone,
    ConversationItemInputAudioTranscriptionCompleted,
    SessionCreated,
    SessionUpdated,
    InputAudioBufferCleared,
    InputAudioBufferSpeechStarted,
    InputAudioBufferSpeechStopped,
    ConversationItemCreated,
    ConversationItemInputAudioTranscriptionFailed,
    ConversationItemTruncated,
    ConversationItemDeleted,
    RateLimitsUpdated,
]

EVENT_TYPE_TO_MODEL = {
    ServerEventTypes.ERROR: ErrorEvent,
    ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE: ResponseAudioTranscriptDone,
    ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DELTA: ResponseAudioTranscriptDelta,
    ServerEventTypes.RESPONSE_AUDIO_DELTA: ResponseAudioDelta,
    ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED: ConversationItemInputAudioTranscriptionCompleted,
    ServerEventTypes.SESSION_CREATED: SessionCreated,
    ServerEventTypes.SESSION_UPDATED: SessionUpdated,
    ServerEventTypes.CONVERSATION_CREATED: ConversationCreated,
    ServerEventTypes.INPUT_AUDIO_BUFFER_COMMITTED: InputAudioBufferCommitted,
    ServerEventTypes.INPUT_AUDIO_BUFFER_CLEARED: InputAudioBufferCleared,
    ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STARTED: InputAudioBufferSpeechStarted,
    ServerEventTypes.INPUT_AUDIO_BUFFER_SPEECH_STOPPED: InputAudioBufferSpeechStopped,
    ServerEventTypes.CONVERSATION_ITEM_CREATED: ConversationItemCreated,
    ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_FAILED: ConversationItemInputAudioTranscriptionFailed,
    ServerEventTypes.CONVERSATION_ITEM_TRUNCATED: ConversationItemTruncated,
    ServerEventTypes.CONVERSATION_ITEM_DELETED: ConversationItemDeleted,
    ServerEventTypes.RESPONSE_CREATED: ResponseCreated,
    ServerEventTypes.RESPONSE_DONE: ResponseDone,
    ServerEventTypes.RESPONSE_OUTPUT_ITEM_ADDED: ResponseOutputItemAdded,
    ServerEventTypes.RESPONSE_OUTPUT_ITEM_DONE: ResponseOutputItemDone,
    ServerEventTypes.RESPONSE_CONTENT_PART_ADDED: ResponseContentPartAdded,
    ServerEventTypes.RESPONSE_CONTENT_PART_DONE: ResponseContentPartDone,
    ServerEventTypes.RESPONSE_TEXT_DELTA: ResponseTextDelta,
    ServerEventTypes.RESPONSE_TEXT_DONE: ResponseTextDone,
    ServerEventTypes.RESPONSE_AUDIO_DONE: ResponseAudioDone,
    ServerEventTypes.RATE_LIMITS_UPDATED: RateLimitsUpdated,
}

def parse_server_event(event_data: dict) -> ServerEvent:
    event_type = event_data.get("type")
    if not event_type:
        raise ValueError("Event data is missing 'type' field")

    model_class = EVENT_TYPE_TO_MODEL.get(event_type)
    if not model_class:
        raise ValueError(f"Unknown event type: {event_type}")

    try:
        return model_class(**event_data)
    except ValidationError as e:
        raise ValueError(f"Failed to parse event of type {event_type}: {str(e)}") from e
```

</details>

## ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ë¼ì´í„° (ë””ìŠ¤í¬ ë° ë©”ëª¨ë¦¬)

```python lines
class StreamingWavWriter:
    """ì˜¤ë””ì˜¤ ì •ìˆ˜ ë˜ëŠ” ë°”ì´íŠ¸ ë°°ì—´ ì²­í¬ë¥¼ WAV íŒŒì¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤."""

    wav_file = None
    buffer = None
    in_memory = False

    def __init__(
        self,
        filename=None,
        channels=INPUT_DEVICE_CHANNELS,
        sample_width=SAMPLE_WIDTH,
        framerate=SAMPLE_RATE,
    ):
        self.in_memory = filename is None
        if self.in_memory:
            self.buffer = io.BytesIO()
            self.wav_file = wave.open(self.buffer, "wb")
        else:
            self.wav_file = wave.open(filename, "wb")

        self.wav_file.setnchannels(channels)
        self.wav_file.setsampwidth(sample_width)
        self.wav_file.setframerate(framerate)

    def append_int16_chunk(self, int16_data):
        if int16_data is not None:
            self.wav_file.writeframes(
                int16_data.tobytes()
                if isinstance(int16_data, np.ndarray)
                else int16_data
            )

    def close(self):
        self.wav_file.close()

    def get_wav_buffer(self):
        assert self.in_memory, "ë²„í¼ëŠ” ìŠ¤íŠ¸ë¦¼ì´ ë©”ëª¨ë¦¬ì— ìˆì„ ë•Œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        return self.buffer
```

## Realtime Audio ëª¨ë¸

Realtime(RT) ì˜¤ë””ì˜¤ ëª¨ë¸ì€ ì›¹ì†Œì¼“ì„ ì‚¬ìš©í•˜ì—¬ OpenAIì˜ Realtime ì˜¤ë””ì˜¤ APIì— ì´ë²¤íŠ¸ë¥¼ ë³´ëƒ…ë‹ˆë‹¤. ì‘ë™ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1.  **init:** ë¡œì»¬ ë²„í¼(ì…ë ¥ ì˜¤ë””ì˜¤)ì™€ ìŠ¤íŠ¸ë¦¼(ì–´ì‹œìŠ¤í„´íŠ¸ ì¬ìƒ ìŠ¤íŠ¸ë¦¼, ì‚¬ìš©ì ì˜¤ë””ì˜¤ ë””ìŠ¤í¬ ë¼ì´í„° ìŠ¤íŠ¸ë¦¼)ì„ ì´ˆê¸°í™”í•˜ê³  Realtime APIì— ëŒ€í•œ ì—°ê²°ì„ ì—½ë‹ˆë‹¤.
2.  **receive_messages_thread**: APIë¡œë¶€í„° ë©”ì‹œì§€ë¥¼ ìˆ˜ì‹ í•˜ëŠ” ìŠ¤ë ˆë“œë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤. ë„¤ ê°€ì§€ ì£¼ìš” ì´ë²¤íŠ¸ ìœ í˜•ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤:
        - RESPONSE_AUDIO_TRANSCRIPT_DONE:
            ì„œë²„ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µì´ ì™„ë£Œë˜ì—ˆìŒì„ ì•Œë¦¬ê³  ëŒ€ë³¸(transcript)ì„ ì œê³µí•©ë‹ˆë‹¤.
        - CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED:
            ì„œë²„ê°€ ì‚¬ìš©ìì˜ ì˜¤ë””ì˜¤ê°€ ì „ì‚¬(transcription)ë˜ì—ˆìŒì„ ì•Œë¦¬ê³  ì‚¬ìš©ì ì˜¤ë””ì˜¤ì˜ ëŒ€ë³¸ì„ ë³´ëƒ…ë‹ˆë‹¤. ì´ ëŒ€ë³¸ì„ Weaveì— ë¡œê·¸í•˜ê³  ì‚¬ìš©ìì—ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤.
        - RESPONSE_AUDIO_DELTA:
            ì„œë²„ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì˜¤ë””ì˜¤ì˜ ìƒˆë¡œìš´ ì²­í¬ë¥¼ ë³´ëƒ…ë‹ˆë‹¤. ì´ë¥¼ ì‘ë‹µ IDë¥¼ í†µí•´ ì§„í–‰ ì¤‘ì¸ ì‘ë‹µ ë°ì´í„°ì— ì¶”ê°€í•˜ê³  ì¬ìƒì„ ìœ„í•´ ì¶œë ¥ ìŠ¤íŠ¸ë¦¼ì— ë”í•©ë‹ˆë‹¤.
        - RESPONSE_DONE:
            ì„œë²„ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µì˜ ì™„ë£Œë¥¼ ì•Œë¦½ë‹ˆë‹¤. í•´ë‹¹ ì‘ë‹µê³¼ ê´€ë ¨ëœ ëª¨ë“  ì˜¤ë””ì˜¤ ì²­í¬ ë° ëŒ€ë³¸ì„ ê°€ì ¸ì™€ Weaveì— ë¡œê·¸í•©ë‹ˆë‹¤.
3.  **send_audio**: í•¸ë“¤ëŸ¬ê°€ ì‚¬ìš©ì ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ë²„í¼ì— ì¶”ê°€í•˜ê³ , ì˜¤ë””ì˜¤ ë²„í¼ê°€ ì¼ì • í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.

```python lines
class RTAudioModel(weave.Model):
    """Whisper ì‚¬ìš©ì ì „ì‚¬ ë¡œê¹… ê¸°ëŠ¥ì´ í¬í•¨ëœ ì‹¤ì‹œê°„ e2e ì˜¤ë””ì˜¤ OpenAI ëª¨ë¸ ìƒí˜¸ì‘ìš©ì„ ìœ„í•œ ëª¨ë¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""

    realtime_model_name: str = "gpt-4o-realtime-preview-2024-10-01"  # ì‹¤ì‹œê°„ e2e ì˜¤ë””ì˜¤ ì „ìš© ëª¨ë¸

    stop_event: Optional[threading.Event] = threading.Event()  # ëª¨ë¸ ì¤‘ì§€ ì´ë²¤íŠ¸
    ws: Optional[websocket.WebSocket] = None  # OpenAI í†µì‹ ìš© ì›¹ì†Œì¼“

    user_wav_writer: Optional[StreamingWavWriter] = (
        None  # ì‚¬ìš©ì ì¶œë ¥ì„ íŒŒì¼ë¡œ ì“°ê¸° ìœ„í•œ ìŠ¤íŠ¸ë¦¼
    )
    input_audio_buffer: Optional[np.ndarray] = None  # ì‚¬ìš©ì ì˜¤ë””ì˜¤ ì²­í¬ìš© ë²„í¼
    assistant_outputs: dict[str, StreamingWavWriter] = (
        None  # Weaveë¡œ ì „ì†¡í•˜ê¸° ìœ„í•´ ì§‘ê³„ëœ ì–´ì‹œìŠ¤í„´íŠ¸ ì¶œë ¥
    )
    playback_stream: Optional[pyaudio.Stream] = (
        None  # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì¬ìƒì„ ìœ„í•œ ì¬ìƒ ìŠ¤íŠ¸ë¦¼
    )

    def __init__(self):
        super().__init__()
        self.stop_event.clear()
        self.user_wav_writer = StreamingWavWriter(
            filename="user_audio.wav", framerate=SAMPLE_RATE
        )
        self.input_audio_buffer = np.array([], dtype=np.int16)
        self.ws = websocket.WebSocket()
        self.assistant_outputs = {}

        # ì¬ìƒì´ í™œì„±í™”ëœ ê²½ìš° ì–´ì‹œìŠ¤í„´íŠ¸ ì˜¤ë””ì˜¤ ì¬ìƒ ìŠ¤íŠ¸ë¦¼ ì—´ê¸°
        if enable_audio_playback:
            self.playback_stream = pyaudio.PyAudio().open(
                format=pyaudio.paInt16,
                channels=OUTPUT_DEVICE_CHANNELS,
                rate=OAI_SAMPLE_RATE,
                output=True,
                output_device_index=OUTPUT_DEVICE_INDEX,
            )

        # ì›¹ì†Œì¼“ ì—°ê²°
        try:
            self.ws.connect(
                f"wss://api.openai.com/v1/realtime?model={self.realtime_model_name}",
                header={
                    "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}",
                    "OpenAI-Beta": "realtime=v1",
                },
            )

            # ì„¤ì • ë©”ì‹œì§€ ì „ì†¡
            config_event = SessionUpdate(
                session=Session(
                    modalities=["text", "audio"],  # ì‚¬ìš©í•  ëª¨ë‹¬ë¦¬í‹°
                    input_audio_transcription=InputAudioTranscription(
                        model="whisper-1"
                    ),  # ì „ì‚¬ìš© whisper-1
                    turn_detection=TurnDetection(
                        type="server_vad",
                        threshold=0.3,
                        prefix_padding_ms=300,
                        silence_duration_ms=600,
                    ),  # ë¬´ìŒ ê°ì§€ë¥¼ ìœ„í•œ ì„œë²„ VAD
                )
            )
            self.ws.send(config_event.model_dump_json(exclude_none=True))
            self.log_ws_message(config_event.model_dump_json(exclude_none=True), "Sent")

            # ë¦¬ìŠ¤ë„ˆ ì‹œì‘
            websocket_thread = threading.Thread(target=self.receive_messages_thread)
            websocket_thread.daemon = True
            websocket_thread.start()

        except Exception as e:
            print(f"Error connecting to WebSocket: {e}")

    ##### Weave ì¸í…Œê·¸ë ˆì´ì…˜ ë° ë©”ì‹œì§€ í•¸ë“¤ëŸ¬ #####
    def handle_assistant_response_audio_delta(self, data: ResponseAudioDelta):
        if data.response_id not in self.assistant_outputs:
            self.assistant_outputs[data.response_id] = StreamingWavWriter(
                framerate=OAI_SAMPLE_RATE
            )

        data_bytes = base64.b64decode(data.delta)
        self.assistant_outputs[data.response_id].append_int16_chunk(data_bytes)

        if enable_audio_playback:
            self.playback_stream.write(data_bytes)

        return {"assistant_audio": data_bytes}

    @weave.op()
    def handle_assistant_response_done(self, data: ResponseDone):
        wave_file_stream = self.assistant_outputs[data.response.id]
        wave_file_stream.close()
        wave_file_stream.buffer.seek(0)
        weave_payload = {
            "assistant_audio": wave.open(wave_file_stream.get_wav_buffer(), "rb"),
            "assistant_transcript": data.response.output[0]
            .content[0]
            .get("transcript", "Transcript Unavailable."),
        }
        return weave_payload

    @weave.op()
    def handle_user_transcription_done(
        self, data: ConversationItemInputAudioTranscriptionCompleted
    ):
        return {"user_transcript": data.transcript}

    ##### ë©”ì‹œì§€ ìˆ˜ì‹  ë° ì†¡ì‹  #####
    def receive_messages_thread(self):
        while not self.stop_event.is_set():
            try:
                data = json.loads(self.ws.recv())
                self.log_ws_message(json.dumps(data, indent=2))

                parsed_event = parse_server_event(data)

                if parsed_event.type == ServerEventTypes.RESPONSE_AUDIO_TRANSCRIPT_DONE:
                    print("Assistant: ", parsed_event.transcript)
                elif (
                    parsed_event.type
                    == ServerEventTypes.CONVERSATION_ITEM_INPUT_AUDIO_TRANSCRIPTION_COMPLETED
                ):
                    print("User: ", parsed_event.transcript)
                    self.handle_user_transcription_done(parsed_event)
                elif parsed_event.type == ServerEventTypes.RESPONSE_AUDIO_DELTA:
                    self.handle_assistant_response_audio_delta(parsed_event)
                elif parsed_event.type == ServerEventTypes.RESPONSE_DONE:
                    self.handle_assistant_response_done(parsed_event)
                elif parsed_event.type == ServerEventTypes.ERROR:
                    print(
                        f"\nError from server: {parsed_event.error.model_dump_json(exclude_none=True)}"
                    )
            except websocket.WebSocketConnectionClosedException:
                print("\nWebSocket connection closed")
                break
            except json.JSONDecodeError:
                continue
            except Exception as e:
                print(f"\nError in receive_messages: {e}")
                break

    def send_audio(self, audio_chunk):
        if self.ws and self.ws.connected:
            self.input_audio_buffer = np.append(
                self.input_audio_buffer, np.frombuffer(audio_chunk, dtype=np.int16)
            )
            if len(self.input_audio_buffer) >= SAMPLE_RATE * CHUNK_DURATION:
                try:
                    # ì˜¤ë””ì˜¤ë¥¼ OAI ìƒ˜í”Œ ë ˆì´íŠ¸ë¡œ ë¦¬ìƒ˜í”Œë§
                    resampled_audio = (
                        resampy.resample(
                            self.input_audio_buffer, SAMPLE_RATE, OAI_SAMPLE_RATE
                        )
                        if SAMPLE_RATE != OAI_SAMPLE_RATE
                        else self.input_audio_buffer
                    )

                    # ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ OAI APIë¡œ ì „ì†¡
                    audio_event = InputAudioBufferAppend(
                        audio=base64.b64encode(
                            resampled_audio.astype(np.int16).tobytes()
                        ).decode("utf-8")  # ì˜¤ë””ì˜¤ ë°°ì—´ì„ b64 ë°”ì´íŠ¸ë¡œ ë³€í™˜
                    )
                    self.ws.send(audio_event.model_dump_json(exclude_none=True))
                    self.log_ws_message(
                        audio_event.model_dump_json(exclude_none=True), "Sent"
                    )
                finally:
                    self.user_wav_writer.append_int16_chunk(self.input_audio_buffer)

                    # ì˜¤ë””ì˜¤ ë²„í¼ ë¹„ìš°ê¸°
                    self.input_audio_buffer = np.array([], dtype=np.int16)
        else:
            print("Error sending audio: websocket not initialized.")

    ##### ì¼ë°˜ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ #####
    def log_ws_message(self, message, direction="Received"):
        with open("websocket_log.txt", "a") as log_file:
            log_file.write(
                f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {direction}: {message}\n"
            )

    def stop(self):
        self.stop_event.set()

        if self.ws:
            self.ws.close()

        self.user_wav_writer.close()
```

## ì˜¤ë””ì˜¤ ë ˆì½”ë”

RTAudio ëª¨ë¸ì˜ `send_audio` ë©”ì†Œë“œì— ì—°ê²°ëœ í•¸ë“¤ëŸ¬ì™€ í•¨ê»˜ pyaudio ì…ë ¥ ìŠ¤íŠ¸ë¦¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ì•ˆì „í•˜ê²Œ ë¹ ì ¸ë‚˜ì˜¬ ìˆ˜ ìˆë„ë¡ ìŠ¤íŠ¸ë¦¼ì„ ë©”ì¸ ìŠ¤ë ˆë“œì— ë°˜í™˜í•©ë‹ˆë‹¤.

```python lines
# ì˜¤ë””ì˜¤ ìº¡ì²˜ ìŠ¤íŠ¸ë¦¼
def record_audio(realtime_model: RTAudioModel) -> pyaudio.Stream:
    """Pyaudio ì…ë ¥ ìŠ¤íŠ¸ë¦¼ì„ ì„¤ì •í•˜ê³  ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì½œë°±ìœ¼ë¡œ RTAudioModelì„ ì‚¬ìš©í•©ë‹ˆë‹¤."""

    def audio_callback(in_data, frame_count, time_info, status):
        realtime_model.send_audio(in_data)
        return (None, pyaudio.paContinue)

    p = pyaudio.PyAudio()
    stream = p.open(
        format=pyaudio.paInt16,
        channels=INPUT_DEVICE_CHANNELS,
        rate=SAMPLE_RATE,
        input=True,
        input_device_index=INPUT_DEVICE_INDEX,
        frames_per_buffer=CHUNK,
        stream_callback=audio_callback,
    )
    stream.start_stream()

    print("Recording started. Please begin speaking to your personal assistant...")
    return stream
```

## ë©”ì¸ ìŠ¤ë ˆë“œ (ì‹¤í–‰!)

ë©”ì¸ ìŠ¤ë ˆë“œëŠ” Weaveê°€ í†µí•©ëœ Realtime Audio ëª¨ë¸ì„ ì‹œì‘í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ë…¹ìŒì´ ì‹œì‘ë˜ê³  ì‚¬ìš©ìì˜ í‚¤ë³´ë“œ ì¸í„°ëŸ½íŠ¸(keyboard interrupt)ë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.

```python lines
weave.init(project_name="realtime-oai-audio-testing")

realtime_model = RTAudioModel()

if realtime_model.ws and realtime_model.ws.connected:
    recording_stream: pyaudio.Stream = record_audio(realtime_model)

    try:
        while not realtime_model.stop_event.is_set():
            time.sleep(1)
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print(f"Error in main loop: {e}")
        import traceback

        traceback.print_exc()
    finally:
        print("Exiting...")
        realtime_model.stop()
        if recording_stream and recording_stream.is_active():
            recording_stream.stop_stream()
            recording_stream.close()
else:
    print(
        "WebSocket connection failed. Please check your API key and internet connection."
    )
```

</details>