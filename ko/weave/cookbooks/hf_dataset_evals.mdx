---
title: "HuggingFace 데이터셋 Evaluation"
description: "W&B Weave와 함께 HuggingFace 데이터셋 Evaluation을 사용하는 방법을 알아보세요"
---

<Note>
이 페이지는 대화형 노트북입니다. 로컬에서 실행하거나 아래 링크를 사용하세요:
- [Google Colab에서 열기](https://colab.research.google.com/github/wandb/docs/blob/main/weave/cookbooks/source/hf_dataset_evals.ipynb)
- [GitHub에서 소스 보기](https://github.com/wandb/docs/blob/main/weave/cookbooks/source/hf_dataset_evals.ipynb)
</Note>

<div id="using-huggingface-datasets-in-evaluations-with-preprocess_model_input">
  # Evaluation에서 `preprocess_model_input`과 함께 HuggingFace Datasets 사용하기
</div>

<div id="note-this-is-a-temporary-workaround">
  ## 참고: 이것은 임시 해결책입니다
</div>

> 이 가이드는 HuggingFace Datasets를 Weave Evaluation과 함께 사용하기 위한 임시 해결책을 보여줍니다.<br /><br/>
우리는 이 과정을 더 간단하게 만들어 줄, 보다 매끄러운 통합을 적극적으로 개발하고 있습니다.\
> 이 방법으로도 사용할 수 있지만, 가까운 시일 내에 외부 데이터셋을 더 쉽게 다룰 수 있도록 개선과 업데이트가 이루어질 예정입니다.

<div id="setup-and-imports">
  ## 설정 및 임포트
</div>

먼저 Weave를 초기화하고 실험을 추적하기 위해 Weights &amp; Biases에 연결합니다.

```python lines
!pip install datasets wandb weave
python
# 변수 초기화
HUGGINGFACE_DATASET = "wandb/ragbench-test-sample"
WANDB_KEY = ""
WEAVE_TEAM = ""
WEAVE_PROJECT = ""

# weave 및 필요한 라이브러리 초기화
import asyncio

import nest_asyncio
import wandb
from datasets import load_dataset

import weave
from weave import Evaluation

# wandb에 로그인하고 weave 초기화
wandb.login(key=WANDB_KEY)
client = weave.init(f"{WEAVE_TEAM}/{WEAVE_PROJECT}")

# 중첩 이벤트 루프를 허용하기 위해 nest_asyncio 적용 (일부 노트북 환경에서 필요)
nest_asyncio.apply()
```


<div id="load-and-prepare-huggingface-dataset">
  ## HuggingFace 데이터셋 로드 및 준비
</div>

* HuggingFace 데이터셋을 로드합니다.
* 데이터셋 행을 참조하기 위한 인덱스 매핑을 생성합니다.
* 이 인덱싱 방식을 통해 원본 데이터셋에 대한 참조를 유지할 수 있습니다.

> **참고:**<br />
> 인덱스에는 각 행이 고유한 식별자를 갖도록 `hf_hub_name`과 `hf_id`를 함께 인코딩합니다.
> 이 고유 다이제스트 값은 Evaluation 실행 중 특정 데이터셋 항목을 추적하고 참조하는 데 사용됩니다.

```python lines
# HuggingFace 데이터셋 로드
ds = load_dataset(HUGGINGFACE_DATASET)
row_count = ds["train"].num_rows

# 데이터셋에 대한 인덱스 매핑 생성
# HF 데이터셋 인덱스를 포함하는 딕셔너리 목록을 생성합니다
# 예시: [{"hf_id": 0}, {"hf_id": 1}, {"hf_id": 2}, ...]
hf_index = [{"hf_id": i, "hf_hub_name": HUGGINGFACE_DATASET} for i in range(row_count)]
```


<div id="define-processing-and-evaluation-functions">
  ## 처리 및 Evaluation 함수 정의하기
</div>

<div id="processing-pipeline">
  ### 처리 파이프라인
</div>

* `preprocess_example`: 인덱스 참조를 평가에 필요한 실제 데이터로 변환합니다
* `hf_eval`: 모델 출력에 점수를 매기는 방법을 정의합니다
* `function_to_evaluate`: 실제로 평가할 함수/모델입니다

```python lines
@weave.op()
def preprocess_example(example):
    """
    평가 전에 각 예제를 전처리합니다.
    Args:
        example: hf_id를 포함하는 Dict
    Returns:
        HF 데이터셋의 프롬프트를 포함하는 Dict
    """
    hf_row = ds["train"][example["hf_id"]]
    return {"prompt": hf_row["question"], "answer": hf_row["response"]}

@weave.op()
def hf_eval(hf_id: int, output: dict) -> dict:
    """
    모델 출력을 평가하기 위한 채점 함수입니다.
    Args:
        hf_id: HF 데이터셋의 인덱스
        output: 평가할 모델의 출력
    Returns:
        평가 점수를 포함하는 Dict
    """
    hf_row = ds["train"][hf_id]
    return {"scorer_value": True}

@weave.op()
def function_to_evaluate(prompt: str):
    """
    평가될 함수입니다 (예: 모델 또는 파이프라인).
    Args:
        prompt: 데이터셋의 입력 프롬프트
    Returns:
        모델 출력을 포함하는 Dict
    """
    return {"generated_text": "testing "}
```


<div id="create-and-run-evaluation">
  ### Evaluation 생성 및 실행
</div>

* hf&#95;index의 각 인덱스에 대해:
  1. `preprocess_example`이 HF 데이터셋에서 해당 데이터를 가져옵니다.
  2. 전처리된 데이터가 `function_to_evaluate`에 전달됩니다.
  3. 출력은 `hf_eval`을 사용해 점수화됩니다.
  4. 결과는 Weave에서 추적됩니다.

```python lines
# 평가 객체 생성
evaluation = Evaluation(
    dataset=hf_index,  # 인덱스 매핑 사용
    scorers=[hf_eval],  # 채점 함수 목록
    preprocess_model_input=preprocess_example,  # 입력 전처리 함수
)

# 비동기로 평가 실행
async def main():
    await evaluation.evaluate(function_to_evaluate)

asyncio.run(main())
```
