---
title: Serverless RL
description: 강화 학습을 사용해 모델을 더 효율적으로 사후 학습(post-train)하는 방법 알아보기
---

현재 퍼블릭 프리뷰로 제공되는 Serverless RL은 개발자가 LLM을 사후 학습해 새로운 동작을 익히게 하고, 멀티턴 에이전트 태스크를 수행할 때 신뢰성, 속도, 비용을 개선할 수 있도록 돕습니다. W&amp;B가 학습 인프라를 ([CoreWeave](https://docs.coreweave.com/docs/platform) 위에) 프로비저닝해 주면서도, 사용자의 환경 설정에 대한 완전한 유연성을 제공합니다. Serverless RL을 사용하면 탄력적으로 자동 확장되어 수십 개의 GPU까지 스케일링되는 관리형 학습 클러스터를 즉시 사용할 수 있습니다. RL 워크플로를 추론 단계와 학습 단계로 분리하고 이를 여러 잡(job)에 멀티플렉싱함으로써, Serverless RL은 GPU 활용도를 높이고 학습 시간과 비용을 줄여 줍니다.

Serverless RL은 다음과 같은 작업에 적합합니다:

* 음성 에이전트
* 심층 리서치 어시스턴트
* 온프레미스 모델
* 콘텐츠 마케팅 분석 에이전트

Serverless RL은 로우랭크 어댑터(LoRA)를 학습해, 에이전트의 구체적인 태스크에 맞게 모델을 특화합니다. 이를 통해 원래 모델의 기능을 실제 운영 환경에서의 경험으로 확장할 수 있습니다. 사용자가 학습한 LoRA는 W&amp;B 계정에 아티팩트로 자동 저장되며, 로컬이나 서드 파티 위치에 백업용으로 저장할 수 있습니다. Serverless RL을 통해 학습한 모델은 W&amp;B Inference에 자동으로 호스팅됩니다.

시작하려면 ART [빠른 시작](https://art.openpipe.ai/getting-started/quick-start) 또는 [Google Colab 노트북](https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb)을 참고하세요.

<div id="why-serverless-rl">
  ## 왜 Serverless RL인가?
</div>

강화학습(RL)은 직접 소유하거나 임대해 사용하는 GPU를 포함해, 다양한 학습 환경에서 활용할 수 있는 강력한 학습 기법들의 집합입니다. Serverless RL은 RL 후속 학습(post-training) 단계에서 다음과 같은 이점을 제공합니다:

* **더 낮은 학습 비용**: 여러 사용자가 인프라를 함께 공유해 효율적으로 활용하고, 각 작업마다 별도로 환경을 설정할 필요가 없으며, 학습을 수행하지 않을 때 GPU 비용을 0으로 줄일 수 있어, Serverless RL은 학습 비용을 크게 절감합니다.
* **더 짧은 학습 시간**: 추론 요청을 여러 GPU에 분산하고, 필요한 시점에 즉시 학습 인프라를 프로비저닝함으로써, Serverless RL은 학습 작업을 가속화해 더 빠르게 반복 실험할 수 있게 해줍니다.
* **자동 배포**: Serverless RL은 학습된 모든 체크포인트를 자동으로 배포하므로, 호스팅 인프라를 수동으로 설정할 필요가 없습니다. 학습된 모델은 로컬, 스테이징, 프로덕션 환경에서 즉시 접근하고 테스트할 수 있습니다.

<div id="how-serverless-rl-uses-wb-services">
  ## Serverless RL이 W&amp;B 서비스를 사용하는 방식
</div>

Serverless RL은 운영을 위해 다음과 같은 W&amp;B 구성 요소를 함께 사용합니다:

* [Inference](/ko/inference): 모델 추론을 실행하기 위해
* [Models](/ko/models): LoRA 어댑터 학습 중 성능 지표를 추적하기 위해
* [Artifacts](/ko/models/artifacts): LoRA 어댑터를 저장하고 버전 관리를 하기 위해
* [Weave (optional)](/ko/weave): 학습 루프의 각 단계에서 모델이 어떻게 응답하는지에 대한 가시성을 확보하기 위해

Serverless RL은 현재 퍼블릭 프리뷰 단계입니다. 프리뷰 기간 동안에는 추론 사용과 아티팩트 저장에 대해서만 요금이 청구됩니다. 프리뷰 기간 동안 어댑터 학습에 대해서는 W&amp;B에서 요금을 청구하지 않습니다.