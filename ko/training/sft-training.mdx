---
title: Serverless SFT
description: W&B에서 지도형 파인튜닝(SFT)을 사용해 모델을 파인튜닝하는 방법 알아보기
---

현재 퍼블릭 프리뷰 상태인 Serverless SFT는 개발자가 정제된 데이터셋에 대해 지도 학습을 사용하여 LLM을 파인튜닝할 수 있도록 도와줍니다. W&amp;B가 대신 [CoreWeave 상에서](https://docs.coreweave.com/docs/platform) 트레이닝 인프라를 프로비저닝하면서도, 사용자의 환경 구성에는 완전한 유연성을 제공합니다. Serverless SFT를 사용하면 트레이닝 워크로드를 처리하기 위해 탄력적으로 자동 확장되는 관리형 트레이닝 클러스터에 즉시 액세스할 수 있습니다.

Serverless SFT는 다음과 같은 작업에 특히 적합합니다.

* **Distillation**: 더 크고 성능이 높은 모델의 지식을 더 작고 빠른 모델로 전이
* **출력 스타일 및 형식 학습**: 특정 응답 형식, 톤, 구조를 따르도록 모델을 트레이닝
* **RL 이전 워밍업**: 추가적인 정제를 위해 강화 학습을 적용하기 전에 지도 학습 예제로 모델을 사전 트레이닝

Serverless SFT는 저랭크 어댑터(LoRA)를 트레이닝하여 특정 작업에 맞게 모델을 특화합니다. 트레이닝한 LoRA는 W&amp;B 계정의 Artifacts에 자동으로 저장되며, 로컬이나 서드파티 위치에 백업용으로 저장할 수도 있습니다. Serverless SFT를 통해 트레이닝한 모델은 W&amp;B Inference에서도 자동으로 호스팅됩니다.

시작하려면 ART의 [Serverless SFT 문서](https://art.openpipe.ai/fundamentals/sft-training)를 참조하세요.

<div id="why-serverless-sft">
  ## 왜 Serverless SFT인가?
</div>

Supervised fine-tuning(SFT)은 모델이 선별된 입출력 예시들로부터 학습하는 트레이닝 기법입니다. W&amp;B의 Serverless SFT는 다음과 같은 이점을 제공합니다:

* **더 낮은 트레이닝 비용**: 다수의 사용자 간에 공용 인프라를 멀티플렉싱하여 공유하고, 각 작업마다 별도의 설정 과정을 건너뛰며, 트레이닝을 수행하지 않을 때 GPU 비용을 0까지 줄임으로써, Serverless SFT는 트레이닝 비용을 크게 절감합니다.
* **더 빠른 트레이닝 시간**: 필요한 시점에 즉시 트레이닝 인프라를 프로비저닝함으로써, Serverless SFT는 트레이닝 작업을 가속하고 더 빠르게 반복 실험을 할 수 있게 해줍니다.
* **자동 배포**: Serverless SFT는 트레이닝한 모든 체크포인트를 자동으로 배포하여, 호스팅 인프라를 수동으로 구성할 필요를 없애줍니다. 트레이닝된 모델은 로컬, 스테이징, 프로덕션 환경에서 즉시 접근하고 테스트할 수 있습니다.

<div id="how-serverless-sft-uses-wb-services">
  ## Serverless SFT가 W&amp;B 서비스를 사용하는 방식
</div>

Serverless SFT는 다음 W&amp;B 구성 요소를 조합해 운영됩니다:

* [Inference](/ko/inference): 모델을 실행하는 데 사용
* [Models](/ko/models): LoRA 어댑터 트레이닝 동안 성능 메트릭을 추적하는 데 사용
* [Artifacts](/ko/models/artifacts): LoRA 어댑터를 저장하고 버전 관리를 하는 데 사용
* [Weave (optional)](/ko/weave): 트레이닝 루프의 각 단계에서 모델 응답을 관측하는 데 사용

Serverless SFT는 현재 퍼블릭 프리뷰 단계입니다. 프리뷰 기간 동안에는 Inference 사용량과 아티팩트 저장에 대해서만 요금이 청구됩니다. W&amp;B는 프리뷰 기간 동안 어댑터 트레이닝에 대해 요금을 부과하지 않습니다.