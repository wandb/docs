---
description: W&B에서 모델 체크포인트 또는 호스팅된 API 모델을 평가하고, 자동 생성되는 리더보드를 통해 결과를 분석합니다.
title: LLM 평가 작업
---

import PreviewLink from '/snippets/ko/_includes/llm-eval-jobs/preview.mdx';

[LLM Evaluation Jobs](/ko/models/launch)는 CoreWeave에서 관리하는 인프라를 사용하여 LLM 모델의 성능을 평가하는 벤치마킹 프레임워크입니다. 최신 업계 표준을 포괄하는 [모델 평가 벤치마크](/ko/models/launch/evaluations) 모음에서 선택한 후, W&amp;B Models의 자동 리더보드와 차트를 사용해 결과를 조회·분석·공유할 수 있습니다. LLM Evaluation Jobs를 사용하면 직접 GPU 인프라를 배포하고 유지 관리해야 하는 복잡한 작업을 없앨 수 있습니다.

<PreviewLink />

<div id="how-it-works">
  ## 작동 방식
</div>

몇 가지 단계만 거치면 모델 체크포인트 또는 공개 접근이 가능한 OpenAI 호환 호스팅 모델을 평가할 수 있습니다:

1. W&amp;B Models에서 평가 작업을 설정합니다. 리더보드를 생성할지 여부와 같은 벤치마크와 설정을 정의합니다.
2. 평가 작업을 Launch에서 실행합니다.
3. 결과와 리더보드를 확인하고 분석합니다.

동일한 대상 프로젝트로 평가 작업을 Launch를 통해 실행할 때마다 해당 프로젝트의 리더보드는 자동으로 업데이트됩니다.

<Frame>
  ![Example evalution job leaderboard](/images/models/llm-evaluation-jobs/model-checkpoint-leaderboard-example.png)
</Frame>

<div id="next-steps">
  ## 다음 단계
</div>

* [Evaluation benchmark 카탈로그](/ko/models/launch/evaluations)를 둘러보세요
* [모델 체크포인트를 평가](/ko/models/launch/evaluate-model-checkpoint)하세요
* [API로 호스팅된 모델을 평가](/ko/models/launch/evaluate-hosted-model)하세요

<div id="more-details">
  ## 자세한 정보
</div>

<div id="pricing">
  ### 가격
</div>

LLM Evaluation Jobs는 관리할 인프라 없이 완전 관리형 CoreWeave 컴퓨트에서 인기 있는 벤치마크를 사용해 모델 체크포인트 또는 호스티드 API를 평가합니다. 사용한 리소스에 대해서만 비용을 지불하며, 유휴 시간에는 비용이 청구되지 않습니다. 가격은 컴퓨트와 스토리지 두 가지 요소로 구성됩니다. 컴퓨트는 공개 프리뷰 기간 동안 무료이며, 일반 제공 시점에 가격을 공지할 예정입니다. 저장된 결과에는 Models Runs에 저장되는 메트릭과 개별 예제별 트레이스가 포함됩니다. 스토리지는 데이터 용량을 기준으로 월별로 청구됩니다. 프리뷰 기간 동안 LLM Evaluation Jobs는 Multi-tenant Cloud에서만 사용할 수 있습니다. 자세한 내용은 [Pricing](https://wandb.ai/pricing) 페이지를 참고하세요.

<div id="job-limits">
  ### 작업 제한
</div>

개별 평가 작업에는 다음과 같은 제한이 있습니다.

* 평가에 사용할 수 있는 모델의 최대 크기는 컨텍스트를 포함해 86 GB입니다.
* 각 작업은 GPU를 최대 2개까지 사용할 수 있습니다.

<div id="requirements">
  ### 요구 사항
</div>

* 모델 체크포인트를 평가하려면 모델 가중치가 VLLM과 호환되는 아티팩트로 패키징되어 있어야 합니다. 자세한 내용과 예제 코드는 [예시: 모델 준비하기](/ko/models/launch/evaluate-model-checkpoint#example-prepare-a-model)를 참조하세요.
* OpenAI와 호환되는 모델을 평가하려면 해당 모델이 공개 URL에서 접근 가능해야 하며, 조직 또는 팀 관리자가 인증을 위한 API 키가 포함된 team secret을 설정해야 합니다.
* 일부 벤치마크는 점수 산정을 위해 OpenAI 모델을 사용합니다. 이러한 벤치마크를 실행하려면 조직 또는 팀 관리자가 필요한 API 키가 포함된 team secrets를 설정해야 합니다. 특정 벤치마크에 이 요구 사항이 있는지 확인하려면 [Evaluation benchmark catalog](/ko/models/launch/evaluations)를 참조하세요.
* 일부 벤치마크는 Hugging Face의 게이트된(gated) 데이터셋에 대한 접근을 필요로 합니다. 이러한 벤치마크를 실행하려면 조직 또는 팀 관리자가 Hugging Face에서 해당 게이트된 데이터셋에 대한 접근을 요청하고, Hugging Face 사용자 액세스 토큰을 생성한 뒤 이를 team secret으로 설정해야 합니다. 특정 벤치마크에 이 요구 사항이 있는지 확인하려면 [Evaluation benchmark catalog](/ko/models/launch/evaluations)를 참조하세요.

이러한 요구 사항을 충족하는 방법에 대한 자세한 내용과 안내는 다음 문서를 참조하세요.

* [모델 체크포인트 평가하기](/ko/models/launch/evaluate-hosted-model)
* [호스팅된 API 모델 평가하기](/ko/models/launch/evaluate-model-checkpoint)