---
description: W&B에서 모델 체크포인트나 호스팅된 API 모델을 평가하고, 자동으로 생성되는 리더보드를 통해 결과를 분석합니다.
title: LLM Evaluation 작업
---

import PreviewLink from '/snippets/ko/_includes/llm-eval-jobs/preview.mdx';

[LLM Evaluation Jobs](/ko/models/launch)는 CoreWeave가 관리하는 인프라를 사용해 LLM 모델의 성능을 평가하는 벤치마킹 프레임워크입니다. 최신 업계 표준을 포괄하는 다양한 [모델 평가 벤치마크](/ko/models/launch/evaluations) 중에서 선택한 뒤, 자동 리더보드와 차트를 통해 W&amp;B Models에서 결과를 확인·분석·공유할 수 있습니다. LLM Evaluation Jobs를 사용하면 직접 GPU 인프라를 배포하고 유지 관리해야 하는 복잡성을 없앨 수 있습니다.

<PreviewLink />

<div id="how-it-works">
  ## 작동 방식
</div>

모델 체크포인트 또는 공개적으로 액세스 가능한 OpenAI 호환 호스팅 모델을 몇 단계만으로 평가할 수 있습니다:

1. W&amp;B Models에서 evaluation 작업을 설정합니다. 리더보드를 생성할지 여부와 같은 벤치마크와 구성을 정의합니다.
2. evaluation 작업을 실행합니다.
3. 결과와 리더보드를 확인하고 분석합니다.

동일한 대상 프로젝트로 evaluation 작업을 실행할 때마다 해당 프로젝트의 리더보드는 자동으로 업데이트됩니다.

<Frame>
  ![예시 evaluation 작업 리더보드](/images/models/llm-evaluation-jobs/model-checkpoint-leaderboard-example.png)
</Frame>

<div id="next-steps">
  ## 다음 단계
</div>

* [Evaluation benchmark 카탈로그](/ko/models/launch/evaluations)를 둘러보세요
* [모델 체크포인트 평가하기](/ko/models/launch/evaluate-model-checkpoint)
* [API로 호스팅된 모델 평가하기](/ko/models/launch/evaluate-hosted-model)

<div id="more-details">
  ## 자세한 정보
</div>

<div id="pricing">
  ### 가격
</div>

LLM Evaluation Jobs는 완전 관리형 CoreWeave 컴퓨트에서 인기 벤치마크를 사용해 모델 체크포인트 또는 호스팅된 API를 평가하며, 인프라를 관리할 필요가 없습니다. 유휴 시간에는 비용이 청구되지 않고, 사용한 리소스에 대해서만 비용을 지불합니다. 요금은 컴퓨트와 스토리지 두 가지로 구성됩니다. 컴퓨트는 공개 프리뷰 기간 동안 무료이며, 일반 출시 시점에 요금을 공지할 예정입니다. 저장되는 결과에는 Models 실행에 기록되는 메트릭과 각 예제별 트레이스가 포함됩니다. 스토리지는 데이터 용량을 기준으로 월 단위로 청구됩니다. 프리뷰 기간 동안 LLM Evaluation Jobs는 Multi-tenant Cloud에서만 사용할 수 있습니다. 자세한 내용은 [Pricing](https://wandb.ai/pricing) 페이지를 참조하세요.

<div id="job-limits">
  ### Job 제한
</div>

개별 Evaluation Job에는 다음과 같은 제한이 있습니다:

* 컨텍스트를 포함해 평가할 수 있는 모델의 최대 크기는 86 GB입니다.
* 각 Job은 GPU 두 개까지 사용할 수 있습니다.

<div id="requirements">
  ### Requirements
</div>

* 모델 체크포인트를 평가하려면 모델 가중치가 VLLM과 호환되는 아티팩트로 패키징되어 있어야 합니다. 자세한 내용과 예제 코드는 [예시: 모델 준비](/ko/models/launch/evaluate-model-checkpoint#example-prepare-a-model)를 참조하세요.
* OpenAI와 호환되는 모델을 평가하려면 해당 모델이 공개 URL에서 접근 가능해야 하며, 조직 또는 팀 관리자가 인증용 API 키를 포함한 팀 시크릿을 설정해야 합니다.
* 일부 벤치마크는 점수를 계산하기 위해 OpenAI 모델을 사용합니다. 이러한 벤치마크를 실행하려면 조직 또는 팀 관리자가 필요한 API 키가 포함된 팀 시크릿을 설정해야 합니다. 특정 벤치마크에 이 요구 사항이 있는지 확인하려면 [Evaluation benchmark catalog](/ko/models/launch/evaluations)를 참조하세요.
* 일부 벤치마크는 Hugging Face의 게이티드 데이터셋에 대한 접근이 필요합니다. 이러한 벤치마크를 실행하려면 조직 또는 팀 관리자가 Hugging Face에서 해당 게이티드 데이터셋에 대한 접근을 요청하고, Hugging Face 사용자 액세스 토큰을 생성한 뒤 이를 팀 시크릿으로 설정해야 합니다. 특정 벤치마크에 이 요구 사항이 있는지 확인하려면 [Evaluation benchmark catalog](/ko/models/launch/evaluations)를 참조하세요.

이 요구 사항을 충족하는 방법에 대한 자세한 내용과 지침은 다음을 참조하세요:

* [모델 체크포인트 평가](/ko/models/launch/evaluate-hosted-model)
* [호스팅된 API 모델 평가](/ko/models/launch/evaluate-model-checkpoint)