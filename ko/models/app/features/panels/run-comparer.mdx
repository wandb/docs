---
description: 여러 실행 간 메트릭 비교
title: 실행 메트릭 비교
---

프로젝트 내 여러 실행의 차이점과 공통점을 확인하려면 Run Comparer를 사용하세요.

<div id="add-a-run-comparer-panel">
  ## 실행 비교 패널 추가
</div>

1. 페이지 오른쪽 상단에서 **패널 추가** 버튼을 선택합니다.
2. **Evaluation** 섹션에서 **Run comparer**를 선택합니다.

<div id="use-run-comparer">
  ## Run Comparer 사용하기
</div>

Run Comparer는 프로젝트에서 화면에 처음 표시되는 10개의 실행에 대해 각 실행의 구성(configuration)과 기록된 메트릭(metric)을 실행당 한 열씩 보여줍니다.

* 비교할 실행을 바꾸려면 왼쪽에 있는 실행 목록을 검색, 필터링, 그룹화 또는 정렬하세요. Run Comparer는 자동으로 업데이트됩니다.
* Run Comparer 상단의 검색 필드를 사용해 Python 버전이나 실행 생성 시간과 같은 구성 키 또는 메타데이터 키를 기준으로 필터링하거나 검색할 수 있습니다.
* 차이만 빠르게 확인하고 동일한 값을 숨기려면 패널 상단에서 **Diff only** 토글을 사용하세요.
* 열 너비나 행 높이를 조정하려면 패널 상단에 있는 서식 버튼을 사용하세요.
* 구성 또는 메트릭 값을 복사하려면 해당 값 위에 마우스를 올린 후 복사 버튼을 클릭하세요. 화면에 전부 표시되지 않더라도 값 전체가 복사됩니다.

<Note>
  기본적으로 Run Comparer는 [`job_type`](/ko/models/ref/python/functions/init) 값이 서로 다른 실행을 구분하지 않습니다. 즉, 프로젝트 내에서 서로 비교하기 적절하지 않은 실행도 비교할 수 있습니다. 예를 들어, training 실행과 모델 evaluation 실행을 비교할 수 있습니다. training 실행에는 실행 로그, 하이퍼파라미터, training loss 메트릭, 모델 자체가 포함될 수 있습니다. evaluation 실행은 이 모델을 사용해 새로운 training 데이터에서 모델 성능을 확인할 수 있습니다.

  Runs Table에서 실행 목록을 검색, 필터링, 그룹화 또는 정렬하면 Run Comparer는 자동으로 업데이트되어 처음 10개의 실행을 비교합니다. `job_type`으로 목록을 필터링하거나 정렬하는 것처럼, Runs Table 내에서 필터링하거나 검색하여 유사한 실행만 비교하세요. 실행 필터링에 대해 더 알아보려면 [filtering runs](/ko/models/runs/filter-runs/)를 참조하세요.
</Note>