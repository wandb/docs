---
title: 실험 생성하기
description: W&B Experiment 를 생성합니다.
---

W&B Python SDK를 사용하여 기계학습 실험을 추적하세요. 그런 다음 인터랙티브 대시보드에서 결과를 검토하거나, [W&B Public API](/models/ref/python/public-api/)를 사용하여 프로그래밍 방식의 access를 위해 데이터를 Python으로 내보낼 수 있습니다.

이 가이드는 W&B 빌딩 블록을 사용하여 W&B Experiment를 생성하는 방법을 설명합니다. 

## W&B Experiment 생성 방법

네 가지 단계로 W&B Experiment를 생성합니다:

1. [W&B Run 초기화](#initialize-a-wb-run)
2. [하이퍼파라미터 사전 캡처](#capture-a-dictionary-of-hyperparameters)
3. [트레이닝 루프 내에서 메트릭 로그 기록](#log-metrics-inside-your-training-loop)
4. [W&B에 아티팩트 로그 기록](#log-an-artifact-to-wb)

### W&B run 초기화
[`wandb.init()`](/models/ref/python/functions/init)을 사용하여 W&B Run을 생성합니다.

다음 코드조각은 이 run을 식별하는 데 도움이 되도록 `“cat-classification”`이라는 이름의 W&B project에 `“My first experiment”`라는 설명을 포함하여 run을 생성합니다. `“baseline”` 및 `“paper1”` 태그를 포함하여 이 run이 향후 논문 발표를 위한 베이스라인 실험임을 상기시킵니다.

```python
import wandb

with wandb.init(
    project="cat-classification",
    notes="My first experiment",
    tags=["baseline", "paper1"],
) as run:
    ...
```

`wandb.init()`은 [Run](/models/ref/python/experiments/run) 오브젝트를 반환합니다.

<Note>
안내: `wandb.init()`을 호출할 때 해당 프로젝트가 이미 존재하는 경우 Runs는 기존 프로젝트에 추가됩니다. 예를 들어, `“cat-classification”`이라는 프로젝트가 이미 있는 경우 해당 프로젝트는 삭제되지 않고 계속 유지됩니다. 대신, 해당 프로젝트에 새로운 run이 추가됩니다.
</Note>

### 하이퍼파라미터 사전 캡처
학습률이나 모델 유형과 같은 하이퍼파라미터 사전을 저장합니다. config에 캡처한 모델 설정은 나중에 결과를 정리하고 쿼리하는 데 유용합니다.

```python
with wandb.init(
    ...,
    config={"epochs": 100, "learning_rate": 0.001, "batch_size": 128},
) as run:
    ...
```

실험을 구성하는 방법에 대한 자세한 내용은 [Configure Experiments](./config)를 참조하세요.

### 트레이닝 루프 내에서 메트릭 로그 기록
[`run.log()`](/models/ref/python/experiments/run/#method-runlog)를 호출하여 정확도 및 손실과 같은 각 트레이닝 단계에 대한 메트릭을 로그로 기록합니다.

```python
model, dataloader = get_model(), get_data()

for epoch in range(run.config.epochs):
    for batch in dataloader:
        loss, accuracy = model.training_step()
        run.log({"accuracy": accuracy, "loss": loss})
```

W&B로 로그를 기록할 수 있는 다양한 데이터 유형에 대한 자세한 내용은 [Log Data During Experiments](/models/track/log/)를 참조하세요.

### W&B에 아티팩트 로그 기록 
선택적으로 W&B Artifact를 로그로 기록합니다. Artifacts를 사용하면 데이터셋과 모델의 버전 관리를 쉽게 할 수 있습니다. 
```python
# 모든 파일 또는 디렉토리를 저장할 수 있습니다. 이 예제에서는 
# 모델에 ONNX 파일을 출력하는 save() 메소드가 있다고 가정합니다.
model.save("path_to_model.onnx")
run.log_artifact("path_to_model.onnx", name="trained-model", type="model")
```
[Artifacts](/models/artifacts/)에 대해 더 알아보거나 [Registry](/models/registry/)에서 모델 버전 관리에 대해 알아보세요.


### 종합하기
앞선 코드조각들을 모두 포함한 전체 스크립트는 다음과 같습니다:
```python
import wandb

with wandb.init(
    project="cat-classification",
    notes="",
    tags=["baseline", "paper1"],
    # run의 하이퍼파라미터를 기록합니다.
    config={"epochs": 100, "learning_rate": 0.001, "batch_size": 128},
) as run:
    # 모델과 데이터를 설정합니다.
    model, dataloader = get_model(), get_data()

    # 모델 성능을 시각화하기 위해 메트릭을 로그로 기록하며 트레이닝을 실행합니다.
    for epoch in range(run.config["epochs"]):
        for batch in dataloader:
            loss, accuracy = model.training_step()
            run.log({"accuracy": accuracy, "loss": loss})

    # 학습된 모델을 아티팩트로 업로드합니다.
    model.save("path_to_model.onnx")
    run.log_artifact("path_to_model.onnx", name="trained-model", type="model")
```

## 다음 단계: 실험 시각화 
W&B Dashboard를 기계학습 모델의 결과를 정리하고 시각화하는 중앙 장소로 사용하세요. 클릭 몇 번만으로 [평행 좌표 플롯](/models/app/features/panels/parallel-coordinates/), [파라미터 중요도 분석](/models/app/features/panels/parameter-importance/) 및 [추가 차트 유형](/models/app/features/panels/)과 같은 풍부하고 인터랙티브 차트를 구성할 수 있습니다.

<Frame>
    <img src="/images/sweeps/quickstart_dashboard_example.png" alt="Quickstart Sweeps Dashboard example"  />
</Frame>

실험 및 특정 run을 확인하는 방법에 대한 자세한 내용은 [Visualize results from experiments](/models/track/workspaces/)를 참조하세요.


## 모범 사례
다음은 실험을 생성할 때 고려해야 할 몇 가지 권장 가이드라인입니다:

1. **Runs 완료**: `with` 문에서 `wandb.init()`을 사용하여 코드가 완료되거나 예외가 발생할 때 run이 자동으로 완료됨으로 표시되도록 하세요.
    * Jupyter 노트북에서는 Run 오브젝트를 직접 관리하는 것이 더 편리할 수 있습니다. 이 경우, Run 오브젝트에서 `finish()`를 명시적으로 호출하여 완료로 표시할 수 있습니다:

        ```python
        # 노트북 셀에서:
        run = wandb.init()

        # 다른 셀에서:
        run.finish()
        ```
2. **Config**: 하이퍼파라미터, 아키텍처, 데이터셋 및 모델을 재현하는 데 사용하려는 기타 모든 항목을 추적하세요. 이들은 열로 표시됩니다. config 열을 사용하여 앱에서 동적으로 run을 그룹화, 정렬 및 필터링하세요.
3. **Project**: 프로젝트는 함께 비교할 수 있는 실험 세트입니다. 각 프로젝트에는 전용 대시보드 페이지가 제공되며, 다양한 모델 버전을 비교하기 위해 여러 run 그룹을 쉽게 켜고 끌 수 있습니다.
4. **Notes**: 스크립트에서 직접 빠른 커밋 메시지를 설정하세요. W&B 앱의 run Overview 섹션에서 노트를 편집하고 엑세스할 수 있습니다.
5. **Tags**: 베이스라인 run과 즐겨찾는 run을 식별하세요. 태그를 사용하여 run을 필터링할 수 있습니다. 나중에 W&B 앱의 프로젝트 대시보드 Overview 섹션에서 태그를 편집할 수 있습니다.
6. **실험 비교를 위해 여러 run 세트 생성**: 실험을 비교할 때, 메트릭을 쉽게 비교할 수 있도록 여러 run 세트를 생성하세요. 동일한 차트나 차트 그룹에서 run 세트를 켜거나 끌 수 있습니다.

다음 코드조각은 위에 나열된 모범 사례를 사용하여 W&B Experiment를 정의하는 방법을 보여줍니다:

```python
import wandb

config = {
    "learning_rate": 0.01,
    "momentum": 0.2,
    "architecture": "CNN",
    "dataset_id": "cats-0192",
}

with wandb.init(
    project="detect-cats",
    notes="tweak baseline",
    tags=["baseline", "paper1"],
    config=config,
) as run:
    ...
```

W&B Experiment를 정의할 때 사용할 수 있는 파라미터에 대한 자세한 내용은 [API Reference Guide](/models/ref/python/)의 [`wandb.init()`](/models/ref/python/functions/init) API 문서를 참조하세요.