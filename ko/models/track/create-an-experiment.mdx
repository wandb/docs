---
description: W&B Experiment를 생성합니다.
title: Experiment 생성하기
---

W&amp;B Python SDK를 사용해 머신 러닝 실험을 추적할 수 있습니다. 이후 대화형 대시보드에서 결과를 검토하거나, 데이터를 Python으로 내보내 [W&amp;B Public API](/ko/models/ref/python/public-api/)를 통해 프로그래밍 방식으로 접근할 수 있습니다.

이 가이드는 W&amp;B 구성 요소를 사용해 W&amp;B Experiment를 생성하는 방법을 설명합니다.

<div id="how-to-create-a-wb-experiment">
  ## W&amp;B Experiment를 생성하는 방법
</div>

다음 네 단계를 따르면 W&amp;B Experiment를 생성할 수 있습니다:

1. [W&amp;B 실행 초기화하기](#initialize-a-wb-run)
2. [하이퍼파라미터 딕셔너리 캡처하기](#capture-a-dictionary-of-hyperparameters)
3. [훈련 루프에서 메트릭 로깅하기](#log-metrics-inside-your-training-loop)
4. [W&amp;B에 아티팩트 로깅하기](#log-an-artifact-to-wb)

<div id="initialize-a-wb-run">
  ### W&amp;B 실행 초기화하기
</div>

[`wandb.init()`](/ko/models/ref/python/functions/init)를 사용하여 W&amp;B 실행을 생성합니다.

다음 코드 스니펫은 `“cat-classification”`이라는 이름의 W&amp;B 프로젝트에서, 이 실행을 식별하는 데 도움이 되는 `“My first experiment”`라는 설명과 함께 실행을 생성합니다. 태그 `“baseline”`과 `“paper1”`는 이 실행이 향후 논문 출판을 위한 기준 실험임을 상기하기 위해 포함됩니다.

```python
import wandb

with wandb.init(
    project="cat-classification",
    notes="My first experiment",
    tags=["baseline", "paper1"],
) as run:
    ...
```

`wandb.init()`은 [실행](/ko/models/ref/python/experiments/run) 객체를 반환합니다.

<Note>
  참고: `wandb.init()`을 호출할 때 해당 프로젝트가 이미 존재한다면, 새 실행은 기존 프로젝트에 추가됩니다. 예를 들어 `"cat-classification"`이라는 프로젝트가 이미 있다면, 그 프로젝트는 계속 존재하며 삭제되지 않습니다. 대신 새 실행이 그 프로젝트에 추가됩니다.
</Note>

<div id="capture-a-dictionary-of-hyperparameters">
  ### 하이퍼파라미터 딕셔너리 저장하기
</div>

learning rate(학습률)나 모델 타입과 같은 하이퍼파라미터를 딕셔너리 형태로 저장하세요. `config`에 저장한 모델 설정은 나중에 결과를 정리하고 검색하는 데 유용합니다.

```python
with wandb.init(
    ...,
    config={"epochs": 100, "learning_rate": 0.001, "batch_size": 128},
) as run:
    ...
```

실험 구성 방법에 대해서는 [실험 구성](./config)을 참고하세요.

<div id="log-metrics-inside-your-training-loop">
  ### 학습 루프에서 메트릭 기록하기
</div>

정확도, 손실 등 각 학습 단계에 대한 메트릭을 기록하려면 [`run.log()`](/ko/models/ref/python/experiments/run/#method-runlog)를 호출하세요.

```python
model, dataloader = get_model(), get_data()

for epoch in range(run.config.epochs):
    for batch in dataloader:
        loss, accuracy = model.training_step()
        run.log({"accuracy": accuracy, "loss": loss})
```

W&amp;B에 기록할 수 있는 다양한 데이터 유형에 대한 자세한 내용은 [실험 중 데이터 기록하기](/ko/models/track/log/)를 참조하세요.

<div id="log-an-artifact-to-wb">
  ### W&amp;B에 Artifact 로깅하기
</div>

필요에 따라 W&amp;B Artifact를 로깅할 수 있습니다. Artifact를 사용하면 데이터셋과 모델을 쉽게 버전 관리할 수 있습니다.

```python
# 파일이나 디렉토리를 저장할 수 있습니다. 이 예시에서는 모델에
# ONNX 파일을 출력하는 save() 메서드가 있다고 가정합니다.
model.save("path_to_model.onnx")
run.log_artifact("path_to_model.onnx", name="trained-model", type="model")
```

[Artifact](/ko/models/artifacts/)에 대해 더 알아보거나 [Registry](/ko/models/registry/)에서 모델 버전 관리에 대해 알아보세요.

<div id="putting-it-all-together">
  ### 지금까지 내용을 하나로 정리하기
</div>

앞에서 살펴본 코드 스니펫을 모두 포함한 전체 스크립트는 아래와 같습니다:

```python
import wandb

with wandb.init(
    project="cat-classification",
    notes="",
    tags=["baseline", "paper1"],
    # 실행의 하이퍼파라미터를 기록합니다.
    config={"epochs": 100, "learning_rate": 0.001, "batch_size": 128},
) as run:
    # 모델과 데이터를 설정합니다.
    model, dataloader = get_model(), get_data()

    # 모델 성능을 시각화하기 위해 메트릭을 로깅하면서 학습을 실행합니다.
    for epoch in range(run.config["epochs"]):
        for batch in dataloader:
            loss, accuracy = model.training_step()
            run.log({"accuracy": accuracy, "loss": loss})

    # 학습된 모델을 아티팩트로 업로드합니다.
    model.save("path_to_model.onnx")
    run.log_artifact("path_to_model.onnx", name="trained-model", type="model")
```

<div id="next-steps-visualize-your-experiment">
  ## 다음 단계: 실험 시각화
</div>

W&amp;B Dashboard를 중앙 허브로 사용해 머신러닝 모델의 결과를 정리하고 시각화하세요. 몇 번의 클릭만으로 [parallel coordinates plots](/ko/models/app/features/panels/parallel-coordinates/), [parameter importance analyzes](/ko/models/app/features/panels/parameter-importance/), [추가 차트 유형](/ko/models/app/features/panels/)과 같은 다양한 대화형 차트를 만들 수 있습니다.

<Frame>
  <img src="/images/sweeps/quickstart_dashboard_example.png" alt="Quickstart Sweeps Dashboard example" />
</Frame>

실험 및 개별 실행을 확인하는 방법에 대한 자세한 내용은 [실험 결과 시각화](/ko/models/track/workspaces/)를 참조하세요.

<div id="best-practices">
  ## 모범 사례
</div>

다음은 실험을 만들 때 참고할 수 있는 몇 가지 권장 사항입니다:

1. **실행을 종료하세요**: `with` 문에서 `wandb.init()`을 사용하여 코드가 완료되거나 예외가 발생할 때 실행이 자동으로 종료되도록 합니다.
   * Jupyter 노트북에서는 직접 Run 객체를 관리하는 것이 더 편리할 수 있습니다. 이 경우 Run 객체에서 `finish()`를 명시적으로 호출하여 실행을 완료 상태로 표시할 수 있습니다:

     ```python
     # 노트북 셀에서:
     run = wandb.init()

     # 다른 셀에서:
     run.finish()
     ```
2. **Config**: 하이퍼파라미터, 아키텍처, 데이터셋, 그리고 모델을 재현하는 데 사용하고 싶은 모든 항목을 추적하세요. 이 값들은 열(column)에 표시되며, 앱에서 config 열을 사용해 실행을 동적으로 그룹화, 정렬, 필터링할 수 있습니다.
3. **Project**: 프로젝트는 함께 비교할 수 있는 일련의 실험입니다. 각 프로젝트에는 전용 대시보드 페이지가 제공되며, 서로 다른 실행 그룹을 쉽게 켜고 끄면서 다양한 모델 버전을 비교할 수 있습니다.
4. **Notes**: 스크립트에서 직접 간단한 커밋 메시지를 설정하세요. 실행의 개요(Overview) 섹션에서 노트를 편집하고 확인할 수 있습니다(W&amp;B App).
5. **Tags**: 기준선 실행과 선호하는 실행을 식별하세요. 태그를 사용해 실행을 필터링할 수 있습니다. 나중에 W&amp;B App의 프로젝트 대시보드 Overview 섹션에서 태그를 수정할 수 있습니다.
6. **여러 실행 집합을 만들어 실험을 비교하세요**: 실험을 비교할 때는 여러 실행 집합을 생성하여 메트릭을 쉽게 비교할 수 있도록 하세요. 동일한 차트 또는 차트 그룹에서 실행 집합을 켜거나 끌 수 있습니다.

다음 코드 예제는 위에 나열된 모범 사례를 사용하여 W&amp;B 실험(Experiment)을 정의하는 방법을 보여줍니다:

```python
import wandb

config = {
    "learning_rate": 0.01,
    "momentum": 0.2,
    "architecture": "CNN",
    "dataset_id": "cats-0192",
}

with wandb.init(
    project="detect-cats",
    notes="tweak baseline",
    tags=["baseline", "paper1"],
    config=config,
) as run:
    ...
```

W&amp;B Experiment를 정의할 때 사용 가능한 매개변수에 대한 자세한 내용은 [API Reference Guide](/ko/models/ref/python/)의 [`wandb.init()`](/ko/models/ref/python/functions/init) API 문서를 참고하세요.
