---
title: Models ë° Datasets ì¶”ì 
---
import { ColabLink } from '/snippets/en/_includes/colab-link.mdx';

<ColabLink url="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W&B_Artifacts.ipynb" />

ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” W&B Artifactsë¥¼ ì‚¬ìš©í•˜ì—¬ ML ì‹¤í—˜ íŒŒì´í”„ë¼ì¸ì„ íŠ¸ë˜í‚¹í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.

[ë¹„ë””ì˜¤ íŠœí† ë¦¬ì–¼](https://tiny.cc/wb-artifacts-video)ì„ í•¨ê»˜ ì‹œì²­í•˜ë©° ë”°ë¼í•´ ë³´ì„¸ìš”.

## Artifactsì— ëŒ€í•˜ì—¬

ê·¸ë¦¬ìŠ¤ì˜ [ì•”í¬ë¼](https://en.wikipedia.org/wiki/Amphora)ì™€ ê°™ì€ ì•„í‹°íŒ©íŠ¸ëŠ” í”„ë¡œì„¸ìŠ¤ì˜ ê²°ê³¼ë¬¼ì¸ ìƒì‚°ëœ ì˜¤ë¸Œì íŠ¸ì…ë‹ˆë‹¤.
MLì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì•„í‹°íŒ©íŠ¸ëŠ” _datasets_ (ë°ì´í„°ì…‹)ê³¼ _models_ (ëª¨ë¸)ì…ë‹ˆë‹¤.

ê·¸ë¦¬ê³  [ì½”ë¡œë‚˜ë„ ì „ ì „ì„¤ì˜ ì‹­ìê°€](https://indianajones.fandom.com/wiki/Cross_of_Coronado)ì²˜ëŸ¼, ì´ëŸ¬í•œ ì¤‘ìš”í•œ ì•„í‹°íŒ©íŠ¸ë“¤ì€ ë°•ë¬¼ê´€ì— ë³´ê´€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
ì¦‰, ì—¬ëŸ¬ë¶„ê³¼ ì—¬ëŸ¬ë¶„ì˜ íŒ€, ê·¸ë¦¬ê³  ë” ë„“ì€ ML ì»¤ë®¤ë‹ˆí‹°ê°€ ì´ë¥¼ í†µí•´ ë°°ìš¸ ìˆ˜ ìˆë„ë¡ ì¹´íƒˆë¡œê·¸í™”ë˜ê³  ì •ë¦¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
ê²°êµ­ íŠ¸ë ˆì´ë‹ì„ ê¸°ë¡í•˜ì§€ ì•ŠëŠ” ì‚¬ëŒë“¤ì€ ì´ë¥¼ ë°˜ë³µí•  ìš´ëª…ì— ì²˜í•˜ê²Œ ë©ë‹ˆë‹¤.

W&Bì˜ Artifacts APIë¥¼ ì‚¬ìš©í•˜ë©´ `Artifact`ë¥¼ W&B `Run`ì˜ ì¶œë ¥ìœ¼ë¡œ ë¡œê·¸í•˜ê±°ë‚˜, ì•„ë˜ ë‹¤ì´ì–´ê·¸ë¨ì²˜ëŸ¼ `Artifact`ë¥¼ `Run`ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì˜ˆì‹œì—ì„œ íŠ¸ë ˆì´ë‹ runì€ ë°ì´í„°ì…‹ì„ ì…ë ¥ë°›ì•„ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

 <Frame>
    <img src="/images/tutorials/artifacts-diagram.png" alt="Artifacts workflow diagram"  />
</Frame>

í•˜ë‚˜ì˜ runì´ ë‹¤ë¥¸ runì˜ ì¶œë ¥ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, `Artifact`ì™€ `Run`ì€ í•¨ê»˜ ìœ í–¥ ê·¸ë˜í”„(ë¹„ìˆœí™˜ ìœ í–¥ ê·¸ë˜í”„ [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph))ë¥¼ í˜•ì„±í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ë…¸ë“œëŠ” `Artifact`ì™€ `Run`ì„ ë‚˜íƒ€ë‚´ë©°, í™”ì‚´í‘œëŠ” `Run`ì„ ì†Œë¹„í•˜ê±°ë‚˜ ìƒì‚°í•˜ëŠ” `Artifact`ì— ì—°ê²°í•©ë‹ˆë‹¤.

## Artifactsë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë° ë°ì´í„°ì…‹ íŠ¸ë˜í‚¹í•˜ê¸°

### ì„¤ì¹˜ ë° ì„í¬íŠ¸

ArtifactsëŠ” ë²„ì „ `0.9.2`ë¶€í„° Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì¼ë¶€ë¡œ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.

ëŒ€ë¶€ë¶„ì˜ ML Python ìŠ¤íƒê³¼ ë§ˆì°¬ê°€ì§€ë¡œ `pip`ë¥¼ í†µí•´ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
# wandb ë²„ì „ 0.9.2 ì´ìƒê³¼ í˜¸í™˜ë©ë‹ˆë‹¤
!pip install wandb -qqq
!apt install tree
```


```python
import os
import wandb
```

### ë°ì´í„°ì…‹ ë¡œê·¸í•˜ê¸°

ë¨¼ì €, ëª‡ ê°€ì§€ Artifactsë¥¼ ì •ì˜í•´ ë³´ê² ìŠµë‹ˆë‹¤.

ì´ ì˜ˆì œëŠ” PyTorchì˜ ["Basic MNIST Example"](https://github.com/pytorch/examples/tree/master/mnist/)ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì§€ë§Œ, [TensorFlow](https://wandb.me/artifacts-colab)ë‚˜ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬, ë˜ëŠ” ìˆœìˆ˜ Pythonì—ì„œë„ ë™ì¼í•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë¨¼ì € `Dataset`ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤:
- íŒŒë¼ë¯¸í„° ì„ íƒì„ ìœ„í•œ `train` (íŠ¸ë ˆì´ë‹) ì„¸íŠ¸
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìœ„í•œ `validation` (ê²€ì¦) ì„¸íŠ¸
- ìµœì¢… ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ `test` (í…ŒìŠ¤íŠ¸) ì„¸íŠ¸

ì•„ë˜ì˜ ì²« ë²ˆì§¸ ì…€ì€ ì´ ì„¸ ê°€ì§€ ë°ì´í„°ì…‹ì„ ì •ì˜í•©ë‹ˆë‹¤.


```python
import random 

import torch
import torchvision
from torch.utils.data import TensorDataset
from tqdm.auto import tqdm

# ê²°ì •ë¡ ì  í–‰ë™ ë³´ì¥
torch.backends.cudnn.deterministic = True
random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# ë°ì´í„° íŒŒë¼ë¯¸í„°
num_classes = 10
input_shape = (1, 28, 28)

# MNIST ë¯¸ëŸ¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ëŠë¦° ë¯¸ëŸ¬ ì œê±°
torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors
                                      if not mirror.startswith("http://yann.lecun.com")]

def load(train_size=50_000):
    """
    # ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤
    """

    # ë°ì´í„°ë¥¼ íŠ¸ë ˆì´ë‹ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• 
    train = torchvision.datasets.MNIST("./", train=True, download=True)
    test = torchvision.datasets.MNIST("./", train=False, download=True)
    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìœ„í•œ ê²€ì¦ ì„¸íŠ¸ ë¶„ë¦¬
    x_train, x_val = x_train[:train_size], x_train[train_size:]
    y_train, y_val = y_train[:train_size], y_train[train_size:]

    training_set = TensorDataset(x_train, y_train)
    validation_set = TensorDataset(x_val, y_val)
    test_set = TensorDataset(x_test, y_test)

    datasets = [training_set, validation_set, test_set]

    return datasets
```

ì´ ì˜ˆì œì—ì„œ ë°˜ë³µë˜ëŠ” íŒ¨í„´ì„ ì„¤ì •í•©ë‹ˆë‹¤: ë°ì´í„°ë¥¼ Artifactë¡œ ë¡œê·¸í•˜ëŠ” ì½”ë“œëŠ” í•´ë‹¹ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œë¥¼ ê°ì‹¸ëŠ” í˜•íƒœê°€ ë©ë‹ˆë‹¤. ì´ ê²½ìš° ë°ì´í„°ë¥¼ ë¡œë“œ(`load`)í•˜ëŠ” ì½”ë“œì™€ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë¡œê·¸(`load_and_log`)í•˜ëŠ” ì½”ë“œê°€ ë¶„ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì´ê²ƒì€ ì¢‹ì€ ê´€í–‰ì…ë‹ˆë‹¤.

ì´ ë°ì´í„°ì…‹ë“¤ì„ Artifactë¡œ ë¡œê·¸í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤:
1. `wandb.init()`ìœ¼ë¡œ `Run` ìƒì„± (L4)
2. ë°ì´í„°ì…‹ì„ ìœ„í•œ `Artifact` ìƒì„± (L10)
3. ê´€ë ¨ `file` ì €ì¥ ë° ë¡œê·¸ (L20, L23)

ì•„ë˜ ì½”ë“œ ì…€ì˜ ì˜ˆì œë¥¼ í™•ì¸í•˜ê³  ë” ìì„¸í•œ ë‚´ìš©ì€ ë’¤ì˜ ì„¹ì…˜ì„ í¼ì³ë³´ì„¸ìš”.


```python
def load_and_log():

    # ë ˆì´ë¸”ì„ ì§€ì •í•  íƒ€ì…ê³¼ í”„ë¡œì íŠ¸ë¥¼ ì„¤ì •í•˜ì—¬ runì„ ì‹œì‘í•©ë‹ˆë‹¤
    with wandb.init(project="artifacts-example", job_type="load-data") as run:
        
        datasets = load()  # ë°ì´í„°ì…‹ ë¡œë“œë¥¼ ìœ„í•œ ë³„ë„ ì½”ë“œ
        names = ["training", "validation", "test"]

        # ğŸº Artifact ìƒì„±
        raw_data = wandb.Artifact(
            "mnist-raw", type="dataset",
            description="Raw MNIST dataset, split into train/val/test",
            metadata={"source": "torchvision.datasets.MNIST",
                      "sizes": [len(dataset) for dataset in datasets]})

        for name, data in zip(names, datasets):
            # ğŸ£ ì•„í‹°íŒ©íŠ¸ì— ìƒˆ íŒŒì¼ì„ ì €ì¥í•˜ê³  ë‚´ìš©ì„ ì‘ì„±í•©ë‹ˆë‹¤
            with raw_data.new_file(name + ".pt", mode="wb") as file:
                x, y = data.tensors
                torch.save((x, y), file)

        # âœï¸ W&Bì— ì•„í‹°íŒ©íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        run.log_artifact(raw_data)

load_and_log()
```

#### `wandb.init()`

`Artifact`ë¥¼ ìƒì„±í•  `Run`ì„ ë§Œë“¤ ë•Œ, í•´ë‹¹ runì´ ì†í•  `project`ë¥¼ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.

ì›Œí¬í”Œë¡œìš°ì— ë”°ë¼ í”„ë¡œì íŠ¸ëŠ” `car-that-drives-itself`ì²˜ëŸ¼ í´ ìˆ˜ë„ ìˆê³  `iterative-architecture-experiment-117`ì²˜ëŸ¼ ì‘ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

> **Best practice**: ê°€ëŠ¥í•˜ë‹¤ë©´ `Artifact`ë¥¼ ê³µìœ í•˜ëŠ” ëª¨ë“  `Run`ì„ ë‹¨ì¼ í”„ë¡œì íŠ¸ ë‚´ì— ìœ ì§€í•˜ì„¸ìš”. ì´ë ‡ê²Œ í•˜ë©´ ê´€ë¦¬ê°€ ë‹¨ìˆœí•´ì§€ì§€ë§Œ, ê±±ì •í•˜ì§€ ë§ˆì„¸ìš”. `Artifact`ëŠ” í”„ë¡œì íŠ¸ ê°„ ì´ë™ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì‹¤í–‰í•˜ëŠ” ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì‘ì—…ì„ íŠ¸ë˜í‚¹í•˜ê¸° ìœ„í•´ `Run`ì„ ìƒì„±í•  ë•Œ `job_type`ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ìœ ìš©í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ Artifacts ê·¸ë˜í”„ë¥¼ ê¹”ë”í•˜ê²Œ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **Best practice**: `job_type`ì€ íŒŒì´í”„ë¼ì¸ì˜ ë‹¨ì¼ ë‹¨ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” ëª…ì¹­ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë°ì´í„°ë¥¼ ë¡œë“œ(`load`)í•˜ëŠ” ì‘ì—…ê³¼ ì „ì²˜ë¦¬(`preprocess`)í•˜ëŠ” ì‘ì—…ì„ êµ¬ë¶„í–ˆìŠµë‹ˆë‹¤.

#### `wandb.Artifact`

ë¬´ì—‡ì¸ê°€ë¥¼ `Artifact`ë¡œ ë¡œê·¸í•˜ë ¤ë©´ ë¨¼ì € `Artifact` ì˜¤ë¸Œì íŠ¸ë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.

ëª¨ë“  `Artifact`ëŠ” `name`ì„ ê°€ì§‘ë‹ˆë‹¤. ì´ëŠ” ì²« ë²ˆì§¸ ì¸ìˆ˜ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.

> **Best practice**: `name`ì€ ì„¤ëª…ì ì´ì–´ì•¼ í•˜ì§€ë§Œ ê¸°ì–µí•˜ê¸° ì‰½ê³  íƒ€ì´í•‘í•˜ê¸° ì¢‹ì•„ì•¼ í•©ë‹ˆë‹¤. í•˜ì´í”ˆìœ¼ë¡œ êµ¬ë¶„ë˜ê³  ì½”ë“œì˜ ë³€ìˆ˜ëª…ê³¼ ì¼ì¹˜í•˜ëŠ” ì´ë¦„ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

ë˜í•œ `type`ì„ ê°€ì§‘ë‹ˆë‹¤. `Run`ì˜ `job_type`ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ì´ëŠ” `Run`ê³¼ `Artifact`ì˜ ê·¸ë˜í”„ë¥¼ ì •ë¦¬í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

> **Best practice**: `type`ì€ ë‹¨ìˆœí•´ì•¼ í•©ë‹ˆë‹¤. `mnist-data-YYYYMMDD`ë³´ë‹¤ëŠ” `dataset`ì´ë‚˜ `model`ê³¼ ê°™ì€ ëª…ì¹­ì„ ì‚¬ìš©í•˜ì„¸ìš”.

ì‚¬ì „(dictionary) í˜•íƒœë¡œ `description`ê³¼ `metadata`ë¥¼ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. `metadata`ëŠ” JSONìœ¼ë¡œ ì§ë ¬í™” ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.

> **Best practice**: `metadata`ëŠ” ê°€ëŠ¥í•œ í•œ ìì„¸í•˜ê²Œ ì‘ì„±í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

#### `artifact.new_file` ë° `run.log_artifact`

`Artifact` ì˜¤ë¸Œì íŠ¸ë¥¼ ë§Œë“  í›„ì—ëŠ” íŒŒì¼ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

ë§ìŠµë‹ˆë‹¤. ë‹¨ì¼ íŒŒì¼ì´ ì•„ë‹ˆë¼ ì—¬ëŸ¬ _files_ (íŒŒì¼ë“¤)ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `Artifact`ëŠ” íŒŒì¼ê³¼ ì„œë¸Œ ë””ë ‰í† ë¦¬ë¥¼ í¬í•¨í•˜ëŠ” ë””ë ‰í† ë¦¬ì™€ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

> **Best practice**: ì˜ë¯¸ê°€ ìˆëŠ” ê²½ìš° `Artifact`ì˜ ë‚´ìš©ì„ ì—¬ëŸ¬ íŒŒì¼ë¡œ ë‚˜ëˆ„ì–´ ì €ì¥í•˜ì„¸ìš”. ì´ëŠ” ë‚˜ì¤‘ì— ê·œëª¨ë¥¼ í™•ì¥í•  ë•Œ ë„ì›€ì´ ë©ë‹ˆë‹¤.

`new_file` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ì„ ì‘ì„±í•¨ê³¼ ë™ì‹œì— `Artifact`ì— ì²¨ë¶€í•©ë‹ˆë‹¤. ì•„ë˜ì—ì„œëŠ” ì´ ë‘ ë‹¨ê³„ë¥¼ ë¶„ë¦¬í•˜ëŠ” `add_file` ë©”ì†Œë“œë„ ì‚¬ìš©í•´ ë³¼ ê²ƒì…ë‹ˆë‹¤.

ëª¨ë“  íŒŒì¼ì„ ì¶”ê°€í–ˆë‹¤ë©´ [wandb.ai](https://wandb.ai)ì— `log_artifact`ë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.

ì¶œë ¥ ê²°ê³¼ì— Run í˜ì´ì§€ë¥¼ í¬í•¨í•œ ëª‡ ê°€ì§€ URLì´ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê±°ê¸°ì—ì„œ ë¡œê·¸ëœ ëª¨ë“  `Artifact`ë¥¼ í¬í•¨í•˜ì—¬ `Run`ì˜ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì•„ë˜ì—ì„œ Run í˜ì´ì§€ì˜ ë‹¤ë¥¸ êµ¬ì„± ìš”ì†Œë“¤ì„ ë” ì˜ í™œìš©í•˜ëŠ” ì˜ˆì‹œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

### ë¡œê·¸ëœ ë°ì´í„°ì…‹ Artifact ì‚¬ìš©í•˜ê¸°

W&Bì˜ `Artifact`ëŠ” ë°•ë¬¼ê´€ì˜ ìœ ë¬¼ê³¼ ë‹¬ë¦¬ ë³´ê´€ë§Œ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ _ì‚¬ìš©_ ë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

ê·¸ ê³¼ì •ì´ ì–´ë–¤ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

ì•„ë˜ ì…€ì€ ì›ì‹œ ë°ì´í„°ì…‹ì„ ì…ë ¥ë°›ì•„ ì •ê·œí™” ë° í˜•íƒœê°€ ì¡°ì •ëœ `preprocess` (ì „ì²˜ë¦¬) ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ëŠ” íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

ì´ë²ˆì—ë„ í•µì‹¬ ì½”ë“œì¸ `preprocess`ì™€ `wandb` ì¸í„°í˜ì´ìŠ¤ ì½”ë“œë¥¼ ë¶„ë¦¬í–ˆìŒì— ìœ ì˜í•˜ì„¸ìš”.


```python
def preprocess(dataset, normalize=True, expand_dims=True):
    """
    ## ë°ì´í„° ì¤€ë¹„
    """
    x, y = dataset.tensors

    if normalize:
        # ì´ë¯¸ì§€ë¥¼ [0, 1] ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§
        x = x.type(torch.float32) / 255

    if expand_dims:
        # ì´ë¯¸ì§€ í˜•íƒœë¥¼ (1, 28, 28)ë¡œ í™•ì •
        x = torch.unsqueeze(x, 1)
    
    return TensorDataset(x, y)
```

ì´ì œ ì´ `preprocess` ë‹¨ê³„ì— `wandb.Artifact` ë¡œê¹…ì„ ì ìš©í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

ì•„ë˜ ì˜ˆì œëŠ” ìƒˆë¡œìš´ ë‹¨ê³„ì¸ `Artifact` ì‚¬ìš©(`use`)ê³¼ ì´ì „ ë‹¨ê³„ì™€ ë™ì¼í•œ ë¡œê·¸(`log`)ë¥¼ ëª¨ë‘ ìˆ˜í–‰í•©ë‹ˆë‹¤. `Artifact`ëŠ” `Run`ì˜ ì…ë ¥ì´ì ì¶œë ¥ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ì „ ì‘ì—…ê³¼ëŠ” ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì‘ì—…ì„ì„ ëª…í™•íˆ í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ `job_type`ì¸ `preprocess-data`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.


```python
def preprocess_and_log(steps):

    with wandb.init(project="artifacts-example", job_type="preprocess-data") as run:

        processed_data = wandb.Artifact(
            "mnist-preprocess", type="dataset",
            description="Preprocessed MNIST dataset",
            metadata=steps)
         
        # âœ”ï¸ ì‚¬ìš©í•  ì•„í‹°íŒ©íŠ¸ë¥¼ ì„ ì–¸í•©ë‹ˆë‹¤
        raw_data_artifact = run.use_artifact('mnist-raw:latest')

        # ğŸ“¥ í•„ìš”í•œ ê²½ìš° ì•„í‹°íŒ©íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤
        raw_dataset = raw_data_artifact.download()
        
        for split in ["training", "validation", "test"]:
            raw_split = read(raw_dataset, split)
            processed_dataset = preprocess(raw_split, **steps)

            with processed_data.new_file(split + ".pt", mode="wb") as file:
                x, y = processed_dataset.tensors
                torch.save((x, y), file)

        run.log_artifact(processed_data)


def read(data_dir, split):
    filename = split + ".pt"
    x, y = torch.load(os.path.join(data_dir, filename))

    return TensorDataset(x, y)
```

ì—¬ê¸°ì„œ ì£¼ëª©í•  ì ì€ ì „ì²˜ë¦¬ì˜ `steps`ê°€ `metadata`ë¡œì„œ `preprocessed_data`ì™€ í•¨ê»˜ ì €ì¥ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì‹¤í—˜ì˜ ì¬í˜„ì„±ì„ í™•ë³´í•˜ë ¤ í•œë‹¤ë©´, ê°€ëŠ¥í•œ ë§ì€ ë©”íƒ€ë°ì´í„°ë¥¼ ìº¡ì²˜í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

ë˜í•œ ë°ì´í„°ì…‹ì´ "ëŒ€í˜• ì•„í‹°íŒ©íŠ¸"ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  `download` ë‹¨ê³„ëŠ” 1ì´ˆë„ ê±¸ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.

ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ ë§ˆí¬ë‹¤ìš´ ì…€ì„ í¼ì³ í™•ì¸í•˜ì„¸ìš”.


```python
steps = {"normalize": True,
         "expand_dims": True}

preprocess_and_log(steps)
```

#### `run.use_artifact()`

ì´ ë‹¨ê³„ëŠ” ê°„ë‹¨í•©ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” `Artifact`ì˜ `name`ê³¼ ì¶”ê°€ ì •ë³´ë§Œ ì•Œë©´ ë©ë‹ˆë‹¤.

ê·¸ "ì¶”ê°€ ì •ë³´"ëŠ” ì‚¬ìš©í•˜ë ¤ëŠ” íŠ¹ì • ë²„ì „ì˜ `Artifact` ì—ì¼ë¦¬ì–´ìŠ¤(`alias`)ì…ë‹ˆë‹¤.

ê¸°ë³¸ì ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ì— ì—…ë¡œë“œëœ ë²„ì „ì€ `latest`ë¡œ íƒœê·¸ë©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ `v0`/`v1` ë“±ìœ¼ë¡œ ì´ì „ ë²„ì „ì„ ì„ íƒí•˜ê±°ë‚˜ `best` ë˜ëŠ” `jit-script`ì™€ ê°™ì´ ì§ì ‘ ì—ì¼ë¦¬ì–´ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [Docker Hub](https://hub.docker.com/) íƒœê·¸ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì—ì¼ë¦¬ì–´ìŠ¤ëŠ” ì´ë¦„ê³¼ `:`ë¡œ êµ¬ë¶„ë˜ë¯€ë¡œ, ìš°ë¦¬ê°€ ì›í•˜ëŠ” `Artifact`ëŠ” `mnist-raw:latest`ì…ë‹ˆë‹¤.

> **Best practice**: ì—ì¼ë¦¬ì–´ìŠ¤ëŠ” ì§§ê³  ëª…í™•í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”. íŠ¹ì • ì†ì„±ì„ ë§Œì¡±í•˜ëŠ” `Artifact`ë¥¼ ì›í•  ë•Œ `latest`ë‚˜ `best`ì™€ ê°™ì€ ì»¤ìŠ¤í…€ `alias`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

#### `artifact.download`

ì´ì œ `download` í˜¸ì¶œì— ëŒ€í•´ ê±±ì •í•˜ì‹¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì‚¬ë³¸ì„ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•˜ë©´ ë©”ëª¨ë¦¬ ë¶€ë‹´ì´ ë‘ ë°°ê°€ ë˜ì§€ ì•Šì„ê¹Œìš”?

ê±±ì •í•˜ì§€ ë§ˆì„¸ìš”. ì‹¤ì œë¡œ ë¬´ì–¸ê°€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê¸° ì „ì— ë¡œì»¬ì— ì˜¬ë°”ë¥¸ ë²„ì „ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì´ëŠ” [í† ë ŒíŠ¸](https://en.wikipedia.org/wiki/Torrent_file)ë‚˜ [`git`ì„ ì´ìš©í•œ ë²„ì „ ê´€ë¦¬](https://blog.thoughtram.io/git/2014/11/18/the-anatomy-of-a-git-commit.html)ì˜ ê¸°ë°˜ì´ ë˜ëŠ” ê¸°ìˆ ì¸ í•´ì‹±(hashing)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

Artifactsê°€ ìƒì„±ë˜ê³  ë¡œê·¸ë¨ì— ë”°ë¼ ì‘ì—… ë””ë ‰í† ë¦¬ì˜ `artifacts` í´ë”ì—ëŠ” ê° `Artifact`ì— ëŒ€í•œ ì„œë¸Œ ë””ë ‰í† ë¦¬ê°€ ì±„ì›Œì§€ê¸° ì‹œì‘í•©ë‹ˆë‹¤. `!tree artifacts`ë¡œ ê·¸ ë‚´ìš©ì„ í™•ì¸í•´ ë³´ì„¸ìš”.


```python
!tree artifacts
```

#### Artifacts í˜ì´ì§€

ì´ì œ `Artifact`ë¥¼ ë¡œê·¸í•˜ê³  ì‚¬ìš©í•´ ë³´ì•˜ìœ¼ë‹ˆ Run í˜ì´ì§€ì˜ Artifacts íƒ­ì„ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤.

`wandb` ì¶œë ¥ì˜ Run í˜ì´ì§€ URLë¡œ ì´ë™í•˜ì—¬ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ "Artifacts" íƒ­ì„ ì„ íƒí•©ë‹ˆë‹¤(ì„¸ ê°œì˜ í•˜í‚¤ í½ì´ ìŒ“ì—¬ ìˆëŠ” ëª¨ì–‘ì˜ ë°ì´í„°ë² ì´ìŠ¤ ì•„ì´ì½˜ì…ë‹ˆë‹¤).

**Input Artifacts** í…Œì´ë¸”ì´ë‚˜ **Output Artifacts** í…Œì´ë¸”ì—ì„œ í–‰ì„ í´ë¦­í•œ ë‹¤ìŒ, íƒ­(**Overview**, **Metadata**)ì„ íƒìƒ‰í•˜ì—¬ í•´ë‹¹ `Artifact`ì— ëŒ€í•´ ë¡œê·¸ëœ ëª¨ë“  ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.

íŠ¹íˆ **Graph View**ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ `Artifact`ì˜ `type`ê³¼ `Run`ì˜ `job_type`ì„ ë‘ ì¢…ë¥˜ì˜ ë…¸ë“œë¡œ ë³´ì—¬ì£¼ë©°, ì†Œë¹„ì™€ ìƒì‚° ê´€ê³„ë¥¼ í™”ì‚´í‘œë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

### ëª¨ë¸ ë¡œê·¸í•˜ê¸°

ì´ê²ƒìœ¼ë¡œ Artifacts APIê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì¶©ë¶„íˆ ì‚´í´ë³´ì•˜ì§€ë§Œ, `Artifact`ê°€ ML ì›Œí¬í”Œë¡œìš°ë¥¼ ì–´ë–»ê²Œ ê°œì„ í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì´ ì˜ˆì œì˜ íŒŒì´í”„ë¼ì¸ ëê¹Œì§€ ë”°ë¼ê°€ ë³´ê² ìŠµë‹ˆë‹¤.

ì—¬ê¸° ì²« ë²ˆì§¸ ì…€ì€ PyTorchë¡œ DNN `model`ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. ì•„ì£¼ ê°„ë‹¨í•œ ConvNetì…ë‹ˆë‹¤.

ë¨¼ì € ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•˜ì§€ ì•Šê³  ì´ˆê¸°í™”ë§Œ í•˜ê² ìŠµë‹ˆë‹¤. ê·¸ë ‡ê²Œ í•˜ë©´ ë‹¤ë¥¸ ëª¨ë“  ìš”ì†Œë¥¼ ì¼ì •í•˜ê²Œ ìœ ì§€í•˜ë©´ì„œ íŠ¸ë ˆì´ë‹ì„ ë°˜ë³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
from math import floor

import torch.nn as nn

class ConvNet(nn.Module):
    def __init__(self, hidden_layer_sizes=[32, 64],
                  kernel_sizes=[3],
                  activation="ReLU",
                  pool_sizes=[2],
                  dropout=0.5,
                  num_classes=num_classes,
                  input_shape=input_shape):
      
        super(ConvNet, self).__init__()

        self.layer1 = nn.Sequential(
              nn.Conv2d(in_channels=input_shape[0], out_channels=hidden_layer_sizes[0], kernel_size=kernel_sizes[0]),
              getattr(nn, activation)(),
              nn.MaxPool2d(kernel_size=pool_sizes[0])
        )
        self.layer2 = nn.Sequential(
              nn.Conv2d(in_channels=hidden_layer_sizes[0], out_channels=hidden_layer_sizes[-1], kernel_size=kernel_sizes[-1]),
              getattr(nn, activation)(),
              nn.MaxPool2d(kernel_size=pool_sizes[-1])
        )
        self.layer3 = nn.Sequential(
              nn.Flatten(),
              nn.Dropout(dropout)
        )

        fc_input_dims = floor((input_shape[1] - kernel_sizes[0] + 1) / pool_sizes[0]) # layer 1 output size
        fc_input_dims = floor((fc_input_dims - kernel_sizes[-1] + 1) / pool_sizes[-1]) # layer 2 output size
        fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-1] # layer 3 output size

        self.fc = nn.Linear(fc_input_dims, num_classes)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.fc(x)
        return x
```

ì—¬ê¸°ì„œëŠ” W&Bë¥¼ ì‚¬ìš©í•˜ì—¬ runì„ íŠ¸ë˜í‚¹í•˜ê³ , ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ [`run.config`](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-config/Configs_in_W%26B.ipynb) ì˜¤ë¸Œì íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

í•´ë‹¹ `config` ì˜¤ë¸Œì íŠ¸ì˜ ì‚¬ì „(`dict`) ë²„ì „ì€ ë§¤ìš° ìœ ìš©í•œ `metadata` ì¡°ê°ì´ë¯€ë¡œ ë°˜ë“œì‹œ í¬í•¨ì‹œí‚¤ì„¸ìš”.


```python
def build_model_and_log(config):
    with wandb.init(project="artifacts-example", job_type="initialize", config=config) as run:
        config = run.config
        
        model = ConvNet(**config)

        model_artifact = wandb.Artifact(
            "convnet", type="model",
            description="Simple AlexNet style CNN",
            metadata=dict(config))

        torch.save(model.state_dict(), "initialized_model.pth")
        # â• ì•„í‹°íŒ©íŠ¸ì— íŒŒì¼ì„ ì¶”ê°€í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ë²•
        model_artifact.add_file("initialized_model.pth")

        run.save("initialized_model.pth")

        run.log_artifact(model_artifact)

model_config = {"hidden_layer_sizes": [32, 64],
                "kernel_sizes": [3],
                "activation": "ReLU",
                "pool_sizes": [2],
                "dropout": 0.5,
                "num_classes": 10}

build_model_and_log(model_config)
```

#### `artifact.add_file()`

ë°ì´í„°ì…‹ ë¡œê¹… ì˜ˆì‹œì²˜ëŸ¼ `new_file`ë¡œ íŒŒì¼ì„ ì‘ì„±í•¨ê³¼ ë™ì‹œì— `Artifact`ì— ì¶”ê°€í•˜ëŠ” ëŒ€ì‹ , í•œ ë‹¨ê³„ì—ì„œ íŒŒì¼ì„ ì‘ì„±í•˜ê³ (ì—¬ê¸°ì„œëŠ” `torch.save`) ë‹¤ë¥¸ ë‹¨ê³„ì—ì„œ `Artifact`ì— ì¶”ê°€(`add`)í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

> **Best practice**: ì¤‘ë³µì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê°€ëŠ¥í•˜ë©´ `new_file`ì„ ì‚¬ìš©í•˜ì„¸ìš”.

#### ë¡œê·¸ëœ ëª¨ë¸ Artifact ì‚¬ìš©í•˜ê¸°

`dataset`ì— `use_artifact`ë¥¼ í˜¸ì¶œí–ˆë˜ ê²ƒì²˜ëŸ¼, `initialized_model`ì— ëŒ€í•´ í˜¸ì¶œí•˜ì—¬ ë‹¤ë¥¸ `Run`ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ë²ˆì—ëŠ” `model`ì„ íŠ¸ë ˆì´ë‹(`train`)í•´ ë³´ê² ìŠµë‹ˆë‹¤.

ë” ìì„¸í•œ ë‚´ìš©ì€ [W&Bì™€ PyTorch ì—°ë™](https://wandb.me/pytorch-colab) Colabì„ í™•ì¸í•˜ì„¸ìš”.


```python
import wandb
import torch.nn.functional as F

def train(model, train_loader, valid_loader, config):
    optimizer = getattr(torch.optim, config.optimizer)(model.parameters())
    model.train()
    example_ct = 0
    for epoch in range(config.epochs):
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = F.cross_entropy(output, target)
            loss.backward()
            optimizer.step()

            example_ct += len(data)

            if batch_idx % config.batch_log_interval == 0:
                print('Train Epoch: {} [{}/{} ({:.0%})]\tLoss: {:.6f}'.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                    batch_idx / len(train_loader), loss.item()))
                
                train_log(loss, example_ct, epoch)

        # ê° ì—í¬í¬ë§ˆë‹¤ ê²€ì¦ ì„¸íŠ¸ì—ì„œ ëª¨ë¸ í‰ê°€
        loss, accuracy = test(model, valid_loader)  
        test_log(loss, accuracy, example_ct, epoch)

    
def test(model, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.cross_entropy(output, target, reduction='sum')  # ë°°ì¹˜ ì†ì‹¤ í•©ì‚°
            pred = output.argmax(dim=1, keepdim=True)  # ìµœëŒ€ ë¡œê·¸ í™•ë¥ ì˜ ì¸ë±ìŠ¤ íšë“
            correct += pred.eq(target.view_as(pred)).sum()

    test_loss /= len(test_loader.dataset)

    accuracy = 100. * correct / len(test_loader.dataset)
    
    return test_loss, accuracy


def train_log(loss, example_ct, epoch):
    loss = float(loss)

    # ë§ˆë²•ì´ ì¼ì–´ë‚˜ëŠ” ê³³
    with wandb.init(project="artifacts-example", job_type="train") as run:
        run.log({"epoch": epoch, "train/loss": loss}, step=example_ct)
        print(f"Loss after " + str(example_ct).zfill(5) + f" examples: {loss:.3f}")
    

def test_log(loss, accuracy, example_ct, epoch):
    loss = float(loss)
    accuracy = float(accuracy)

    # ë§ˆë²•ì´ ì¼ì–´ë‚˜ëŠ” ê³³
    with wandb.init() as run:
        run.log({"epoch": epoch, "validation/loss": loss, "validation/accuracy": accuracy}, step=example_ct)
        print(f"Loss/accuracy after " + str(example_ct).zfill(5) + f" examples: {loss:.3f}/{accuracy:.3f}")
```

ì´ë²ˆì—ëŠ” ë‘ ê°œì˜ ê°œë³„ì ì¸ `Artifact` ìƒì„± `Run`ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

ì²« ë²ˆì§¸ runì´ `model` íŠ¸ë ˆì´ë‹(`train`)ì„ ë§ˆì¹˜ë©´, ë‘ ë²ˆì§¸ runì€ `test_dataset`ì— ëŒ€í•œ ì„±ëŠ¥ì„ í‰ê°€(`evaluate`)í•˜ì—¬ `trained-model` `Artifact`ë¥¼ ì†Œë¹„í•©ë‹ˆë‹¤.

ë˜í•œ ë„¤íŠ¸ì›Œí¬ê°€ ê°€ì¥ í˜¼ë™í•˜ëŠ”(ì¦‰, `categorical_crossentropy`ê°€ ê°€ì¥ ë†’ì€) 32ê°œì˜ ì˜ˆì‹œë¥¼ ì¶”ì¶œí•  ê²ƒì…ë‹ˆë‹¤.

ì´ëŠ” ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ì˜ ë¬¸ì œë¥¼ ì§„ë‹¨í•˜ëŠ” ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤.


```python
def evaluate(model, test_loader):
    """
    ## í•™ìŠµëœ ëª¨ë¸ í‰ê°€
    """

    loss, accuracy = test(model, test_loader)
    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)

    return loss, accuracy, highest_losses, hardest_examples, true_labels, predictions

def get_hardest_k_examples(model, testing_set, k=32):
    model.eval()

    loader = DataLoader(testing_set, 1, shuffle=False)

    # ë°ì´í„°ì…‹ì˜ ê° í•­ëª©ì— ëŒ€í•œ ì†ì‹¤ê³¼ ì˜ˆì¸¡ê°’ íšë“
    losses = None
    predictions = None
    with torch.no_grad():
        for data, target in loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = F.cross_entropy(output, target)
            pred = output.argmax(dim=1, keepdim=True)
            
            if losses is None:
                losses = loss.view((1, 1))
                predictions = pred
            else:
                losses = torch.cat((losses, loss.view((1, 1))), 0)
                predictions = torch.cat((predictions, pred), 0)

    argsort_loss = torch.argsort(losses, dim=0)

    highest_k_losses = losses[argsort_loss[-k:]]
    hardest_k_examples = testing_set[argsort_loss[-k:]][0]
    true_labels = testing_set[argsort_loss[-k:]][1]
    predicted_labels = predictions[argsort_loss[-k:]]

    return highest_k_losses, hardest_k_examples, true_labels, predicted_labels
```

ì´ ë¡œê¹… í•¨ìˆ˜ë“¤ì€ ìƒˆë¡œìš´ `Artifact` ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë³„ë„ë¡œ ì„¤ëª…í•˜ì§€ ì•Šê² ìŠµë‹ˆë‹¤. ë‹¨ì§€ `Artifact`ë¥¼ ì‚¬ìš©(`use`), ë‹¤ìš´ë¡œë“œ(`download`), ë¡œê·¸(`log`)í•˜ëŠ” ê³¼ì •ì¼ ë¿ì…ë‹ˆë‹¤.


```python
from torch.utils.data import DataLoader

def train_and_log(config):

    with wandb.init(project="artifacts-example", job_type="train", config=config) as run:
        config = run.config

        data = run.use_artifact('mnist-preprocess:latest')
        data_dir = data.download()

        training_dataset =  read(data_dir, "training")
        validation_dataset = read(data_dir, "validation")

        train_loader = DataLoader(training_dataset, batch_size=config.batch_size)
        validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size)
        
        model_artifact = run.use_artifact("convnet:latest")
        model_dir = model_artifact.download()
        model_path = os.path.join(model_dir, "initialized_model.pth")
        model_config = model_artifact.metadata
        config.update(model_config)

        model = ConvNet(**model_config)
        model.load_state_dict(torch.load(model_path))
        model = model.to(device)
 
        train(model, train_loader, validation_loader, config)

        model_artifact = wandb.Artifact(
            "trained-model", type="model",
            description="Trained NN model",
            metadata=dict(model_config))

        torch.save(model.state_dict(), "trained_model.pth")
        model_artifact.add_file("trained_model.pth")
        run.save("trained_model.pth")

        run.log_artifact(model_artifact)

    return model

    
def evaluate_and_log(config=None):
    
    with wandb.init(project="artifacts-example", job_type="report", config=config) as run:
        data = run.use_artifact('mnist-preprocess:latest')
        data_dir = data.download()
        testing_set = read(data_dir, "test")

        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=128, shuffle=False)

        model_artifact = run.use_artifact("trained-model:latest")
        model_dir = model_artifact.download()
        model_path = os.path.join(model_dir, "trained_model.pth")
        model_config = model_artifact.metadata

        model = ConvNet(**model_config)
        model.load_state_dict(torch.load(model_path))
        model.to(device)

        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)

        run.summary.update({"loss": loss, "accuracy": accuracy})

        run.log({"high-loss-examples":
            [wandb.Image(hard_example, caption=str(int(pred)) + "," +  str(int(label)))
             for hard_example, pred, label in zip(hardest_examples, preds, true_labels)]})
```


```python
train_config = {"batch_size": 128,
                "epochs": 5,
                "batch_log_interval": 25,
                "optimizer": "Adam"}

model = train_and_log(train_config)
evaluate_and_log()
```