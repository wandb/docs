---
title: Hugging Face Diffusers
---

import { ColabLink } from '/snippets/ko/_includes/colab-link.mdx';

<ColabLink url="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/diffusers/lcm-diffusers.ipynb" />

[Hugging Face Diffusers](https://huggingface.co/docs/diffusers)는 이미지, 오디오, 심지어 분자의 3D 구조까지 생성할 수 있는 최첨단 사전 학습 확산 모델용 대표 라이브러리입니다. W&amp;B와의 통합을 통해 사용 편의성을 해치지 않으면서도 대화형 중앙 대시보드에서 풍부하고 유연한 실험 추적, 미디어 시각화, 파이프라인 아키텍처, 구성 관리 기능을 제공할 수 있습니다.

<div id="next-level-logging-in-just-two-lines">
  ## 단 두 줄로 시작하는 한 차원 높은 로깅
</div>

프롬프트, 네거티브 프롬프트, 생성된 미디어, 그리고 실험에 관련된 모든 설정을 코드 두 줄만 추가해서 로깅할 수 있습니다. 아래는 로깅을 시작하기 위한 두 줄의 코드입니다:

```python
# autolog 함수 가져오기
from wandb.integration.diffusers import autolog

# 파이프라인 호출 전에 autolog 호출
autolog(init=dict(project="diffusers_logging"))
```

<Frame caption="실험 결과가 기록되는 예시입니다.">
  <img src="/images/integrations/diffusers-autolog-4.gif" alt="실험 결과 기록" />
</Frame>

<div id="get-started">
  ## 시작하기
</div>

1. `diffusers`, `transformers`, `accelerate`, `wandb`를 설치합니다.

   * 명령줄:

     ```shell
     pip install --upgrade diffusers transformers accelerate wandb
     ```

   * 노트북:

     ```bash
     !pip install --upgrade diffusers transformers accelerate wandb
     ```

2. `autolog`를 사용해 W&amp;B 실행을 초기화하고 [지원되는 모든 파이프라인 호출](https://github.com/wandb/wandb/blob/main/wandb/integration/diffusers/autologger.py#L12-L72)의 입력과 출력을 자동으로 추적합니다.

   `autolog()` 함수를 호출할 때 `init` 매개변수를 함께 전달할 수 있으며, 이 매개변수에는 [`wandb.init()`](/ko/models/ref/python/functions/init)에 필요한 매개변수들을 담은 딕셔너리를 전달합니다.

   `autolog()`를 호출하면 W&amp;B 실행을 초기화하고, [지원되는 모든 파이프라인 호출](https://github.com/wandb/wandb/blob/main/wandb/integration/diffusers/autologger.py#L12-L72)의 입력과 출력을 자동으로 추적합니다.

   * 각 파이프라인 호출은 워크스페이스 내의 개별 [table](/ko/models/tables/)로 추적되며, 해당 파이프라인 호출과 연관된 config는 그 실행의 config에 있는 workflow 목록에 추가됩니다.
   * 프롬프트, 네거티브 프롬프트, 생성된 미디어는 [`wandb.Table`](/ko/models/tables/)에 로깅됩니다.
   * 시드와 파이프라인 아키텍처를 포함해 실험과 연관된 기타 모든 config는 실행의 config 섹션에 저장됩니다.
   * 각 파이프라인 호출에서 생성된 미디어는 실행의 [media panels](/ko/models/track/log/media/)에도 로깅됩니다.

   <Note>
     [지원되는 파이프라인 호출 목록](https://github.com/wandb/wandb/blob/main/wandb/integration/diffusers/autologger.py#L12-L72)을 확인할 수 있습니다. 이 통합에 대해 새로운 기능을 요청하거나 관련 버그를 보고하려면 [W&amp;B GitHub issues 페이지](https://github.com/wandb/wandb/issues)에 이슈를 생성하세요.
   </Note>

<div id="examples">
  ## 예제
</div>

<div id="autologging">
  ### 자동 로깅
</div>

다음은 autolog 기능이 동작하는 간단한 end-to-end 예제입니다:

<Tabs>
  <Tab title="스크립트">
    ```python
    import torch
    from diffusers import DiffusionPipeline

    # autolog 함수를 import합니다
    from wandb.integration.diffusers import autolog

    # pipeline을 호출하기 전에 autolog를 호출합니다
    autolog(init=dict(project="diffusers_logging"))

    # diffusion pipeline을 초기화합니다
    pipeline = DiffusionPipeline.from_pretrained(
        "stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16
    ).to("cuda")

    # prompt, negative prompt, seed를 정의합니다.
    prompt = ["a photograph of an astronaut riding a horse", "a photograph of a dragon"]
    negative_prompt = ["ugly, deformed", "ugly, deformed"]
    generator = torch.Generator(device="cpu").manual_seed(10)

    # 이미지를 생성하기 위해 pipeline을 호출합니다
    images = pipeline(
        prompt,
        negative_prompt=negative_prompt,
        num_images_per_prompt=2,
        generator=generator,
    )
    ```
  </Tab>

  <Tab title="노트북">
    ```python
    import torch
    from diffusers import DiffusionPipeline

    import wandb

    # autolog 함수를 import합니다
    from wandb.integration.diffusers import autolog

    run = wandb.init()

    # pipeline을 호출하기 전에 autolog를 호출합니다
    autolog(init=dict(project="diffusers_logging"))

    # diffusion pipeline을 초기화합니다
    pipeline = DiffusionPipeline.from_pretrained(
        "stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16
    ).to("cuda")

    # prompt, negative prompt, seed를 정의합니다.
    prompt = ["a photograph of an astronaut riding a horse", "a photograph of a dragon"]
    negative_prompt = ["ugly, deformed", "ugly, deformed"]
    generator = torch.Generator(device="cpu").manual_seed(10)

    # 이미지를 생성하기 위해 pipeline을 호출합니다
    images = pipeline(
        prompt,
        negative_prompt=negative_prompt,
        num_images_per_prompt=2,
        generator=generator,
    )

    # 실험을 종료합니다
    run.finish()
    ```
  </Tab>
</Tabs>

* 단일 실험의 결과:

  <Frame>
    <img src="/images/integrations/diffusers-autolog-2.gif" alt="실험 결과 로깅" />
  </Frame>

* 여러 실험의 결과:

  <Frame>
    <img src="/images/integrations/diffusers-autolog-1.gif" alt="실험 결과 로깅" />
  </Frame>

* 실험 구성:

  <Frame>
    <img src="/images/integrations/diffusers-autolog-3.gif" alt="실험 구성 로깅" />
  </Frame>

<Note>
  IPython 노트북 환경에서 pipeline을 호출한 이후에는 [`wandb.Run.finish()`](/ko/models/ref/python/functions/finish)를 명시적으로 호출해야 합니다. Python 스크립트를 실행할 때는 필요하지 않습니다.
</Note>

<div id="tracking-multi-pipeline-workflows">
  ### 다중 파이프라인 워크플로 추적
</div>

이 섹션에서는 일반적인 [Stable Diffusion XL + Refiner](https://huggingface.co/docs/diffusers/using-diffusers/sdxl#base-to-refiner-model) 워크플로에서 autolog 기능 사용 예시를 보여 줍니다. 이 워크플로에서는 [`StableDiffusionXLPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl)이 생성한 잠재 표현(latent)이 해당 refiner에 의해 정제됩니다.

<Tabs>
  <Tab title="Python 스크립트">
    ```python
    import torch
    from diffusers import StableDiffusionXLImg2ImgPipeline, StableDiffusionXLPipeline
    from wandb.integration.diffusers import autolog

    # SDXL 기본 파이프라인 초기화
    base_pipeline = StableDiffusionXLPipeline.from_pretrained(
        "stabilityai/stable-diffusion-xl-base-1.0",
        torch_dtype=torch.float16,
        variant="fp16",
        use_safetensors=True,
    )
    base_pipeline.enable_model_cpu_offload()

    # SDXL 리파이너 파이프라인 초기화
    refiner_pipeline = StableDiffusionXLImg2ImgPipeline.from_pretrained(
        "stabilityai/stable-diffusion-xl-refiner-1.0",
        text_encoder_2=base_pipeline.text_encoder_2,
        vae=base_pipeline.vae,
        torch_dtype=torch.float16,
        use_safetensors=True,
        variant="fp16",
    )
    refiner_pipeline.enable_model_cpu_offload()

    prompt = "a photo of an astronaut riding a horse on mars"
    negative_prompt = "static, frame, painting, illustration, sd character, low quality, low resolution, greyscale, monochrome, nose, cropped, lowres, jpeg artifacts, deformed iris, deformed pupils, bad eyes, semi-realistic worst quality, bad lips, deformed mouth, deformed face, deformed fingers, deformed toes standing still, posing"

    # 무작위성을 제어하여 실험을 재현 가능하게 만듭니다.
    # 시드는 WandB에 자동으로 기록됩니다.
    seed = 42
    generator_base = torch.Generator(device="cuda").manual_seed(seed)
    generator_refiner = torch.Generator(device="cuda").manual_seed(seed)

    # Diffusers용 WandB Autolog를 호출합니다. 프롬프트, 생성된 이미지,
    # 파이프라인 아키텍처 및 관련된 모든 실험 설정이 W&B에 자동으로 기록되어
    # 이미지 생성 실험을 쉽게 재현, 공유 및 분석할 수 있습니다.
    autolog(init=dict(project="sdxl"))

    # 기본 파이프라인을 호출하여 잠재 벡터 생성
    image = base_pipeline(
        prompt=prompt,
        negative_prompt=negative_prompt,
        output_type="latent",
        generator=generator_base,
    ).images[0]

    # 리파이너 파이프라인을 호출하여 정제된 이미지 생성
    image = refiner_pipeline(
        prompt=prompt,
        negative_prompt=negative_prompt,
        image=image[None, :],
        generator=generator_refiner,
    ).images[0]
    ```
  </Tab>

  <Tab title="노트북">
    ```python
    import torch
    from diffusers import StableDiffusionXLImg2ImgPipeline, StableDiffusionXLPipeline

    import wandb
    from wandb.integration.diffusers import autolog

    run = wandb.init()

    # SDXL 기본 파이프라인 초기화
    base_pipeline = StableDiffusionXLPipeline.from_pretrained(
        "stabilityai/stable-diffusion-xl-base-1.0",
        torch_dtype=torch.float16,
        variant="fp16",
        use_safetensors=True,
    )
    base_pipeline.enable_model_cpu_offload()

    # SDXL 리파이너 파이프라인 초기화
    refiner_pipeline = StableDiffusionXLImg2ImgPipeline.from_pretrained(
        "stabilityai/stable-diffusion-xl-refiner-1.0",
        text_encoder_2=base_pipeline.text_encoder_2,
        vae=base_pipeline.vae,
        torch_dtype=torch.float16,
        use_safetensors=True,
        variant="fp16",
    )
    refiner_pipeline.enable_model_cpu_offload()

    prompt = "a photo of an astronaut riding a horse on mars"
    negative_prompt = "static, frame, painting, illustration, sd character, low quality, low resolution, greyscale, monochrome, nose, cropped, lowres, jpeg artifacts, deformed iris, deformed pupils, bad eyes, semi-realistic worst quality, bad lips, deformed mouth, deformed face, deformed fingers, deformed toes standing still, posing"

    # 무작위성을 제어하여 실험을 재현 가능하게 만듭니다.
    # 시드는 WandB에 자동으로 기록됩니다.
    seed = 42
    generator_base = torch.Generator(device="cuda").manual_seed(seed)
    generator_refiner = torch.Generator(device="cuda").manual_seed(seed)

    # Diffusers용 WandB Autolog를 호출합니다. 프롬프트, 생성된 이미지,
    # 파이프라인 아키텍처 및 모든 관련 실험 설정이 W&B에 자동으로 기록되어
    # 이미지 생성 실험을 쉽게 재현, 공유 및 분석할 수 있습니다.
    autolog(init=dict(project="sdxl"))

    # 기본 파이프라인을 호출하여 잠재 벡터 생성
    image = base_pipeline(
        prompt=prompt,
        negative_prompt=negative_prompt,
        output_type="latent",
        generator=generator_base,
    ).images[0]

    # 리파이너 파이프라인을 호출하여 정제된 이미지 생성
    image = refiner_pipeline(
        prompt=prompt,
        negative_prompt=negative_prompt,
        image=image[None, :],
        generator=generator_refiner,
    ).images[0]

    # 실험 종료
    run.finish()
    ```
  </Tab>
</Tabs>

* Stable Diffusion XL + Refiner 실험 예시:
  <Frame>
    <img src="/images/integrations/diffusers-autolog-6.gif" alt="Stable Diffusion XL experiment tracking" />
  </Frame>

<div id="more-resources">
  ## 추가 리소스
</div>

* [Stable Diffusion용 프롬프트 엔지니어링 가이드](https://wandb.ai/geekyrakshit/diffusers-prompt-engineering/reports/A-Guide-to-Prompt-Engineering-for-Stable-Diffusion--Vmlldzo1NzY4NzQ3)
* [PIXART-α: 텍스트-투-이미지 생성용 Diffusion Transformer 모델](https://wandb.ai/geekyrakshit/pixart-alpha/reports/PIXART-A-Diffusion-Transformer-Model-for-Text-to-Image-Generation--Vmlldzo2MTE1NzM3)