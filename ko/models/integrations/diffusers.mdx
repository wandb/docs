---
title: Hugging Face Diffusers
---

import { ColabLink } from '/snippets/ko/_includes/colab-link.mdx';

<ColabLink url="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/diffusers/lcm-diffusers.ipynb" />

[Hugging Face Diffusers](https://huggingface.co/docs/diffusers/index)는 최첨단 사전 학습 디퓨전 모델을 사용해 이미지, 오디오, 심지어 분자의 3D 구조까지 생성할 수 있는 대표적인 라이브러리입니다. W&amp;B 인테그레이션은 이러한 사용 편의성을 해치지 않으면서, 풍부하고 유연한 실험 추적, 미디어 시각화, 파이프라인 아키텍처, 설정 관리를 인터랙티브한 중앙 집중식 대시보드에 더해 줍니다.

<div id="next-level-logging-in-just-two-lines">
  ## 코드 두 줄로 한 단계 더 진화한 로깅
</div>

코드 두 줄만 추가하면 실험과 관련된 모든 프롬프트, 네거티브 프롬프트, 생성된 미디어, 그리고 설정(config)을 로깅할 수 있습니다. 로깅을 시작하기 위한 코드 두 줄은 다음과 같습니다:

```python
# autolog 함수 가져오기
from wandb.integration.diffusers import autolog

# 파이프라인 호출 전에 autolog 호출
autolog(init=dict(project="diffusers_logging"))
```

<Frame caption="실험 결과가 어떻게 로그로 기록되는지 보여주는 예시입니다.">
  <img src="/images/integrations/diffusers-autolog-4.gif" alt="실험 결과 로깅 예시" />
</Frame>

<div id="get-started">
  ## 시작하기
</div>

1. `diffusers`, `transformers`, `accelerate`, 그리고 `wandb`를 설치합니다.

   * 명령줄:

     ```shell
     pip install --upgrade diffusers transformers accelerate wandb
     ```

   * 노트북:

     ```bash
     !pip install --upgrade diffusers transformers accelerate wandb
     ```

2. `autolog`를 사용하여 W&amp;B run을 초기화하고, [지원되는 모든 파이프라인 호출](https://github.com/wandb/wandb/blob/main/wandb/integration/diffusers/autologger.py#L12-L72)의 입력과 출력을 자동으로 추적합니다.

   `autolog()` 함수를 호출할 때 `init` 매개변수를 함께 사용할 수 있으며, 이 매개변수에는 [`wandb.init()`](/ko/models/ref/python/functions/init)에 필요한 파라미터 사전을 전달할 수 있습니다.

   `autolog()`를 호출하면 W&amp;B run이 초기화되고, [지원되는 모든 파이프라인 호출](https://github.com/wandb/wandb/blob/main/wandb/integration/diffusers/autologger.py#L12-L72)의 입력과 출력이 자동으로 추적됩니다.

   * 각 파이프라인 호출은 워크스페이스 내의 개별 [테이블](/ko/models/tables/)에 추적되며, 해당 파이프라인 호출과 관련된 config는 해당 run의 config에 있는 워크플로 목록에 추가됩니다.
   * 프롬프트, 네거티브 프롬프트, 생성된 미디어는 [`wandb.Table`](/ko/models/tables/)에 로깅됩니다.
   * 시드와 파이프라인 아키텍처를 포함해 실험과 관련된 나머지 모든 config는 해당 run의 config 섹션에 저장됩니다.
   * 각 파이프라인 호출로 생성된 미디어는 해당 run의 [media panels](/ko/models/track/log/media/)에도 로깅됩니다.

   <Note>
     [지원되는 파이프라인 호출 목록](https://github.com/wandb/wandb/blob/main/wandb/integration/diffusers/autologger.py#L12-L72)을 확인할 수 있습니다. 이 인테그레이션에 대한 새로운 기능을 요청하거나 관련 버그를 신고하려면, [W&amp;B GitHub Issues 페이지](https://github.com/wandb/wandb/issues)에 이슈를 생성하세요.
   </Note>

<div id="examples">
  ## 예제
</div>

<div id="autologging">
  ### 자동 로깅(Autologging)
</div>

다음은 자동 로깅이 동작하는 모습을 보여 주는 간단한 엔드 투 엔드 예시입니다:

<Tabs>
  <Tab title="스크립트">
    ```python
    import torch
    from diffusers import DiffusionPipeline

    # autolog 함수를 임포트합니다
    from wandb.integration.diffusers import autolog

    # 파이프라인을 호출하기 전에 autolog를 호출합니다
    autolog(init=dict(project="diffusers_logging"))

    # Diffusion 파이프라인을 초기화합니다
    pipeline = DiffusionPipeline.from_pretrained(
        "stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16
    ).to("cuda")

    # 프롬프트, 네거티브 프롬프트, 시드를 정의합니다
    prompt = ["a photograph of an astronaut riding a horse", "a photograph of a dragon"]
    negative_prompt = ["ugly, deformed", "ugly, deformed"]
    generator = torch.Generator(device="cpu").manual_seed(10)

    # 파이프라인을 호출해 이미지를 생성합니다
    images = pipeline(
        prompt,
        negative_prompt=negative_prompt,
        num_images_per_prompt=2,
        generator=generator,
    )
    ```
  </Tab>

  <Tab title="노트북">
    ```python
    import torch
    from diffusers import DiffusionPipeline

    import wandb

    # autolog 함수를 임포트합니다
    from wandb.integration.diffusers import autolog

    run = wandb.init()

    # 파이프라인을 호출하기 전에 autolog를 호출합니다
    autolog(init=dict(project="diffusers_logging"))

    # Diffusion 파이프라인을 초기화합니다
    pipeline = DiffusionPipeline.from_pretrained(
        "stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16
    ).to("cuda")

    # 프롬프트, 네거티브 프롬프트, 시드를 정의합니다
    prompt = ["a photograph of an astronaut riding a horse", "a photograph of a dragon"]
    negative_prompt = ["ugly, deformed", "ugly, deformed"]
    generator = torch.Generator(device="cpu").manual_seed(10)

    # 파이프라인을 호출해 이미지를 생성합니다
    images = pipeline(
        prompt,
        negative_prompt=negative_prompt,
        num_images_per_prompt=2,
        generator=generator,
    )

    # 실험을 종료합니다
    run.finish()
    ```
  </Tab>
</Tabs>

* 단일 실험 결과:

  <Frame>
    <img src="/images/integrations/diffusers-autolog-2.gif" alt="실험 결과 로깅" />
  </Frame>

* 여러 실험 결과:

  <Frame>
    <img src="/images/integrations/diffusers-autolog-1.gif" alt="실험 결과 로깅" />
  </Frame>

* 실험 설정(config):

  <Frame>
    <img src="/images/integrations/diffusers-autolog-3.gif" alt="실험 설정 로깅" />
  </Frame>

<Note>
  파이프라인을 호출한 뒤 IPython 노트북 환경에서 코드를 실행할 때는 [`wandb.Run.finish()`](/ko/models/ref/python/functions/finish)를 명시적으로 호출해야 합니다. 파이썬 스크립트를 실행할 때는 이 호출이 필요하지 않습니다.
</Note>

<div id="tracking-multi-pipeline-workflows">
  ### 다중 파이프라인 워크플로 추적
</div>

이 섹션에서는 일반적인 [Stable Diffusion XL + Refiner](https://huggingface.co/docs/diffusers/using-diffusers/sdxl#base-to-refiner-model) 워크플로에서 autolog 기능 사용 예시를 보여줍니다. 이 워크플로에서는 [`StableDiffusionXLPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl)이 생성한 latent 벡터가 해당 Refiner에 의해 정제됩니다.

<Tabs>
  <Tab title="Python 스크립트">
    ```python
    import torch
    from diffusers import StableDiffusionXLImg2ImgPipeline, StableDiffusionXLPipeline
    from wandb.integration.diffusers import autolog

    # SDXL 기본 파이프라인 초기화
    base_pipeline = StableDiffusionXLPipeline.from_pretrained(
        "stabilityai/stable-diffusion-xl-base-1.0",
        torch_dtype=torch.float16,
        variant="fp16",
        use_safetensors=True,
    )
    base_pipeline.enable_model_cpu_offload()

    # SDXL 리파이너 파이프라인 초기화
    refiner_pipeline = StableDiffusionXLImg2ImgPipeline.from_pretrained(
        "stabilityai/stable-diffusion-xl-refiner-1.0",
        text_encoder_2=base_pipeline.text_encoder_2,
        vae=base_pipeline.vae,
        torch_dtype=torch.float16,
        use_safetensors=True,
        variant="fp16",
    )
    refiner_pipeline.enable_model_cpu_offload()

    prompt = "a photo of an astronaut riding a horse on mars"
    negative_prompt = "static, frame, painting, illustration, sd character, low quality, low resolution, greyscale, monochrome, nose, cropped, lowres, jpeg artifacts, deformed iris, deformed pupils, bad eyes, semi-realistic worst quality, bad lips, deformed mouth, deformed face, deformed fingers, deformed toes standing still, posing"

    # 무작위성을 제어하여 실험을 재현 가능하게 만듭니다.
    # 시드는 WandB에 자동으로 기록됩니다.
    seed = 42
    generator_base = torch.Generator(device="cuda").manual_seed(seed)
    generator_refiner = torch.Generator(device="cuda").manual_seed(seed)

    # Diffusers용 WandB Autolog를 호출합니다. 프롬프트, 생성된 이미지,
    # 파이프라인 아키텍처 및 관련된 모든 실험 설정이 W&B에 자동으로 기록되어
    # 이미지 생성 실험을 쉽게 재현, 공유 및 분석할 수 있습니다.
    autolog(init=dict(project="sdxl"))

    # 기본 파이프라인을 호출하여 잠재 벡터 생성
    image = base_pipeline(
        prompt=prompt,
        negative_prompt=negative_prompt,
        output_type="latent",
        generator=generator_base,
    ).images[0]

    # 리파이너 파이프라인을 호출하여 정제된 이미지 생성
    image = refiner_pipeline(
        prompt=prompt,
        negative_prompt=negative_prompt,
        image=image[None, :],
        generator=generator_refiner,
    ).images[0]
    ```
  </Tab>

  <Tab title="노트북">
    ```python
    import torch
    from diffusers import StableDiffusionXLImg2ImgPipeline, StableDiffusionXLPipeline

    import wandb
    from wandb.integration.diffusers import autolog

    run = wandb.init()

    # SDXL 기본 파이프라인 초기화
    base_pipeline = StableDiffusionXLPipeline.from_pretrained(
        "stabilityai/stable-diffusion-xl-base-1.0",
        torch_dtype=torch.float16,
        variant="fp16",
        use_safetensors=True,
    )
    base_pipeline.enable_model_cpu_offload()

    # SDXL 리파이너 파이프라인 초기화
    refiner_pipeline = StableDiffusionXLImg2ImgPipeline.from_pretrained(
        "stabilityai/stable-diffusion-xl-refiner-1.0",
        text_encoder_2=base_pipeline.text_encoder_2,
        vae=base_pipeline.vae,
        torch_dtype=torch.float16,
        use_safetensors=True,
        variant="fp16",
    )
    refiner_pipeline.enable_model_cpu_offload()

    prompt = "a photo of an astronaut riding a horse on mars"
    negative_prompt = "static, frame, painting, illustration, sd character, low quality, low resolution, greyscale, monochrome, nose, cropped, lowres, jpeg artifacts, deformed iris, deformed pupils, bad eyes, semi-realistic worst quality, bad lips, deformed mouth, deformed face, deformed fingers, deformed toes standing still, posing"

    # 무작위성을 제어하여 실험을 재현 가능하게 만듭니다.
    # 시드는 WandB에 자동으로 기록됩니다.
    seed = 42
    generator_base = torch.Generator(device="cuda").manual_seed(seed)
    generator_refiner = torch.Generator(device="cuda").manual_seed(seed)

    # Diffusers용 WandB Autolog를 호출합니다. 프롬프트, 생성된 이미지,
    # 파이프라인 아키텍처 및 관련된 모든 실험 설정이 W&B에 자동으로 기록되어
    # 이미지 생성 실험을 쉽게 재현, 공유 및 분석할 수 있습니다.
    autolog(init=dict(project="sdxl"))

    # 기본 파이프라인을 호출하여 잠재 벡터 생성
    image = base_pipeline(
        prompt=prompt,
        negative_prompt=negative_prompt,
        output_type="latent",
        generator=generator_base,
    ).images[0]

    # 리파이너 파이프라인을 호출하여 정제된 이미지 생성
    image = refiner_pipeline(
        prompt=prompt,
        negative_prompt=negative_prompt,
        image=image[None, :],
        generator=generator_refiner,
    ).images[0]

    # 실험 종료
    run.finish()
    ```
  </Tab>
</Tabs>

* Stable Diffusion XL + Refiner 실험 예시:
  <Frame>
    <img src="/images/integrations/diffusers-autolog-6.gif" alt="Stable Diffusion XL 실험 추적" />
  </Frame>

<div id="more-resources">
  ## 추가 자료
</div>

* [Stable Diffusion용 프롬프트 엔지니어링 가이드](https://wandb.ai/geekyrakshit/diffusers-prompt-engineering/reports/A-Guide-to-Prompt-Engineering-for-Stable-Diffusion--Vmlldzo1NzY4NzQ3)
* [PIXART-α: 텍스트-투-이미지 생성을 위한 디퓨전 트랜스포머 모델](https://wandb.ai/geekyrakshit/pixart-alpha/reports/PIXART-A-Diffusion-Transformer-Model-for-Text-to-Image-Generation--Vmlldzo2MTE1NzM3)