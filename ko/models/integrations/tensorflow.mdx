---
title: TensorFlow
---

import { ColabLink } from '/snippets/ko/_includes/colab-link.mdx';

<ColabLink url="https://colab.research.google.com/drive/1JCpAbjkCFhYMT7LCQ399y35TS3jlMpvM" />


<div id="get-started">
  ## 시작하기
</div>

이미 TensorBoard를 사용 중이라면 wandb와 쉽게 연동할 수 있습니다.

```python
import tensorflow as tf
import wandb
```


<div id="log-custom-metrics">
  ## 커스텀 메트릭 로깅
</div>

TensorBoard에 기록되지 않는 추가 커스텀 메트릭을 로깅해야 하는 경우, 코드에서 `run.log()`를 호출하면 됩니다. 예: `run.log({"custom": 0.8}) `

TensorBoard와 동기화할 때는 `run.log()`의 step 인자를 설정할 수 없습니다. 다른 step 값을 설정하려면, 다음과 같이 step 메트릭과 함께 메트릭을 로깅하면 됩니다.

```python
with wandb.init(config=tf.flags.FLAGS, sync_tensorboard=True) as run:
    run.log({"custom": 0.8, "global_step":global_step}, step=global_step)
```


<div id="tensorflow-estimators-hook">
  ## TensorFlow Estimator 훅
</div>

로그되는 항목을 더 세밀하게 제어하고 싶다면, wandb는 TensorFlow Estimator용 훅도 제공합니다. 이 훅은 그래프의 모든 `tf.summary` 값을 기록합니다.

```python
import tensorflow as tf
import wandb

run = wandb.init(config=tf.FLAGS)

estimator.train(hooks=[wandb.tensorflow.WandbHook(steps_per_log=1000)])
run.finish()
```


<div id="log-manually">
  ## 수동으로 로그 기록하기
</div>

TensorFlow에서 메트릭을 기록하는 가장 간단한 방법은 TensorFlow 로거를 사용해 `tf.summary`를 로그로 남기는 것입니다:

```python
import wandb
run = wandb.init(config=tf.flags.FLAGS, sync_tensorboard=True)
with tf.Session() as sess:
    # ...
    wandb.tensorflow.log(tf.summary.merge_all())
```

TensorFlow 2에서는 커스텀 루프를 사용해 모델을 트레이닝할 때 `tf.GradientTape`를 사용하는 것이 권장됩니다. 자세한 내용은 [TensorFlow custom training walkthrough](https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough)를 참고하세요. 커스텀 TensorFlow 트레이닝 루프에서 메트릭을 로깅하기 위해 `wandb`를 사용하려면 다음 코드 스니펫을 참고하세요:

```python
    with tf.GradientTape() as tape:
        # 확률 가져오기
        predictions = model(features)
        # 손실 계산
        loss = loss_func(labels, predictions)

    # 메트릭 로깅
    run.log("loss": loss.numpy())
    # 그래디언트 가져오기
    gradients = tape.gradient(loss, model.trainable_variables)
    # 가중치 업데이트
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```

[TensorFlow 2에서 트레이닝 루프를 커스터마이징하는 전체 예제](https://www.wandb.com/articles/wandb-customizing-training-loops-in-tensorflow-2)가 제공됩니다.


<div id="how-is-wb-different-from-tensorboard">
  ## W&B는 TensorBoard와 어떻게 다른가요?
</div>

공동 창업자들이 W&B 작업을 시작했을 때, OpenAI에서 TensorBoard로 불편을 겪던 사용자들을 위한 도구를 만들어야겠다고 생각했습니다. 다음은 우리가 특히 개선에 집중한 몇 가지입니다:

1. **모델 재현**: W&B는 실험, 탐색, 그리고 나중에 모델을 재현하는 데 강력합니다. 메트릭뿐 아니라 하이퍼파라미터와 코드 버전까지 모두 기록하고, 버전 관리 상태와 모델 체크포인트도 저장해 주기 때문에 프로젝트를 재현 가능하게 만들 수 있습니다.
2. **자동 구성**: 협업자의 프로젝트를 이어받거나, 휴가에서 돌아왔거나, 오래된 프로젝트를 다시 꺼내는 상황에서도, W&B는 시도된 모든 모델을 쉽게 파악할 수 있게 해 주어 아무도 실험을 다시 돌리느라 시간, GPU 자원이나 탄소 배출을 낭비하지 않게 도와줍니다.
3. **빠르고 유연한 인테그레이션**: 5분 만에 W&B를 프로젝트에 추가할 수 있습니다. 무료 오픈 소스 Python 패키지를 설치하고 코드에 몇 줄만 추가하면, 모델을 실행할 때마다 깔끔하게 로깅된 메트릭과 기록을 확인할 수 있습니다.
4. **지속적이고 중앙화된 대시보드**: 로컬 머신, 공유 랩 클러스터, 클라우드의 스팟 인스턴스 등 어디에서 모델을 학습하든, 결과는 동일한 중앙 대시보드로 공유됩니다. 다른 머신에서 TensorBoard 파일을 복사하고 정리하는 데 시간을 쓸 필요가 없습니다.
5. **강력한 테이블**: 서로 다른 모델의 결과를 검색, 필터링, 정렬, 그룹화할 수 있습니다. 수천 개의 모델 버전을 훑어보며 다양한 작업에 대해 성능이 가장 좋은 모델을 찾기 쉽습니다. TensorBoard는 대규모 프로젝트에서 잘 동작하도록 설계되지 않았습니다.
6. **협업 도구**: W&B를 사용해 복잡한 머신러닝 프로젝트를 체계적으로 관리할 수 있습니다. W&B 링크를 공유하는 것은 매우 간단하며, 비공개 Teams를 사용해 모두가 결과를 공유 프로젝트로 전송하게 할 수 있습니다. 또한 Reports를 통해 협업을 지원하는데, 인터랙티브 시각화를 추가하고 작업 내용을 마크다운으로 설명할 수 있습니다. 이는 작업 로그를 남기거나, 지도교수와 결과를 공유하거나, 연구실이나 팀에 결과를 발표하는 데 훌륭한 방법입니다.

[무료 계정](https://wandb.ai)으로 시작하세요

<div id="examples">
  ## 예시
</div>

인테그레이션이 어떻게 작동하는지 확인할 수 있도록 몇 가지 예시를 준비했습니다:

* [GitHub 예시](https://github.com/wandb/examples/blob/master/examples/tensorflow/tf-estimator-mnist/mnist.py): TensorFlow Estimator를 사용한 MNIST 예시
* [GitHub 예시](https://github.com/wandb/examples/blob/master/examples/tensorflow/tf-cnn-fashion/train.py): 순수 TensorFlow를 사용한 Fashion MNIST 예시
* [W&amp;B 대시보드](https://app.wandb.ai/l2k2/examples-tf-estimator-mnist/runs/p0ifowcb): W&amp;B에서 결과 보기
* TensorFlow 2에서 트레이닝 루프 커스터마이징 - [글](https://www.wandb.com/articles/wandb-customizing-training-loops-in-tensorflow-2) | [대시보드](https://app.wandb.ai/sayakpaul/custom_training_loops_tf)