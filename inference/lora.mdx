---
title: "Serverless LoRA Inference"
linkTitle: "Serverless LoRA Inference"
description: >
  Bring your own custom LoRA for serving fine-tuned models on W&B Inference.
---

LoRA (Low-Rank Adaptation) weights are lightweight model updates that allow you to efficiently fine-tune large language models. Instead of updating all of a model's parameters, LoRA methods train small sets of additional weights, significantly reducing compute and storage requirements. These adapter weights can then be merged with or dynamically applied to a base model at inference time.

W&B Inference lets you serve your custom fine-tuned LoRA weights on fully managed CoreWeave GPU clusters, with no infrastructure to provision or manage. 

## At a glance

1. Upload your adapter weights as a W&B artifact 
2. Use the artifact URI as the model name in the OpenAI-compatible API
3. We hot-load your adapter onto a preloaded base model and return the response

Minimal example (after publishing an adapter):

```python
from openai import OpenAI

model_name = f"wandb-artifact:///{TEAM}/{PROJECT}/qwen_lora:latest"

client = OpenAI(
    base_url="https://api.inference.wandb.ai/v1",
    api_key=API_KEY,
    project=f"{TEAM}/{PROJECT}",
)

resp = client.chat.completions.create(
    model=model_name,
    messages=[{"role": "user", "content": "Say 'Hello World!'"}],
)
print(resp.choices[0].message.content)
```

## Choose your path

- Option A: Train with W&B Training (serverless RL). Your adapter becomes a W&B artifact; use the artifact URI directly. Learn more in [W&B Training](/training).
- Option B: Upload your own adapter. Upload as a W&B artifact of type `lora` and include `wandb.base_model` metadata matching a supported base model.

## Upload your own adapter

Here’s a Quickstart for you to create and call your first LoRA: [Getting Started notebook](https://wandb.me/lora_nb)

```python
import wandb

run = wandb.init(entity=TEAM, project=PROJECT)

artifact = wandb.Artifact(
    "qwen_lora",
    type="lora",
    metadata={"wandb.base_model": "Qwen/Qwen2.5-14B-Instruct"},
    storage_region="coreweave-us",
)

artifact.add_dir("<path-to-lora-weights>")
run.log_artifact(artifact)
```

## Requirements

- Artifact type: `lora`
- Files: PEFT-style adapters, see [Getting Started notebook](https://wandb.me/lora_nb)
- Metadata: `wandb.base_model` must exactly match a supported base model string
- Storage: `storage_region="coreweave-us"` (required for low-latency serving)

## How it works

Bring your LoRA from anywhere: W&B Models, W&B Serverless RL, your own training environment, or a third party. If you trained the LoRA on W&B Models or Serverless RL, you’re all set; your LoRA adapter is already a W&B Models artifact. If you trained elsewhere, simply upload it as an artifact using the wandb SDK. Then call our API with the base model and artifact reference. 

W&B Inference hot-loads the adapter at request time onto a preloaded base model running on CoreWeave GPUs, executes your prompt, and returns the output. That means no clusters to provision, no images to build, and no endpoints to manage. 

Because adapters are attached dynamically, you can roll out improvements by bumping the artifact version, enabling a clean handoff from training to production and a frictionless path to serving multiple fine-tuned variants without standing up separate deployments.

LoRAs are the definitive solution for managing and deploying customized model variants, enabling lightning-fast deployment cycles. We use W&B artifacts as a version-controlled storage layer for your fine-tuned LoRAs, which we make available for dynamic serving at inference time on powerful, hosted base models.

## Step-by-step breakdown

| **Step** | **Impact** |
| --- | --- |
| Train or create your LoRA weights | For fine-tuning, a GPU environment (like T4) is needed. |
| Upload the LoRA weights to W&B | The weights are logged as a W&B artifact of type ‘lora’, enabling lineage and versioning. |
| Set the artifact's storage region to coreweave-us | This step is critical; it places the versioned LoRA files in the high-performance storage required by our integrated inference cluster. |
| Send an inference request | The service pulls and applies the specified LoRA version to the base model dynamically, on demand. |

## Why you should use W&B Inference for custom models

- **Version control for serving:** The artifact URI explicitly includes the project, run, and version (:v10), providing traceability back to the training and hyperparameters used to create the LoRA weights.
- **Serverless inference (zero infra to manage):** You avoid the complexity of setting up and scaling serving infrastructure for every LoRA iteration. The system handles dynamic loading and hot-swapping of your weights in the background.
- **Accelerated iteration:** Because only the small LoRA weights are updated and managed via artifacts, you can cycle from training to production validation instantly.

## Supported Base Models

Inference is currently configured for the following LLMs (exact strings must be used in `wandb.base_model`). More models coming soon:

- `OpenPipe/Qwen3-14B-Instruct`
- `Qwen/Qwen2.5-14B-Instruct`
- `meta-llama/Llama-3.1-8B-Instruct`

## Get started

To get started, check out this [getting started notebook](https://wandb.me/lora_nb).

