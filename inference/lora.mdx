---
title: "Use Serverless LoRAs with Inference"
linkTitle: "Use Serverless LoRA Inference"
description: >
  Bring your own custom LoRA for serving fine-tuned models on W&B Inference.
---

LoRA (Low-Rank Adaptation) weights are lightweight model updates that allow you to efficiently fine-tune large language models. Instead of updating all of a model's parameters, LoRA methods train small sets of additional weights, significantly reducing compute and storage requirements. These adapter weights can then be merged with or dynamically applied to a base model at inference time.

This allows you to do things like multi-tenant customer personalization where you can serve personalized LLM responses for customers without having to deploy multiple bespoke versions of the same model.

W&B Inference lets you serve your custom fine-tuned LoRA weights on fully managed CoreWeave GPU clusters, with no infrastructure to provision or manage. 

## Why you should use W&B Inference for custom models

- **Version control for serving:** The artifact URI explicitly includes the project, run, and version (:v10), providing traceability back to the training and hyperparameters used to create the LoRA weights.
- **Serverless inference (zero infra to manage):** You avoid the complexity of setting up and scaling serving infrastructure for every LoRA iteration. The system handles dynamic loading and hot-swapping of your weights in the background.
- **Accelerated iteration:** Because only the small LoRA weights are updated and managed via artifacts, you can cycle from training to production validation instantly.

## Workflow overview

1. Train or upload your adapter weights as a W&B artifact
2. Use the artifact URI as the model name in the OpenAI-compatible API
3. We hot-load your adapter onto a preloaded base model and return the response

The following example demonstrates how to call your custom LoRA model using W&B Inference:

```python
from openai import OpenAI

model_name = f"wandb-artifact:///{WB_TEAM}/{WB_PROJECT}/qwen_lora:latest"

client = OpenAI(
    base_url="https://api.inference.wandb.ai/v1",
    api_key=API_KEY,
    project=f"{WB_TEAM}/{WB_PROJECT}",
)

resp = client.chat.completions.create(
    model=model_name,
    messages=[{"role": "user", "content": "Say 'Hello World!'"}],
)
print(resp.choices[0].message.content)
```

Check out this [getting started notebook](https://wandb.me/lora_nb) for an interactive demonstration of how to create a LoRA and upload it to W&B as an artifact.

## Prerequisites

To get started with using LoRA's in your W&B Inference requests, you need:

* A [W&B API key](/models/integrations/add-wandb-to-any-library#create-an-api-key)
* A [W&B project](/models/track/project-page)
* **Python 3.8+** with packages:
  `pip install wandb openai`


## How to add LoRAs and use them

You can add LoRAs to your W&B account and start using them using two methods:

<Tabs>
  <Tab title="Upload Your Own Adapter">
    Upload your own LoRA adapter as a W&B artifact. This option is perfect if you've trained your LoRA elsewhere (local environment, cloud provider, or third-party service).

    This Python code uploads your locally-stored LoRA adapter weights to W&B as a versioned artifact. It creates a `lora` type artifact with the required metadata (base model and storage region), adds your adapter files from a local directory, and logs it to your W&B project for use with inference.

    ```python
    import wandb

    run = wandb.init(entity=WB_TEAM, project=WB_PROJECT)

    artifact = wandb.Artifact(
        "qwen_lora",
        type="lora",
        metadata={"wandb.base_model": "Qwen/Qwen2.5-14B-Instruct"},
        storage_region="coreweave-us",
    )

    artifact.add_dir("<path-to-lora-weights>")
    run.log_artifact(artifact)
    ```

    ### Key Requirements

    To use your own LoRAs with Inference:

    * The LoRA must have been trained using one of the models listed in the [Supported Base Models section](#supported-base-models).
    * A LoRA saved as a PEFT-style adapter and `lora` type artifact in your W&B account.
    * The LoRA must be stored in the `storage_region="coreweave-us"` for low latency.
    * Specify which model the LoRA was trained on in its metadata at `wandb.base_model`. The string must exactly match one of those in the [Supported Base Models section](#supported-base-models).
  </Tab>
  <Tab title="Train with W&B Training">
    Train a new LoRA with [W&B Training (serverless RL)](/training). Your adapter automatically becomes a W&B artifact that you can use directly.

    For detailed information on how to train your own adapter, see [OpenPipe's ART quickstart](https://art.openpipe.ai/getting-started/quick-start).

    Once training is complete, your LoRA is automatically available as an artifact.
  </Tab>
</Tabs>

Once your LoRA has been added to your project as an artifact, use the artifact's URI in your inference calls, like this:

```python
# After training completes, use your artifact directly
model_name = f"wandb-artifact:///{WB_TEAM}/{WB_PROJECT}/your_trained_lora:latest"
```

## Supported Base Models

Inference is currently configured for the following LLMs (exact strings must be used in `wandb.base_model`). More models coming soon:

- `OpenPipe/Qwen3-14B-Instruct`
- `Qwen/Qwen2.5-14B-Instruct`
- `meta-llama/Llama-3.1-8B-Instruct`

## Pricing information

Using LoRAs with W&B Inference charges standard rates for the following things:

* [Training the LoRA](https://wandb.ai/site/pricing/reinforcement-learning/) (if you didn't bring your own)
* [Storage](https://wandb.ai/site/pricing/) of the LoRA as an artifact
* [Requests](/inference/usage-limits#account-tiers-and-default-usage-caps) to the Inference service