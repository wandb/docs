---
title: "Set up monitors and guardrails"
description: "Ensure LLM safety and measure output quality in production applications"
---

<Frame>
![Feedback](/images/guardrails_scorers.png)
</Frame>

Weave supports two types of evaluation:

| Type | When it runs | Use case |
|------|--------------|----------|
| **Offline evaluation** | Pre-production | Test against a consistent dataset before deployment. See [Evaluations](/weave/guides/core-types/evaluations). |
| **Online evaluation** | Production | Score live inputs and outputs as they flow through your app. |

This guide covers **online evaluation**, which includes:

- **Monitors**: Passively score production traffic to surface trends and issues. No code changes required.
- **Guardrails**: Actively intervene when scores exceed thresholds (for example, block toxic content). Requires code changes.

Use the following comparison table to better understand when you should use a guardrail or a monitor:

| Aspect | Monitors | Guardrails |
|--------|----------|------------|
| **Purpose** | Passive observation for analysis | Active intervention to prevent issues |
| **Code changes** | None required | Required |
| **Timing** | Asynchronous, in background | Real-time, before output reaches users |
| **Sampling** | Configurable (for example, 10% of calls) | Usually every request |
| **Control flow** | No impact on application | Can block or modify outputs |

When deciding which to use, it's important to know that:
* You can set up monitors in the UI without changing your code.
* Guardrails require code changes and are only needed when you must take action based on scores.
* Every scorer result is automatically stored in Weave’s database. This means your guardrails double as monitors without any extra configuration. You can always analyze historical scorer results, regardless of how they were originally used.

## How to create a monitor in Weave

To create a monitor in Weave:

1. Open the [W&B UI](https://wandb.ai/home) and then open your Weave project.
2. From the Weave side-nav, select **Monitors** and then select the **+ New Monitor** button. This opens the Create new monitor menu.
3. In the Create new monitor menu, configure the following fields:
   - **Name**: Must start with a letter or number. Can contain letters, numbers, hyphens, and underscores.
   - **Description** (optional): Explain what the monitor does.
   - **Active monitor** toggle: Turn the monitor on or off.
   - **Calls to monitor**:
     - **Operations**: Choose one or more `@weave.op`s to monitor. You must log at least one trace that uses the op before it appears in the list of available ops.
     - **Filter** (optional): Narrow down which calls are eligible (for example, by `max_tokens` or `top_p`).
     - **Sampling rate**: The percentage of calls to score (0% to 100%).
       <Tip>
       A lower sampling rate reduces costs, since each scoring call has an associated cost.
       </Tip>
   - **LLM-as-a-judge configuration**:
     - **Scorer name**: Must start with a letter or number. Can contain letters, numbers, hyphens, and underscores.
     - **Judge model**: Select the model that scores your ops. Options include:
       - [Saved models](../tools/playground#saved-models)
       - Models from providers configured by your W&B admin
       - [W&B Inference models](../integrations/inference)
     - **Configuration name**: A name for this model configuration.
     - **System prompt**: Defines the judging model's role and persona, for example, "You are an impartial AI judge."
     - **Response format**: The format the judge should output its response in, such as a `json_object` or plain `text`.
     - **Scoring prompt**: The evaluation task used to score your ops. You can reference variables from your ops in your scoring prompts. For example, "Evaluate whether `{output}` is accurate based on `{ground_truth}`." See [Prompt variables](/weave/guides/evaluation/scorers#access-variables-from-your-ops-in-scoring-prompts).

Once you have configured the monitor's fields, click **Create monitor**. This adds the monitor to your Weave project. When your code starts generating traces, you can review the scores in the **Traces** tab by selecting the monitor's name and reviewing the data in the resulting panel.

Weave automatically stores all scorer results in the [Call](/weave/concepts/fundamental-components#calls) object's `feedback` field.

### Example: Create a truthfulness monitor

The following example creates a monitor that evaluates the truthfulness of generated statements.

1. Define a function that generates statements. Some are truthful, others are not:

```python
import weave
import random
import openai

weave.init("my-team/my-weave-project")

client = openai.OpenAI()

@weave.op()
def generate_statement(ground_truth: str) -> str:
    if random.random() < 0.5:
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {
                    "role": "user",
                    "content": f"Generate a statement that is incorrect based on this fact: {ground_truth}"
                }
            ]
        )
        return response.choices[0].message.content
    else:
        return ground_truth
```

2. Run the function at least once to log a trace in your project. This allows you to set up a monitor in the W&B UI:

```python
generate_statement("The Earth revolves around the Sun.")
```

3. Open your Weave project in the W&B UI and select **Monitors** from the side-nav. Then select **New Monitor**.
4. In the Create new monitor menu, configure the fields using the following values:
   - **Name**: `truthfulness-monitor`
   - **Description**: `Evaluates the truthfulness of generated statements.`
   - **Active monitor**: Toggle **on**.

   ![Creating a monitor part 1](/weave/guides/evaluation/img/monitors-ui-1.png)

   - **Operations**: Select `generate_statement`.
   - **Sampling rate**: Set to `100%` to score every call.

   ![Creating a monitor part 2](/weave/guides/evaluation/img/monitors-ui-2.png)

   - **Scorer name**: `truthfulness-scorer`
   - **Judge model**: `o3-mini-2025-01-31`
   - **System prompt**: `You are an impartial AI judge. Your task is to evaluate the truthfulness of statements.`
   - **Response format**: `json_object`
   - **Scoring prompt**:
     ```text
     Evaluate whether the output statement is accurate based on the input statement.

     This is the input statement: {ground_truth}

     This is the output statement: {output}

     The response should be a JSON object with the following fields:
     - is_true: a boolean stating whether the output statement is true or false based on the input statement.
     - reasoning: your reasoning as to why the statement is true or false.
     ```

   ![Creating a monitor part 3](/weave/guides/evaluation/img/monitors-ui-3.png)

5. Click **Create Monitor**. This adds the monitor to your Weave project.


6. Back in your Python script, invoke your function using statements of varying degrees of truthfulness to test the scoring function:

```python
generate_statement("The Earth revolves around the Sun.")
generate_statement("Water freezes at 0 degrees Celsius.")
generate_statement("The Great Wall of China was built over several centuries.")
```

7. After running the script using several different statements, open the W&B UI and navigate to the **Traces** tab. Select any **LLMAsAJudgeScorer.score** trace to see the results.

![Monitor trace](/weave/guides/evaluation/img/monitors-4.png)


## Set up guardrails using Weave

Guardrails actively intervene in your app’s behavior based on scores. Unlike monitors, guardrails require code changes because they need to affect your application’s control flow.

Weave guardrails use inline Weave Scorers to assess the input from a user or the output from an LLM and adjust the LLM's responses in real time. You can configure custom scorers or use built-in scorers to assess content for a variety of purposes. The following examples show you how to use a built-in scorer and a custom scorer to adjust output from an LLM.

### Example: Create a guardrail using a built-in moderation scorer

The following example sends user prompts to OpenAI's GPT-4o mini model. The model's response is then passed to Weave's built-in access to [OpenAI's moderation API](https://platform.openai.com/docs/guides/moderation) to assess whether the LLM's response contains harmful or toxic content.

```python
import weave
import openai
from weave.scorers import OpenAIModerationScorer
import asyncio

# Initialize Weave
weave.init("your-team-name/your-project-name")

# Initialize OpenAI client
client = openai.OpenAI()  # Uses OPENAI_API_KEY env var

# Initialize the moderation scorer
moderation_scorer = OpenAIModerationScorer()

# Send prompts to OpenAI
@weave.op
def generate_response(prompt: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=200
    )
    return response.choices[0].message.content

# Guardrail function checks responses for toxicity
async def generate_safe_response(prompt: str) -> str:
    """Generate a response with content moderation guardrail."""
    # Get both the result and Call object
    result, call = generate_response.call(prompt)
    
    # Apply the moderation scorer before returning to user
    score = await call.apply_scorer(moderation_scorer)
    print("This is the score object:", score)
    
    # Check if content was flagged
    if not score.result.get("passed", True): 
        categories = score.result.get("categories", {})
        flagged_categories = list(categories.keys()) if categories else []
        print(f"Content blocked. Flagged categories: {flagged_categories}")
        return "I'm sorry, I can't provide that response due to content policy restrictions."
    
    return result

# Run the examples
if __name__ == "__main__":
    
    prompts = [
        "What's the capital of France?",
        "Tell me a funny fact about dogs.",
    ]
    
    for prompt in prompts:
        print(f"\nPrompt: {prompt}")
        response = asyncio.run(generate_safe_response(prompt))
        print(f"Response: {response}")
```

You can reference variables from your ops in your scoring prompts. For example, "Evaluate whether `{output}` is accurate based on `{ground_truth}`." See [prompt variables](/weave/guides/evaluation/scorers#access-variables-from-your-ops-in-scoring-prompts) for more information.

### Example: Create a guardrail using a custom scorer

The following example creates a custom guardrail that detects personally identifiable information (PII) in LLM responses, such as email addresses, phone numbers, or social security numbers. This prevents sensitive information from being exposed in generated content.

```python
import weave
import openai
import re
import asyncio
from weave import Scorer

weave.init("your-team-name/your-project-name")

client = openai.OpenAI()

class PIIDetectionScorer(Scorer):
    """Detects PII in LLM outputs to prevent data leaks."""
    
    @weave.op
    def score(self, output: str) -> dict:
        """
        Check for common PII patterns in the output.
        
        Returns:
            dict: Contains 'passed' (bool) and 'detected_types' (list)
        """
        detected_types = []
        
        # Email pattern
        if re.search(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', output):
            detected_types.append("email")
        
        # Phone number pattern (US format)
        if re.search(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', output):
            detected_types.append("phone")
        
        # SSN pattern
        if re.search(r'\b\d{3}-\d{2}-\d{4}\b', output):
            detected_types.append("ssn")
        
        return {
            "passed": len(detected_types) == 0,
            "detected_types": detected_types
        }

# Initialize scorer outside the function for optimal performance
pii_scorer = PIIDetectionScorer()

@weave.op
def generate_response(prompt: str) -> str:
    """Generate a response using an LLM."""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=200
    )
    return response.choices[0].message.content

async def generate_safe_response(prompt: str) -> str:
    """Generate a response with PII detection guardrail."""
    result, call = generate_response.call(prompt)
    
    # Apply PII detection scorer
    score = await call.apply_scorer(pii_scorer)
    
    # Block response if PII detected
    if not score.result.get("passed", True):
        detected_types = score.result.get("detected_types", [])
        return f"I cannot provide a response that may contain sensitive information (detected: {', '.join(detected_types)})."
    
    return result

# Example usage
if __name__ == "__main__":
    prompts = [
        "What's the weather like today?",
        "Can you help me contact someone at john.doe@example.com?",
        "Tell me about machine learning.",
    ]
    
    for prompt in prompts:
        print(f"\nPrompt: {prompt}")
        response = asyncio.run(generate_safe_response(prompt))
        print(f"Response: {response}")
```

This guardrail blocks any response that contains email addresses, phone numbers, or SSNs, preventing accidental exposure of sensitive information.

### Integrate Weave with AWS Bedrock's guardrail

The `BedrockGuardrailScorer` uses AWS Bedrock's guardrail feature to detect and filter content based on configured policies.

**Prerequisites:**
- An AWS account with Bedrock access
- A configured guardrail in the AWS Bedrock console
- The `boto3` Python package

<Tip>
You don't need to create your own Bedrock client. Weave creates it for you. To specify a region, pass the `bedrock_runtime_kwargs` parameter to the scorer.
</Tip>

For details on creating a guardrail in AWS, see the [Bedrock guardrails notebook](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/responsible_ai/bedrock-guardrails/guardrails-api.ipynb).

```python
import weave
from weave.scorers.bedrock_guardrails import BedrockGuardrailScorer

weave.init("my_app")

guardrail_scorer = BedrockGuardrailScorer(
    guardrail_id="your-guardrail-id",
    guardrail_version="DRAFT",
    source="INPUT",
    bedrock_runtime_kwargs={"region_name": "us-east-1"}
)

@weave.op
def generate_text(prompt: str) -> str:
    # Your text generation logic here
    return "Generated text..."

async def generate_safe_text(prompt: str) -> str:
    result, call = generate_text.call(prompt)

    score = await call.apply_scorer(guardrail_scorer)

    if not score.result.passed:
        if score.result.metadata.get("modified_output"):
            return score.result.metadata["modified_output"]
        return "I cannot generate that content due to content policy restrictions."

    return result
```

## Production best practices

Follow these best practices when implementing guardrails and monitors in production:

### Initialize scorers efficiently

Initialize scorers outside your main function for optimal performance. This is especially important when:
- Your scorers load ML models
- You're using local LLMs where latency is critical
- Your scorers maintain network connections
- You have high-traffic applications

The following example initializes the scorer at the module level and reuses it:

```python
import weave
from weave.scorers import OpenAIModerationScorer

# Initialize scorer at module level (created once, reused many times)
moderation_scorer = OpenAIModerationScorer()

@weave.op
def generate_text(prompt: str) -> str:
    # Your LLM generation logic here
    return "Generated response..."

async def generate_response(prompt: str) -> str:
    result, call = generate_text.call(prompt)
    # Reuse the pre-initialized scorer (more efficient than creating a new one each time)
    score = await call.apply_scorer(moderation_scorer)
    return result
```

### Monitor multiple aspects

Apply multiple scorers to get comprehensive insights into your LLM's performance:

```python
async def evaluate_comprehensively(call):
    await call.apply_scorer(ToxicityScorer())
    await call.apply_scorer(QualityScorer())
    await call.apply_scorer(LatencyScorer())
```

### Set appropriate sampling rates

Use sampling to reduce costs while maintaining coverage. For monitoring scenarios, sample 10-20% of calls. For guardrails that must block unsafe content, score every call.

The following example monitors only 10% of calls:

```python
import random
import weave
from weave.scorers import OpenAIModerationScorer

# Initialize scorer at module level
moderation_scorer = OpenAIModerationScorer()

@weave.op
def generate_response(prompt: str) -> str:
    # Your LLM generation logic here
    return "Generated response..."

async def generate_with_monitoring(prompt: str) -> str:
    """Generate a response with sampled monitoring."""
    result, call = generate_response.call(prompt)
    
    # Only monitor 10% of calls to reduce costs
    if random.random() < 0.1:
        await call.apply_scorer(moderation_scorer)
    
    return result
```

This allows you to monitor production traffic at scale while controlling costs. You can adjust the sampling rate based on your traffic volume and monitoring needs.