---
title: "Send OpenTelemetry Traces"
description: "Ingest OpenTelemetry compatible trace data through a dedicated endpoint"
---

## Overview
Weave supports ingestion of OpenTelemetry compatible trace data through a dedicated endpoint. This endpoint allows you to send OTLP (OpenTelemetry Protocol) formatted trace data directly to your Weave project.

## Endpoint details

**Path**: `/otel/v1/traces`
**Method**: POST
**Content-Type**: `application/x-protobuf`
**Base URL**: The base URL for the OTEL trace endpoint depends on your W&B deployment type:

- Multi-tenant Cloud:  
  `https://trace.wandb.ai/otel/v1/traces`

- Dedicated Cloud and Self-Managed instances:  
  `https://<your-subdomain>.wandb.io/traces/otel/v1/traces`

Replace `<your-subdomain>` with your organization's unique W&B domain, e.g., `acme.wandb.io`.

## Authentication
Standard W&B authentication is used. You must have write permissions to the project where you're sending trace data.

## Required Headers
- `project_id: <your_entity>/<your_project_name>`
- `Authorization=Basic <Base64 Encoding of api:$WANDB_API_KEY>`

## Examples:

You must modify the following fields before you can run the code samples below:
1. `WANDB_API_KEY`: You can get this from [https://wandb.ai/authorize](https://wandb.ai/authorize).
2. Entity: You can only log traces to the project under an entity that you have access to. You can find your entity name by visiting your W&N dashboard at [https://wandb.ai/home], and checking the **Teams** field in the left sidebar.
3. Project Name: Choose a fun name!
4. `OPENAI_API_KEY`: You can obtain this from the [OpenAI dashboard](https://platform.openai.com/api-keys).

### OpenInference Instrumentation:

This example shows how to use the OpenAI instrumentation. There are many more available which you can find in the official repository: https://github.com/Arize-ai/openinference

First, install the required dependencies:

```bash
pip install openai openinference-instrumentation-openai opentelemetry-exporter-otlp-proto-http
```

Next, paste the following code into a python file such as `openinference_example.py`

```python lines
import base64
import openai
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk import trace as trace_sdk
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor
from openinference.instrumentation.openai import OpenAIInstrumentor

OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
WANDB_BASE_URL = "https://trace.wandb.ai"
PROJECT_ID = "<your-entity>/<your-project>"

OTEL_EXPORTER_OTLP_ENDPOINT = f"{WANDB_BASE_URL}/otel/v1/traces"

# Can be found at https://wandb.ai/authorize
WANDB_API_KEY = "<your-wandb-api-key>"
AUTH = base64.b64encode(f"api:{WANDB_API_KEY}".encode()).decode()

OTEL_EXPORTER_OTLP_HEADERS = {
    "Authorization": f"Basic {AUTH}",
    "project_id": PROJECT_ID,
}

tracer_provider = trace_sdk.TracerProvider()

# Configure the OTLP exporter
exporter = OTLPSpanExporter(
    endpoint=OTEL_EXPORTER_OTLP_ENDPOINT,
    headers=OTEL_EXPORTER_OTLP_HEADERS,
)

# Add the exporter to the tracer provider
tracer_provider.add_span_processor(SimpleSpanProcessor(exporter))

# Optionally, print the spans to the console.
tracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))

OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)

def main():
    client = openai.OpenAI(api_key=OPENAI_API_KEY)
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "Describe OTEL in a single sentence."}],
        max_tokens=20,
        stream=True,
        stream_options={"include_usage": True},
    )
    for chunk in response:
        if chunk.choices and (content := chunk.choices[0].delta.content):
            print(content, end="")

if __name__ == "__main__":
    main()
```

Finally, once you have set the fields specified above to their correct values, run the code:

```bash
python openinference_example.py
```

### OpenLLMetry Instrumentation:

The following example shows how to use the OpenAI instrumentation. Additional examples are available at [https://github.com/traceloop/openllmetry/tree/main/packages](https://github.com/traceloop/openllmetry/tree/main/packages).

First install the required dependencies:

```bash
pip install openai opentelemetry-instrumentation-openai opentelemetry-exporter-otlp-proto-http
```

Next, paste the following code into a python file such as `openllmetry_example.py`. Note that this is the same code as above, except the `OpenAIInstrumentor` is imported from `opentelemetry.instrumentation.openai` instead of `openinference.instrumentation.openai`

```python lines
import base64
import openai
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk import trace as trace_sdk
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor
from opentelemetry.instrumentation.openai import OpenAIInstrumentor

OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
WANDB_BASE_URL = "https://trace.wandb.ai"
PROJECT_ID = "<your-entity>/<your-project>"

OTEL_EXPORTER_OTLP_ENDPOINT = f"{WANDB_BASE_URL}/otel/v1/traces"

# Can be found at https://wandb.ai/authorize
WANDB_API_KEY = "<your-wandb-api-key>"
AUTH = base64.b64encode(f"api:{WANDB_API_KEY}".encode()).decode()

OTEL_EXPORTER_OTLP_HEADERS = {
    "Authorization": f"Basic {AUTH}",
    "project_id": PROJECT_ID,
}

tracer_provider = trace_sdk.TracerProvider()

# Configure the OTLP exporter
exporter = OTLPSpanExporter(
    endpoint=OTEL_EXPORTER_OTLP_ENDPOINT,
    headers=OTEL_EXPORTER_OTLP_HEADERS,
)

# Add the exporter to the tracer provider
tracer_provider.add_span_processor(SimpleSpanProcessor(exporter))

# Optionally, print the spans to the console.
tracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))

OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)

def main():
    client = openai.OpenAI(api_key=OPENAI_API_KEY)
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "Describe OTEL in a single sentence."}],
        max_tokens=20,
        stream=True,
        stream_options={"include_usage": True},
    )
    for chunk in response:
        if chunk.choices and (content := chunk.choices[0].delta.content):
            print(content, end="")

if __name__ == "__main__":
    main()
```

Finally, once you have set the fields specified above to their correct values, run the code:

```bash
python openllmetry_example.py
```

### Without Instrumentation

If you would prefer to use OTEL directly instead of an instrumentation package, you may do so. Span attributes will be parsed according to the OpenTelemetry semantic conventions described at [https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/).

First, install the required dependencies:

```bash
pip install openai opentelemetry-sdk opentelemetry-api opentelemetry-exporter-otlp-proto-http
```

Next, paste the following code into a python file such as `opentelemetry_example.py`

```python lines
import json
import base64
import openai
from opentelemetry import trace
from opentelemetry.sdk import trace as trace_sdk
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor

OPENAI_API_KEY = "YOUR_OPENAI_API_KEY"
WANDB_BASE_URL = "https://trace.wandb.ai"
PROJECT_ID = "<your-entity>/<your-project>"

OTEL_EXPORTER_OTLP_ENDPOINT = f"{WANDB_BASE_URL}/otel/v1/traces"

# Can be found at https://wandb.ai/authorize
WANDB_API_KEY = "<your-wandb-api-key>"
AUTH = base64.b64encode(f"api:{WANDB_API_KEY}".encode()).decode()

OTEL_EXPORTER_OTLP_HEADERS = {
    "Authorization": f"Basic {AUTH}",
    "project_id": PROJECT_ID,
}

tracer_provider = trace_sdk.TracerProvider()

# Configure the OTLP exporter
exporter = OTLPSpanExporter(
    endpoint=OTEL_EXPORTER_OTLP_ENDPOINT,
    headers=OTEL_EXPORTER_OTLP_HEADERS,
)

# Add the exporter to the tracer provider
tracer_provider.add_span_processor(SimpleSpanProcessor(exporter))

# Optionally, print the spans to the console.
tracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))

trace.set_tracer_provider(tracer_provider)
# Creates a tracer from the global tracer provider
tracer = trace.get_tracer(__name__)
tracer.start_span('name=standard-span')

def my_function():
    with tracer.start_as_current_span("outer_span") as outer_span:
        client = openai.OpenAI()
        input_messages=[{"role": "user", "content": "Describe OTEL in a single sentence."}]
        # This will only appear in the side panel
        outer_span.set_attribute("input.value", json.dumps(input_messages))
        # This follows conventions and will appear in the dashboard
        outer_span.set_attribute("gen_ai.system", 'openai')
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=input_messages,
            max_tokens=20,
            stream=True,
            stream_options={"include_usage": True},
        )
        out = ""
        for chunk in response:
            if chunk.choices and (content := chunk.choices[0].delta.content):
                out += content
        # This will only appear in the side panel
        outer_span.set_attribute("output.value", json.dumps({"content": out}))

if __name__ == "__main__":
    my_function()
```

Finally, once you have set the fields specified above to their correct values, run the code:

```bash
python opentelemetry_example.py
```

The span attribute prefixes `gen_ai` and `openinference` are used to determine which convention to use, if any, when interpreting the trace. If neither key is detected, then all span attributes are visible in the trace view. The full span is available in the side panel when you select a trace.


## Attribute Mappings

Weave automatically maps OpenTelemetry span attributes from various instrumentation frameworks to its internal data model. This enables seamless integration with popular observability tools like OpenInference, Vercel AI SDK, MLflow, Vertex AI, and Traceloop without requiring you to modify your existing schemas. When multiple attribute names map to the same field, Weave applies them in priority order, allowing frameworks to coexist in the same traces.

| Attribute Field Name              | W&B Mapping                   | Description                                 | Type                        | Example                                        
|:----------------------------------|:------------------------------|:--------------------------------------------|:----------------------------|:-----------------------------------------------
| `ai.prompt`                       | `inputs`                      | User prompt (Vercel).                       | String, List, Dict          | `"Write a short haiku about summer."`          
| `gen_ai.prompt`                   | `inputs`                      | OpenTelemetry AI prompt/messages.           | List, Dict, String          | `[{"role":"user","content":"abc"}]`            
| `input.value`                     | `inputs`                      | OpenInference input value.                  | String, List, Dict          | `{"text":"Tell a joke"}`                       
| `mlflow.spanInputs`               | `inputs`                      | MLflow span inputs.                         | String, List, Dict          | `["prompt text"]`                              
| `traceloop.entity.input`          | `inputs`                      | Traceloop input.                            | String, List, Dict          | "Translate this to French"`                    
| `gcp.vertex.agent.tool_call_args` | `inputs`                      | Vertex tool call args.                      | Dict                        | `{"args":{"query":"weather in SF"}}`           
| `gcp.vertex.agent.llm_request`    | `inputs`                      | Vertex LLM request.                         | Dict                        | `{"contents":[{"role":"user","parts":[...]}]}` 
| `input`                           | `inputs`                      | Generic fallback input.                     | String, List, Dict          | `"Summarize this text"`                        
| `inputs`                          | `inputs`                      | Generic plural input.                       | List, Dict, String          | `["Summarize this text"]`                      
| `ai.response`                     | `outputs`                     | Model response (Vercel).                    | String, List, Dict          | `"Here is a haiku..."`                         
| `gen_ai.completion`               | `outputs`                     | OpenTelemetry AI completion.                | String, List, Dict          | `"Completion text"`                            
| `output.value`                    | `outputs`                     | OpenInference output value.                 | String, List, Dict          | `{"text":"Answer text"}`                       
| `mlflow.spanOutputs`              | `outputs`                     | MLflow span outputs.                        | String, List, Dict          | `["answer"]`                                   
| `gen_ai.content.completion`       | `outputs`                     | OpenLit content completion.                 | String                      | `"Answer text"`                                
| `traceloop.entity.output`         | `outputs`                     | Traceloop output.                           | String, List, Dict          | `"Answer text"`                                
| `gcp.vertex.agent.tool_response`  | `outputs`                     | Vertex tool response.                       | Dict, String                | `{"toolResponse":"ok"}`                        
| `gcp.vertex.agent.llm_response`   | `outputs`                     | Vertex LLM response.                        | Dict, String                | `{"candidates":[...]}`                         
| `output`                          | `outputs`                     | Generic fallback output.                    | String, List, Dict          | `"Answer text"`                                
| `outputs`                         | `outputs`                     | Generic plural output.                      | List, Dict, String          | `["Answer text"]`                              
| `gen_ai.usage.input_tokens`       | `usage.input_tokens`          | Count of input tokens.                      | Int                         | `42`                                           
| `gen_ai.usage.prompt_tokens`      | `usage.prompt_tokens`         | Count of prompt tokens.                     | Int                         | `30`                                           
| `llm.token_count.prompt`          | `usage.prompt_tokens`         | Prompt token count (alt key).               | Int                         | `30`                                           
| `ai.usage.promptTokens`           | `usage.prompt_tokens`         | Prompt tokens (Vercel).                     | Int                         | `30`                                           
| `gen_ai.usage.completion_tokens`  | `usage.completion_tokens`     | Count of completion tokens.                 | Int                         | `40`                                           
| `llm.token_count.completion`      | `usage.completion_tokens`     | Completion token count (alt key).           | Int                         | `40`                                           
| `ai.usage.completionTokens`       | `usage.completion_tokens`     | Completion tokens (Vercel).                 | Int                         | `40`                                           
| `llm.usage.total_tokens`          | `usage.total_tokens`          | Total tokens used.                          | Int                         | `70`                                           
| `llm.token_count.total`           | `usage.total_tokens`          | Total token count (alt key).                | Int                         | `70`                                           
| `gen_ai.system`                   | `attributes.system`           | System prompt/instructions.                 | String                      | `"You are a helpful assistant."`               
| `llm.system`                      | `attributes.system`           | System prompt/instructions (OpenInference). | String                      | `"You are a helpful assistant."`               
| `weave.span.kind`                 | `attributes.kind`             | Span kind/type (Weave).                     | String                      | `"llm"`                                        
| `traceloop.span.kind`             | `attributes.kind`             | Span kind/type (Traceloop).                 | String                      | `"llm"`                                        
| `openinference.span.kind`         | `attributes.kind`             | Span kind/type (OpenInference).             | String                      | `"llm"`                                        
| `gen_ai.response.model`           | `attributes.model`            | Model name/id (OpenTelemetry AI).           | String                      | `"gpt-4o"`                                     
| `llm.model_name`                  | `attributes.model`            | Model name (OpenInference).                 | String                      | `"gpt-4o-mini"`                                
| `ai.model.id`                     | `attributes.model`            | Model ID (Vercel).                          | String                      | `"gpt-4o"`                                     
| `llm.provider`                    | `attributes.provider`         | Model provider/vendor.                      | String                      | `"openai"`                                     
| `ai.model.provider`               | `attributes.provider`         | Provider/vendor (Vercel).                   | String                      | `"openai"`                                     
| `gen_ai.request`                  | `attributes.model_parameters` | Model generation parameters.                | Dict                        | `{"temperature":0.7,"max_tokens":256}`         
| `llm.invocation_parameters`       | `attributes.model_parameters` | Invocation parameters (OpenInference).      | Dict                        | `{"temperature":0.2}`                          
| `wandb.display_name`              | `display_name`                | Custom UI title.                            | String                      | `"User Message"`                               
| `gcp.vertex.agent.session_id`     | `thread_id`                   | Vertex session ID as thread ID.             | String                      | `"thread_123"`                                 
| `wandb.thread_id`                 | `thread_id`                   | Explicit thread identifier.                 | String                      | `"thread_123"`                                 
| `wb_run_id`                       | `wb_run_id`                   | Associated W&B run ID.                      | String                      | `"abc123"`                                     
| `wandb.wb_run_id`                 | `wb_run_id`                   | Associated W&B run ID (alt key).            | String                      | `"abc123"`                                     
| `gcp.vertex.agent.session_id`     | `is_turn`                     | Truthy marks span as a turn.                | Boolean                     | `true`                                         
| `wandb.is_turn`                   | `is_turn`                     | Marks span as a thread turn.                | Boolean                     | `true`                                         
| `langfuse.startTime`              | `start_time` (override)       | Override span start time.                   | Timestamp (ISO8601/unix ns) | `"2024-01-01T12:00:00Z"`                       
| `langfuse.endTime`                | `end_time` (override)         | Override span end time.                     | Timestamp (ISO8601/unix ns) | `"2024-01-01T12:00:01Z"`                       

