---
title: Integrations overview
description: "Seamlessly trace and monitor LLM calls across 30+ providers and frameworks with Weave's automatic patching, supporting OpenAI, Anthropic, Google AI, and major orchestration tools without code changes."
---




W&B Weave provides logging integrations for popular LLM providers and orchestration frameworks. These integrations allow you to seamlessly trace calls made through various libraries, enhancing your ability to monitor and analyze your AI applications.

If you use LLM provider libraries (such as OpenAI, Anthropic, Cohere, or Mistral) in your application, you want those API calls to show up in W&B Weave as traced Calls: inputs, outputs, latency, token usage, and cost. Without help, you would have to wrap every `client.chat.completions.create()` (or equivalent) in `@weave.op` or manual instrumentation, which is tedious and easy to miss something.

Weave automatically intercepts (patches) supported LLM client libraries. Your application code stays unchanged: you use the provider SDK as usual, and each request is recorded as a Weave Call. You get full tracing with minimal setup.


## LLM Providers

LLM providers are the vendors that offer access to large language models for generating predictions. Weave integrates with these providers to log and trace the interactions with their APIs:

- **[W&B Inference Service](https://docs.wandb.ai/inference/)**
- **[Amazon Bedrock](/weave/guides/integrations/bedrock)**
- **[Anthropic](/weave/guides/integrations/anthropic)**
- **[Cerebras](/weave/guides/integrations/cerebras)**
- **[Cohere](/weave/guides/integrations/cohere)**
- **[Google](/weave/guides/integrations/google)**
- **[Groq](/weave/guides/integrations/groq)**
- **[Hugging Face Hub](/weave/guides/integrations/huggingface)**
- **[LiteLLM](/weave/guides/integrations/litellm)**
- **[Microsoft Azure](/weave/guides/integrations/azure)**
- **[MistralAI](/weave/guides/integrations/mistral)**
- **[NVIDIA NIM](/weave/guides/integrations/nvidia_nim)**
- **[OpenAI](/weave/guides/integrations/openai)**
- **[OpenRouter](/weave/guides/integrations/openrouter)**
- **[Together AI](/weave/guides/integrations/together_ai)**

**[Local Models](/weave/guides/integrations/local_models)**: For when you're running models on your own infrastructure.

## Frameworks

Frameworks help orchestrate the actual execution pipelines in AI applications. They provide tools and abstractions for building complex workflows. Weave integrates with these frameworks to trace the entire pipeline:

- **[OpenAI Agents SDK](/weave/guides/integrations/openai_agents)**
- **[LangChain](/weave/guides/integrations/langchain)**
- **[LlamaIndex](/weave/guides/integrations/llamaindex)**
- **[DSPy](/weave/guides/integrations/dspy)**
- **[Instructor](/weave/guides/integrations/instructor)**
- **[CrewAI](/weave/guides/integrations/crewai)**
- **[Smolagents](/weave/guides/integrations/smolagents)**
- **[PydanticAI](/weave/guides/integrations/pydantic_ai)**
- **[Google Agent Development Kit (ADK)](/weave/guides/integrations/google_adk)**
- **[AutoGen](/weave/guides/integrations/autogen)**
- **[Verdict](/weave/guides/integrations/verdict)**
- **[TypeScript SDK](/weave/guides/integrations/js)**
- **[Agno](/weave/guides/integrations/agno)**
- **[Koog](/weave/guides/integrations/koog)**

## RL Frameworks
- **[Verifiers](/weave/guides/integrations/verifiers)**

## Protocols

Weave integrates with standardized protocols that enable communication between AI applications and their supporting services:

- **[Model Context Protocol (MCP)](/weave/guides/integrations/mcp)**

Choose an integration from the lists above to learn more about how to use Weave with your preferred LLM provider, framework, or protocol. Whether you're directly accessing LLM APIs, building complex pipelines, or using standardized protocols, Weave provides the tools to trace and analyze your AI applications effectively.
