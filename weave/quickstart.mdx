---
title: "Track LLM apps"
description: "Begin debugging LLM apps by adding tracing."
mode: "wide"
---

W&B Weave makes it easy to track and evaluate your LLM applications. Follow these steps to track your first call. 

- [Open in Google Colab](https://colab.research.google.com/github/wandb/docs/blob/main/weave/cookbooks/source/intro_notebook.ipynb)
- [View source on GitHub](https://github.com/wandb/docs/blob/main/weave/cookbooks/source/intro_notebook.ipynb)


## Install Weave

First install the Weave library:

<Tabs>
  <Tab title="Python">
    ```bash
    pip install weave
    ```
  </Tab>
  <Tab title="TypeScript">
    ```bash
    pnpm install weave
    ```
  </Tab>
</Tabs>

## Get your API key

Create a [Weights & Biases (W&B) account](https://wandb.ai) and [copy your API key](https://wandb.ai/authorize). Your key allows you to authenticate with your W&B account.

Once you have secured your API key, you can either set it as an environment variable or enter it when prompted as your code runs. As a best practice, we recommend setting it as the environment variable `WANDB_API_KEY`. Weave automatically searches for this variable upon execution.

## Log a trace to a new project

To begin tracking your code and logging traces to Weave:

1. Import the `weave` library into your code.
2. Call `weave.init('your_wb_team/project_name')` in your code to send tracking information to your W&B [team](/platform/app/settings-page/teams) and [project](/platform/hosting/iam/org_team_struct#project). If you do not set a team, the traces are sent to your [default team](/platform/app/settings-page/user-settings/#default-team). If the specified project does not exist in your team, Weave creates it.
3. **(Optional)** Add the [`@weave.op()` decorator](/weave/guides/tracking/ops) to specific functions you want to track. If you do not add `@weave.op()`, Weave automatically tracks the inputs, outputs, and code of your top-level functions. The decorator has the following syntax in TypeScript: `weave.op(your_function)`

The following example code sends a request to OpenAI (requires [OpenAI API key](https://platform.openai.com/docs/quickstart/step-2-setup-your-api-key)) and Weave records the requests tracing information. The request asks the OpenAI model to extract dinosaur names from the input and identify each dinosaur's diet (herbivore or carnivore).

Run the following example code to track your first project with Weave:

<Tabs>
  <Tab title="Python">
    ```python lines {1-2,7,9-10,26-27}
    # Imports the Weave library
    import weave
    from openai import OpenAI

    client = OpenAI()

    # Weave automatically tracks the inputs, outputs and code of this function

    # Uncomment @weave.op() to specifically track the `extract_dinos` function
    # @weave.op()
    def extract_dinos(sentence: str) -> dict:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": """In JSON format extract a list of `dinosaurs`, with their `name`,
    their `common_name`, and whether its `diet` is a herbivore or carnivore"""
                },
                {
                    "role": "user",
                    "content": sentence
                }
                ],
                response_format={ "type": "json_object" }
            )
        return response.choices[0].message.content

    # Initializes Weave, and sets the team and project to log data to
    weave.init('quickstart_team/jurassic-park')

    sentence = """I watched as a Tyrannosaurus rex (T. rex) chased after a Triceratops (Trike), \
    both carnivore and herbivore locked in an ancient dance. Meanwhile, a gentle giant \
    Brachiosaurus (Brachi) calmly munched on treetops, blissfully unaware of the chaos below."""

    result = extract_dinos(sentence)
    print(result)
    ```
  </Tab>
  <Tab title="TypeScript">
    ```typescript lines {1-2,7-8,20,24-25}
    // Imports the Weave library
    import * as weave from 'weave';
    import OpenAI from 'openai';
    
    const openai = new OpenAI();

    // Weave automatically tracks the inputs, outputs and code of this function  
    async function extractDinos(input: string) {
      const response = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [
          {
            role: 'user',
            content: `In JSON format extract a list of 'dinosaurs', with their 'name', their 'common_name', and whether its 'diet' is a herbivore or carnivore: ${input}`,
          },
        ],
      });
      return response.choices[0].message.content;
    }
    const extractDinosOp = weave.op(extractDinos);

    async function main() {

      // Initializes Weave, and sets the team and project to log data to
      await weave.init('quickstart_team/jurassic-park');

      const result = await extractDinosOp(
        'I watched as a Tyrannosaurus rex (T. rex) chased after a Triceratops (Trike), both carnivore and herbivore locked in an ancient dance. Meanwhile, a gentle giant Brachiosaurus (Brachi) calmly munched on treetops, blissfully unaware of the chaos below.'
      );
      console.log(result);
    }

    main();

    ```
  </Tab>
</Tabs>

When you call the `extract_dinos` function, Weave outputs links to view your traces in the terminal.

## See traces of your application in your project

Click the link in your terminal or paste it into your browser to open the Weave UI. In the **Traces** panel of the Weave UI, you can click on the trace to see its data, such as its input, output, latency, and token usage.

![Weave Trace Outputs 1](/images/tutorial_trace_1.png)

## Learn more about Traces

- Learn how to [decorate your functions and retrieve call information](/weave/quickstart-tracing).
- Try the [Playground](/weave/guides/tools/playground) to test different models on logged traces.
- [Explore integrations](/weave/guides/integrations/). Weave automatically tracks calls made to OpenAI, Anthropic and many more LLM libraries. If your LLM library isn't currently one of our integrations you can track calls to other LLMs libraries or frameworks easily by wrapping them with `@weave.op()`.

## Next Steps

[Get started evaluating your app](/weave/tutorial-eval) and then see how to [evaluate a RAG application](/weave/tutorial-rag).
