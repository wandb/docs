---
title: "Track LLM apps"
description: "Begin debugging LLM apps by adding tracing."
mode: "wide"
---

import WeaveInstall from "/snippets/en/_includes/weave-install.mdx";

W&B Weave makes it easy to track and evaluate your LLM applications. Follow these steps to track your first call. 

- [Open in Google Colab](https://colab.research.google.com/github/wandb/docs/blob/main/weave/cookbooks/source/intro_notebook.ipynb)
- [View source on GitHub](https://github.com/wandb/docs/blob/main/weave/cookbooks/source/intro_notebook.ipynb)


<WeaveInstall/>

## Log a trace to a new project

To begin tracking your code and logging traces to Weave:

1. Import the `weave` library into your code.
2. Call `weave.init('your_wb_team/project_name')` in your code to send tracking information to your W&B [team](/platform/app/settings-page/teams) and [project](/platform/hosting/iam/org_team_struct#project). If you do not set a team, the traces are sent to your [default team](/platform/app/settings-page/user-settings/#default-team). If the specified project does not exist in your team, Weave creates it.
3. Add the [`@weave.op()` decorator](/weave/guides/tracking/ops) to specific functions you want to track. While Weave automatically tracks calls to supported LLMs, adding the Weave decorator allows you to track the inputs, outputs, and code of specific functions. The decorator uses the following syntax in TypeScript: `weave.op(your_function)`

The following example code sends a request to OpenAI (requires [OpenAI API key](https://platform.openai.com/docs/quickstart/step-2-setup-your-api-key)) and Weave records the request's tracing information. The request asks the OpenAI model to extract dinosaur names from the input and identify each dinosaur's diet (herbivore or carnivore).

Run the following example code to track your first project with Weave:

<Tabs>
  <Tab title="Python">
    ```python lines {1-2,7,9-10,26-27}
    # Imports the Weave library
    import weave
    from openai import OpenAI

    client = OpenAI()

    # Weave automatically tracks the inputs, outputs and code of this function
    @weave.op()
    def extract_dinos(sentence: str) -> dict:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": """In JSON format extract a list of `dinosaurs`, with their `name`,
    their `common_name`, and whether its `diet` is a herbivore or carnivore"""
                },
                {
                    "role": "user",
                    "content": sentence
                }
                ],
                response_format={ "type": "json_object" }
            )
        return response.choices[0].message.content

    # Initializes Weave, and sets the team and project to log data to
    weave.init('quickstart_team/jurassic-park')

    sentence = """I watched as a Tyrannosaurus rex (T. rex) chased after a Triceratops (Trike), \
    both carnivore and herbivore locked in an ancient dance. Meanwhile, a gentle giant \
    Brachiosaurus (Brachi) calmly munched on treetops, blissfully unaware of the chaos below."""

    result = extract_dinos(sentence)
    print(result)
    ```
  </Tab>
  <Tab title="TypeScript">
    ```typescript lines {1-2,7-8,20,24-25}
    // Imports the Weave library
    import * as weave from 'weave';
    import OpenAI from 'openai';
    
    const openai = new OpenAI();

    // Weave automatically tracks the inputs, outputs and code of this function  
    async function extractDinos(input: string) {
      const response = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [
          {
            role: 'user',
            content: `In JSON format extract a list of 'dinosaurs', with their 'name', their 'common_name', and whether its 'diet' is a herbivore or carnivore: ${input}`,
          },
        ],
      });
      return response.choices[0].message.content;
    }
    const extractDinosOp = weave.op(extractDinos);

    async function main() {

      // Initializes Weave, and sets the team and project to log data to
      await weave.init('quickstart_team/jurassic-park');

      const result = await extractDinosOp(
        'I watched as a Tyrannosaurus rex (T. rex) chased after a Triceratops (Trike), both carnivore and herbivore locked in an ancient dance. Meanwhile, a gentle giant Brachiosaurus (Brachi) calmly munched on treetops, blissfully unaware of the chaos below.'
      );
      console.log(result);
    }

    main();

    ```
  </Tab>
</Tabs>

When you call the `extract_dinos` function, Weave outputs links to view your traces in the terminal.

## See traces of your application in your project

Click the link in your terminal or paste it into your browser to open the Weave UI. In the **Traces** panel of the Weave UI, you can click on the trace to see its data, such as its input, output, latency, and token usage.

![Weave Trace Outputs 1](/images/tutorial_trace_1.png)

## Learn more about Traces

- Learn how to [decorate your functions and retrieve call information](/weave/quickstart-tracing).
- Try the [Playground](/weave/guides/tools/playground) to test different models on logged traces.
- [Explore integrations](/weave/guides/integrations/). Weave automatically tracks calls made to OpenAI, Anthropic and many more LLM libraries. If your LLM library isn't currently one of our integrations you can track calls to other LLMs libraries or frameworks easily by wrapping them with `@weave.op()`.

## Next Steps

[Get started evaluating your app](/weave/tutorial-eval) and then see how to [evaluate a RAG application](/weave/tutorial-rag).