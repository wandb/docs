{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Traces\n",
        "\n",
        "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "Weave is a toolkit for developing AI-powered applications.\n",
        "\n",
        "Use Weave traces to capture the inputs, outputs, and internal structure of your Python function automatically to observe and debug LLM applications.\n",
        "\n",
        "By decorating a function with `@weave.op`, Weave records a rich trace of how your function runs, including any nested operations or external API calls. This makes it easy to debug, understand, and visualize how your code is interacting with language models, all from within your notebook.\n",
        "\n",
        "To get started, complete the prerequisites. Then, define a function decorated with `@weave.op` decorator and run it on an example input to track LLM calls. Weave captures and visualizes the trace automatically."
      ],
      "metadata": {
        "id": "8ZmKBnUE8bGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure your dependencies are installed with:\n",
        "!pip install openai weave"
      ],
      "metadata": {
        "id": "jxECTbwy8cvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "#@title Set up your credentials\n",
        "inference_provider = \"W&B Inference\" #@param [\"W&B Inference\", \"OpenAI\"]\n",
        "\n",
        "# Setup your W&B project and credentials\n",
        "os.environ[\"WANDB_ENTITY_PROJECT\"] = input(\"Setup your W&B project (team name/project name): \")\n",
        "os.environ[\"WANDB_API_KEY\"] = getpass.getpass(\"Setup your W&B API key (Find it at https://wandb.ai/authorize): \")\n",
        "\n",
        "# Setup your OpenAI API key\n",
        "if inference_provider == \"OpenAI\":\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key (Find it at https://platform.openai.com/api-keys): \")"
      ],
      "metadata": {
        "id": "ujOegcPY8j7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTUHuulv8afx"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import weave\n",
        "\n",
        "weave.init(os.environ[\"WANDB_ENTITY_PROJECT\"])\n",
        "\n",
        "@weave.op  # Decorator to track requests\n",
        "def create_completion(message: str) -> str:\n",
        "    if inference_provider == \"W&B Inference\":\n",
        "      client = OpenAI(\n",
        "          base_url=\"https://api.inference.wandb.ai/v1\",\n",
        "          api_key=os.environ[\"WANDB_API_KEY\"],\n",
        "          project=os.environ[\"WANDB_ENTITY_PROJECT\"],\n",
        "      )\n",
        "      model_name: str = \"OpenPipe/Qwen3-14B-Instruct\"\n",
        "    if inference_provider == \"OpenAI\":\n",
        "      client = OpenAI()\n",
        "      model_name: str = \"gpt-4.1-nano\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": message},\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "message = \"Tell me a joke.\"\n",
        "create_completion(message)"
      ]
    }
  ]
}