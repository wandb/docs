{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learn Weave with W&B Inference\n",
        "\n",
        "This guide shows you how to use W&B Weave with [W&B Inference](https://docs.wandb.ai/inference/). Using W&B Inference, you can build and trace LLM applications using live open-source models without setting up your own infrastructure or managing API keys from multiple providers. Just obtain your W&B API key and use it to interact with [all models hosted by W&B Inference](https://docs.wandb.ai/inference/models/).\n",
        "\n",
        "## What you'll learn\n",
        "\n",
        "This guide shows you how to:\n",
        "\n",
        "- Set up Weave and W&B Inference\n",
        "- Build a basic LLM application with automatic tracing\n",
        "- Compare multiple models\n",
        "- Evaluate model performance on a dataset\n",
        "- View your results in the Weave UI\n",
        "\n",
        "## Install Weave and OpenAI"
      ],
      "metadata": {
        "id": "gfpToYNRdk1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install weave openai"
      ],
      "metadata": {
        "id": "GYRXa32Yd4If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Trace your first LLM call\n",
        "\n",
        "Start with a basic example that uses Llama 3.1-8B through W&B Inference.\n",
        "\n",
        "When you run this code, Weave:\n",
        "- Traces your LLM call automatically\n",
        "- Logs inputs, outputs, latency, and token usage\n",
        "- Provides a link to view your trace in the Weave UI"
      ],
      "metadata": {
        "id": "ZZ0lir7qdz1Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0IOwBZkdihh"
      },
      "outputs": [],
      "source": [
        "import weave\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
        "\n",
        "# Initialize Weave - replace with your-team/your-project\n",
        "weave.init(\"<team-name>/inference-quickstart\")\n",
        "\n",
        "# Create an OpenAI-compatible client pointing to W&B Inference\n",
        "client = openai.OpenAI(\n",
        "    base_url='https://api.inference.wandb.ai/v1',\n",
        "    api_key= WANDB_API_KEY,  # Set your API as the WANDB_API_KEY environment variable\n",
        "    project=\"dans-test-team/inference-quickstart\",  # Required for usage tracking\n",
        ")\n",
        "\n",
        "# Decorate your function to enable tracing; use the standard OpenAI client\n",
        "@weave.op()\n",
        "def ask_llama(question: str) -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Call your function - Weave automatically traces everything\n",
        "result = ask_llama(\"What are the benefits of using W&B Weave for LLM development?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Build a text summarization application\n",
        "\n",
        "Next, try running this code, which is a basic summarization app that shows how Weave traces nested operations:"
      ],
      "metadata": {
        "id": "CAu2vifFeDUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import weave\n",
        "import openai\n",
        "\n",
        "WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
        "\n",
        "# Initialize Weave - replace with your-team/your-project\n",
        "weave.init(\"<team-name>/inference-quickstart\")\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url='https://api.inference.wandb.ai/v1',\n",
        "    api_key= WANDB_API_KEY,  # Set your API as the WANDB_API_KEY environment variable\n",
        "    project=\"<team-name>/inference-quickstart\",  # Required for usage tracking\n",
        ")\n",
        "\n",
        "@weave.op()\n",
        "def extract_key_points(text: str) -> list[str]:\n",
        "    \"\"\"Extract key points from a text.\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Extract 3-5 key points from the text. Return each point on a new line.\"},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ],\n",
        "    )\n",
        "    # Returns response without blank lines\n",
        "    return [line for line in response.choices[0].message.content.strip().splitlines() if line.strip()]\n",
        "\n",
        "@weave.op()\n",
        "def create_summary(key_points: list[str]) -> str:\n",
        "    \"\"\"Create a concise summary based on key points.\"\"\"\n",
        "    points_text = \"\\n\".join(f\"- {point}\" for point in key_points)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Create a one-sentence summary based on these key points.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Key points:\\n{points_text}\"}\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "@weave.op()\n",
        "def summarize_text(text: str) -> dict:\n",
        "    \"\"\"Main summarization pipeline.\"\"\"\n",
        "    key_points = extract_key_points(text)\n",
        "    summary = create_summary(key_points)\n",
        "    return {\n",
        "        \"key_points\": key_points,\n",
        "        \"summary\": summary\n",
        "    }\n",
        "\n",
        "# Try it with sample text\n",
        "sample_text = \"\"\"\n",
        "The Apollo 11 mission was a historic spaceflight that landed the first humans on the Moon\n",
        "on July 20, 1969. Commander Neil Armstrong and lunar module pilot Buzz Aldrin descended\n",
        "to the lunar surface while Michael Collins remained in orbit. Armstrong became the first\n",
        "person to step onto the Moon, followed by Aldrin 19 minutes later. They spent about\n",
        "two and a quarter hours together outside the spacecraft, collecting samples and taking photographs.\n",
        "\"\"\"\n",
        "\n",
        "result = summarize_text(sample_text)\n",
        "print(\"Key Points:\", result[\"key_points\"])\n",
        "print(\"\\nSummary:\", result[\"summary\"])"
      ],
      "metadata": {
        "id": "_pHQPYr4eF9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Compare multiple models\n",
        "\n",
        "W&B Inference provides access to multiple models. Use the following code to compare the performance of Llama and DeepSeek's respective responses:"
      ],
      "metadata": {
        "id": "SgkuP67beLFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import weave\n",
        "import openai\n",
        "\n",
        "WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
        "\n",
        "# Initialize Weave - replace with your-team/your-project\n",
        "weave.init(\"<team-name>/inference-quickstart\")\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url='https://api.inference.wandb.ai/v1',\n",
        "    api_key= WANDB_API_KEY,  # Set your API as the WANDB_API_KEY environment variable\n",
        "    project=\"<team-name>/inference-quickstart\",  # Required for usage tracking\n",
        ")\n",
        "\n",
        "# Define a Model class to compare different LLMs\n",
        "class InferenceModel(weave.Model):\n",
        "    model_name: str\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, question: str) -> str:\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": question}\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "# Create instances for different models\n",
        "llama_model = InferenceModel(model_name=\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "deepseek_model = InferenceModel(model_name=\"deepseek-ai/DeepSeek-V3-0324\")\n",
        "\n",
        "# Compare their responses\n",
        "test_question = \"Explain quantum computing in one paragraph for a high school student.\"\n",
        "\n",
        "print(\"Llama 3.1 8B response:\")\n",
        "print(llama_model.predict(test_question))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "print(\"DeepSeek V3 response:\")\n",
        "print(deepseek_model.predict(test_question))"
      ],
      "metadata": {
        "id": "RDIxMR-WealP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Evaluate model performance\n",
        "\n",
        "Evaluate how well a model performs on a Q&A task using Weave's built-in `EvaluationLogger`. This provides structured evaluation tracking with automatic aggregation, token usage capture, and rich comparison features in the UI.\n",
        "\n",
        "Append the following code to the script you used in step 3:"
      ],
      "metadata": {
        "id": "yF_4aTkbeiVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import { EvaluationLogger } from 'weave';\n",
        "\n",
        "// Create a simple dataset\n",
        "interface DatasetExample {\n",
        "  question: string;\n",
        "  expected?: string;\n",
        "  expected_one_of?: string[];\n",
        "}\n",
        "\n",
        "const dataset: DatasetExample[] = [\n",
        "  { question: 'What is 2 + 2?', expected: '4' },\n",
        "  { question: 'What is the capital of France?', expected: 'Paris' },\n",
        "  { question: 'Name a primary color', expected_one_of: ['red', 'blue', 'yellow'] },\n",
        "];\n",
        "\n",
        "// Define a scorer\n",
        "const accuracyScorer = weave.op(function accuracyScorer(args: {\n",
        "  expected: string;\n",
        "  output: string;\n",
        "  expected_one_of?: string[];\n",
        "}): { correct: boolean; score: number } {\n",
        "  const outputClean = args.output.trim().toLowerCase();\n",
        "\n",
        "  let isCorrect: boolean;\n",
        "  if (args.expected_one_of) {\n",
        "    isCorrect = args.expected_one_of.some(option =>\n",
        "      outputClean.includes(option.toLowerCase())\n",
        "    );\n",
        "  } else {\n",
        "    isCorrect = outputClean.includes(args.expected.toLowerCase());\n",
        "  }\n",
        "\n",
        "  return { correct: isCorrect, score: isCorrect ? 1.0 : 0.0 };\n",
        "});\n",
        "\n",
        "// Evaluate a model using Weave's EvaluationLogger\n",
        "async function evaluateModel(\n",
        "  model: (question: string) => Promise<string>,\n",
        "  modelName: string,\n",
        "  dataset: DatasetExample[]\n",
        "): Promise<void> {\n",
        "  // Initialize EvaluationLogger BEFORE calling the model to capture token usage\n",
        "  // This is especially important for W&B Inference to track costs\n",
        "  // Convert model name to a valid format (replace non-alphanumeric chars with underscores)\n",
        "  const safeModelName = modelName.replace(/\\//g, '_').replace(/-/g, '_').replace(/\\./g, '_');\n",
        "  const evalLogger = new EvaluationLogger({\n",
        "    name: 'inference_evaluation',\n",
        "    model: { name: safeModelName },\n",
        "    dataset: 'qa_dataset'\n",
        "  });\n",
        "\n",
        "  for (const example of dataset) {\n",
        "    // Get model prediction\n",
        "    const output = await model(example.question);\n",
        "\n",
        "    // Log the prediction\n",
        "    const predLogger = evalLogger.logPrediction(\n",
        "      { question: example.question },\n",
        "      output\n",
        "    );\n",
        "\n",
        "    // Score the output\n",
        "    const score = await accuracyScorer({\n",
        "      expected: example.expected || '',\n",
        "      output: output,\n",
        "      expected_one_of: example.expected_one_of\n",
        "    });\n",
        "\n",
        "    // Log the score\n",
        "    predLogger.logScore('accuracy', score.score);\n",
        "\n",
        "    // Finish logging for this prediction\n",
        "    predLogger.finish();\n",
        "  }\n",
        "\n",
        "  // Log summary - Weave automatically aggregates the accuracy scores\n",
        "  await evalLogger.logSummary();\n",
        "  console.log(`Evaluation complete for ${modelName} (logged as: ${safeModelName}). View results in the Weave UI.`);\n",
        "}\n",
        "\n",
        "// Compare multiple models - a key feature of Weave's evaluation framework\n",
        "const modelsToCompare = [\n",
        "  { model: llamaModel, name: 'meta-llama/Llama-3.1-8B-Instruct' },\n",
        "  { model: deepseekModel, name: 'deepseek-ai/DeepSeek-V3-0324' },\n",
        "];\n",
        "\n",
        "for (const { model, name } of modelsToCompare) {\n",
        "  await evaluateModel(model, name, dataset);\n",
        "}\n",
        "\n",
        "// In the Weave UI, navigate to the Evals tab to compare results across models"
      ],
      "metadata": {
        "id": "LdoQvGZceoVu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}