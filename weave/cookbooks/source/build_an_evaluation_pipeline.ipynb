{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCJQ_4Wf5Wn_"
      },
      "source": [
        "# Learn how to build an evaluation pipeline with Weave Models and Evaluations\n",
        "\n",
        "Evaluations help you iterate and improve your applications by testing them against a set of examples after you make changes. Weave provides first-class support for tracking evaluations with `Model` and `Evaluation` classes. The APIs are designed with minimal assumptions, allowing flexibility for a wide array of use cases.\n",
        "\n",
        "The guide walks you through how to:\n",
        "\n",
        "* Set up a `Model`\n",
        "* Create a dataset to test an LLM's responses against\n",
        "* Define a scoring function to compare model output to expected outputs\n",
        "* Run an evaluation that tests the model against dataset using the scoring function and an additional built-in scorer\n",
        "* View the results of the evaluation in the Weave UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgrSY50I0KKa"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RlVQ1jl5yfS"
      },
      "source": [
        "## Install and import the necessary libraries and functions\n",
        "\n",
        "Import the following libraries into your script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xBh6uBW0Q--"
      },
      "outputs": [],
      "source": [
        "pip install weave openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsTGblYL5OAy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import openai\n",
        "import asyncio\n",
        "import weave\n",
        "from weave.scorers import MultiTaskBinaryClassificationF1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCQv5V4MyoIs"
      },
      "source": [
        "## Set OpenAI API key and W&B team\n",
        "\n",
        "The following cell prompts you to enter your OpenAI API key and the name of the W&B team you want to send output to. If you do not have a W&B team, [create one](https://docs.wandb.ai/platform/hosting/iam/access-management/manage-organization#create-a-team).\n",
        "\n",
        "The notebook also prompts you to enter you to enter your W&B API key when you run `weave.init()` the first time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wq1I6BryxrD"
      },
      "outputs": [],
      "source": [
        "# Enter your OpenAI API key and your W&B team name when prompted\n",
        "OPENAI_API_KEY = input(\"Enter your OpenAI API key: \")\n",
        "WB_TEAM_NAME = input(\"Enter your W&B team name: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spNgIusa55cs"
      },
      "source": [
        "## Build a `Model`\n",
        "\n",
        "In Weave, [`Models` are objects](/weave/guides/core-types/models) that capture both the behavior of your model/agent (logic, prompt, parameters) and its versioned metadata (parameters, code, micro-config) so you can track, compare, evaluate and iterate reliably.\n",
        "\n",
        "When you instantiate a `Model`, Weave automatically captures its configuration and behaviors and updates the version when there are changes. This allows you to track its performance over time as you iterate on it.\n",
        "\n",
        "`Model`s are declared by subclassing `Model` and implementing a `predict` function definition, which takes one example and returns the response.\n",
        "\n",
        "The following example model uses OpenAI to extract the names, colors, and flavors of alien fruits from sentences sent to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu40eWB259Jp"
      },
      "outputs": [],
      "source": [
        "class ExtractFruitsModel(weave.Model):\n",
        "    model_name: str\n",
        "    prompt_template: str\n",
        "\n",
        "    @weave.op()\n",
        "    async def predict(self, sentence: str) -> dict:\n",
        "        client = openai.AsyncClient(api_key=OPENAI_API_KEY)\n",
        "\n",
        "        response = await client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": self.prompt_template.format(sentence=sentence)}\n",
        "            ],\n",
        "        )\n",
        "        result = response.choices[0].message.content\n",
        "        if result is None:\n",
        "            raise ValueError(\"No response from model\")\n",
        "        parsed = json.loads(result)\n",
        "        return parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATjrIKhW6EqF"
      },
      "source": [
        "The `ExtractFruitsModel` class inherits from (or subclasses) `weave.Model` so that Weave can track the instantiated object. `@weave.op` decorates the predict function to track its inputs and outputs.\n",
        "\n",
        "You can instantiate `Model` objects like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci3Hgx6g6HnY"
      },
      "outputs": [],
      "source": [
        "weave.init(WB_TEAM_NAME + '/eval_pipeline_quickstart')\n",
        "\n",
        "model = ExtractFruitsModel(\n",
        "    model_name='gpt-3.5-turbo-1106',\n",
        "    prompt_template='Extract fields (\"fruit\": <str>, \"color\": <str>, \"flavor\": <str>) from the following text, as json: {sentence}'\n",
        ")\n",
        "\n",
        "sentence = \"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.\"\n",
        "\n",
        "await model.predict(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvXYr0Do6OYr"
      },
      "source": [
        "## Create a dataset\n",
        "\n",
        "Next, you need a dataset to evaluate your model on. A [`Dataset` is a collection of examples stored as a Weave object](/weave/guides/core-types/datasets).\n",
        "\n",
        "The following example dataset defines three example input sentences and their correct answers (`labels`), and then formats them in a JSON table table format that scoring functions can read.\n",
        "\n",
        "This example builds a list of examples in code, but you can also log them one at a time from your running application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2ebsKQx6S5x"
      },
      "outputs": [],
      "source": [
        "sentences = [\"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.\",\n",
        "\"Pounits are a bright green color and are more savory than sweet.\",\n",
        "\"Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\"]\n",
        "labels = [\n",
        "    {'fruit': 'neoskizzles', 'color': 'purple', 'flavor': 'candy'},\n",
        "    {'fruit': 'pounits', 'color': 'bright green', 'flavor': 'savory'},\n",
        "    {'fruit': 'glowls', 'color': 'pale orange', 'flavor': 'sour and bitter'}\n",
        "]\n",
        "examples = [\n",
        "    {'id': '0', 'sentence': sentences[0], 'target': labels[0]},\n",
        "    {'id': '1', 'sentence': sentences[1], 'target': labels[1]},\n",
        "    {'id': '2', 'sentence': sentences[2], 'target': labels[2]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MKpu3aH6Ydh"
      },
      "source": [
        "Then create your dataset using the `weave.Dataset()` class and publish it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjAoQRz-6ba5"
      },
      "outputs": [],
      "source": [
        "weave.init(WB_TEAM_NAME + '/eval_pipeline_quickstart')\n",
        "dataset = weave.Dataset(name='fruits', rows=examples)\n",
        "weave.publish(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIbmhBsq6fKv"
      },
      "source": [
        "## Define custom scoring functions\n",
        "\n",
        "When using Weave evaluations, Weave expects a `target` to compare `output` against. The following scoring function takes two dictionaries (`target` and `output`) and returns a dictionary of boolean values indicating whether the output matches the target. The `@weave.op()` decorator enables Weave to track the scoring function's execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m08EagY6n2i"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def fruit_name_score(target: dict, output: dict) -> dict:\n",
        "    return {'correct': target['fruit'] == output['fruit']}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_SgEbEN6xP6"
      },
      "source": [
        "To make your own scoring function, learn more in the [Scorers](/weave/guides/evaluation/scorers) guide.\n",
        "\n",
        "In some applications, you may want to create custom `Scorer` classes. For example, you might create a standardized `LLMJudge` class with specific parameters (such as chat model or prompt), specific row scoring, and aggregate score calculation. See the tutorial on defining a `Scorer` class in the next chapter on [Model-Based Evaluation of RAG applications](/weave/tutorial-rag#optional-defining-a-scorer-class) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09tbG6jQ62ar"
      },
      "source": [
        "## Use a built-in scorer and run the evaluation\n",
        "\n",
        "Along with custom scoring functions, you can also use [Weave's built-in scorers](/weave/guides/evaluation/builtin_scorers). In the following evaluation, `weave.Evaluation()` uses the `fruit_name_score` function defined in the previous section and the built-in `MultiTaskBinaryClassificationF1` scorer, which computes [F1 scores](https://en.wikipedia.org/wiki/F-score).\n",
        "\n",
        "The following example runs an evaluation of `ExtractFruitsModel` on the `fruits` dataset using the scoring the two functions and logs the results to Weave."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4UcdSG265jy"
      },
      "outputs": [],
      "source": [
        "weave.init(WB_TEAM_NAME + '/eval_pipeline_quickstart''eval_pipeline_quickstart')\n",
        "\n",
        "evaluation = weave.Evaluation(\n",
        "    name='fruit_eval',\n",
        "    dataset=dataset,\n",
        "    scorers=[\n",
        "        MultiTaskBinaryClassificationF1(class_names=[\"fruit\", \"color\", \"flavor\"]),\n",
        "        fruit_name_score\n",
        "    ],\n",
        ")\n",
        "\n",
        "# if you're in a Jupyter Notebook, run:\n",
        "await evaluation.evaluate(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scmNnXcT7Gly"
      },
      "source": [
        "## View your evaluation results\n",
        "\n",
        "Weave automatically captures traces of each prediction and score. Click on the link printed by the evaluation to view the results in the Weave UI.\n",
        "\n",
        "![Evaluation results](/images/evals-hero.png)\n",
        "\n",
        "## Learn more about Weave evaluations\n",
        "\n",
        "* Learn more about how to [build and use scorers](/weave/guides/evaluation/scorers).\n",
        "* Check out Weave's [built-in scoring functions](/weave/guides/evaluation/builtin_scorers).\n",
        "* Learn about [Model-Based Evaluation](/weave/guides/evaluation/scorers#model-based-evaluation) for using LLMs as judges.\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "[Build a RAG application](/weave/tutorial-rag) to learn about evaluating retrieval-augmented generation."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
