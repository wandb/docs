---
title: "Features"
description: "A comprehensive overview of W&B Weave capabilities for LLM application development."
---

{/* DRAFT: This page needs review and refinement before publishing. */}

W&B Weave provides a complete toolkit for building, testing, and operating LLM applications. This page summarizes the key features available in Weave.

## Tracing and observability

Track every call in your LLM application to understand behavior, debug issues, and optimize performance.

| Feature | Description |
|---------|-------------|
| **Automatic LLM tracing** | Weave automatically traces calls to supported LLM providers (OpenAI, Anthropic, Google, and others) without code changes. |
| **Custom function tracing** | Use `@weave.op()` to trace any function and capture inputs, outputs, timing, and errors. |
| **Nested call tracking** | See parent-child relationships between calls to understand your application's execution flow. |
| **Cost tracking** | Automatically calculate token usage and costs for LLM calls based on provider pricing. |
| **Rich media support** | Log and display images, audio, video, and other media types in traces. |
| **Trace comparison** | Compare traces side-by-side to understand differences between executions. |
| **Thread tracking** | Group related calls into threads for multi-turn conversation analysis. |
| **OpenTelemetry support** | Send traces from OpenTelemetry-instrumented applications to Weave. |

## Evaluation

Systematically test your LLM application against curated examples to measure and improve quality.

| Feature | Description |
|---------|-------------|
| **Evaluation pipelines** | Run your application against datasets and score outputs with custom or built-in scorers. |
| **Datasets** | Create, version, and manage collections of test examples. |
| **Custom scorers** | Write scoring functions tailored to your application's quality criteria. |
| **Built-in scorers** | Use pre-built scorers for common tasks like hallucination detection, summarization quality, and more. |
| **LLM judges** | Use LLMs to evaluate outputs with customizable judge prompts. |
| **Leaderboards** | Compare model performance across evaluations with configurable leaderboard views. |
| **EvaluationLogger** | Log evaluation results from external systems or custom evaluation workflows. |

## Versioning

Track changes to your prompts, models, and data to understand what changed and reproduce results.

| Feature | Description |
|---------|-------------|
| **Object versioning** | Weave automatically versions any tracked object when it changes. |
| **Model tracking** | Version model configurations, parameters, and code together. |
| **Prompt management** | Version prompt templates and track which prompts were used in each call. |
| **Dataset versioning** | Track changes to evaluation datasets over time. |
| **Op versioning** | Automatically version functions when their code changes. |

## Production tools

Collect feedback, monitor quality, and protect your application in production.

| Feature | Description |
|---------|-------------|
| **Feedback collection** | Capture user feedback, annotations, and corrections on production traces. |
| **Guardrails** | Run scorers on production traffic to catch issues before they reach users. |
| **Monitors** | Track quality metrics over time and set up alerts for regressions. |
| **PII redaction** | Automatically redact sensitive information from traces. |
| **Attributes** | Tag traces with custom metadata for filtering and analysis. |

## Developer experience

Tools to help you iterate faster during development.

| Feature | Description |
|---------|-------------|
| **Playground** | Test prompts and models interactively with the Weave Playground. |
| **Evaluation playground** | Run quick evaluations on traced calls directly from the UI. |
| **Saved views** | Save and share filtered views of your traces and evaluations. |
| **Comparison tools** | Compare objects, traces, and evaluation results side-by-side. |

## Integrations

Connect Weave to your existing tools and infrastructure.

### LLM providers

Weave automatically traces calls to these providers:

- Amazon Bedrock
- Anthropic
- Azure OpenAI
- Cerebras
- Cohere
- Google (Gemini, Vertex AI)
- Groq
- Hugging Face
- LiteLLM
- Mistral
- NVIDIA NIM
- OpenAI
- OpenRouter
- Together AI
- Local models (Ollama, vLLM)

### Frameworks

Weave integrates with popular LLM frameworks:

- OpenAI Agents SDK
- LangChain
- LlamaIndex
- DSPy
- Instructor
- CrewAI
- Smolagents
- Pydantic AI
- Google ADK
- Agno
- Koog
- AutoGen
- Verdict
- Verifiers

### Protocols

- Model Context Protocol (MCP)

## Platform and deployment

| Feature | Description |
|---------|-------------|
| **W&B Cloud** | Use Weave on W&B's managed cloud infrastructure. |
| **Self-managed deployment** | Deploy Weave on your own infrastructure for data residency requirements. |
| **Team collaboration** | Share projects, traces, and evaluations with your team. |
| **Access control** | Manage permissions with W&B's identity and access management. |

## SDKs

Weave provides official SDKs for:

- **Python**: Full-featured SDK with all Weave capabilities
- **TypeScript**: SDK for Node.js applications with core tracing and evaluation features

## Next steps

<CardGroup cols={2}>
  <Card title="Quickstart" icon="rocket" href="/weave/quickstart">
    Get started with Weave in 5 minutes
  </Card>
  <Card title="Pricing" icon="credit-card" href="/weave/details/pricing">
    Understand Weave pricing and plans
  </Card>
</CardGroup>

