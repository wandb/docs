---
description: The Prompts Quickstart shows how to visualise and debug the execution flow of your LLM chains and pipelines
---

# Quickstart

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://wandb.me/prompts-quickstart)


<head>
  <title>Prompts Quickstart</title>
</head>

This Quickstart guide will walk you how to use [Trace](intro.md) to visualize and debug calls to LangChain or any other LLM Chain.

<!-- This Quickstart guide will walk you how to use Weights & Biases (W&B) Prompts tools to visualise and debug the execution flow of your LLM chains or pipelines. -->


## Use Trace with LangChain

With one line of code W&B Trace will automatically and continuously log calls to a [LangChain Model](https://python.langchain.com/en/latest/modules/models.html), [Chain](https://python.langchain.com/en/latest/modules/chains.html), or [Agent](https://python.langchain.com/en/latest/modules/agents.html).

Follow the steps below to visualize and debug LangChain. For this demo, we will use a LangChain Math agent.

### 1. Import and initialize WandbTracer

First, import `WandbTracer` from `wandb.integration.langchain`.  Then call the `init()` method to make W&B start watching for calls to LangChain Models, Chains, or Agents.

```python
from wandb.integration.langchain import WandbTracer

WandbTracer.init({"project": "wandb_prompts"})
```

You can optionally pass a dictionary with argument that `wandb.init()` accepts to `WandbTracer.init`. This includes a project name, team name, entity, and more. For more information about [`wandb.init`](../../ref/python/init.md), see the API Reference Guide.


Once the chain execution completes, any call to a LangChain object is logged automatically to the W&B Trace. 

### 2. Set up your LangChain Agent
Import an OpenAI Langchain Agent and create a math tool(function) with `load_tools`.  Next create a math agent with the [`initialize_agent`](https://python.langchain.com/en/latest/_modules/langchain/agents/initialize.html) method and pass the tool object to `initialize_agent`:

```python
from langchain.llms import OpenAI
from langchain.agents import load_tools, initialize_agent, AgentType

llm = OpenAI(temperature=0)
tools = load_tools(["llm-math"], llm=llm)
math_agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)
```

### 3. Make calls to your Agent

Every call made to the agent (in this example, `math_agent`) is logged once the execution is complete.

The parameters used to create objects are also logged:

```python
questions = [
    "Find the square root of 5.4.",
    "What is 3 divided by 7.34 raised to the power of pi?",
    "What is the sin of 0.47 radians, divided by the cube root of 27?"
]

for question in questions:
  try:
    answer = math_agent.run(question)
    print(answer)
  except Exception as e:
    print(e)
    pass
```

### 4. View the trace

Click on the Weights & Biases [run](../runs/intro.md) link generated by `WandbTracer.init` in the previous step. This will redirect you to your project workspace in the W&B App. 

Select a run you created to view the trace table, trace timeline and the model architecture of your LLM. 

![](/images/prompts/trace_timeline_detailed.png)




### 5. Stop watching
We recommend that you call `WandbTracer.finish` to close all W&B processes when you are finished with your development.

```python
WandbTracer.finish()
```



## Use Trace with Any LLM Chain or Plug-In

When logging with Trace, a single run can have multiple calls to a LLM, Tool, Chain or Agent logged to it, there is no need to start a new run after each generation from your model, each call will be appended to the Trace Table.

To use Trace with your own chains, plug-ins or pipelines, you first need to create traces using the `Span` and `TraceTree` data types. A _Span_ represents a unit of work.

### 1. Create a Span
First, create a span object. Import `trace_tree` from the `wandb.sdk.data_types`:

```python
from wandb.sdk.data_types import trace_tree

# span = trace_tree.Span(name="Example Span")
# Parent Span - Create a span for your high level agent
agent_span = trace_tree.Span(name="Auto-GPT", span_kind = trace_tree.SpanKind.AGENT)
```

Spans can be of type `AGENT`, `CHAIN`, `TOOL` or `LLM`

### 2. Add child Spans
Nest child Spans within the parent span so that they are nested and in the correct order in the Trace Timeline view. Below, 2 child spans and 1 grandchild span are created.

```python
tool_span = trace_tree.Span(
  name="Tool 1", span_kind = trace_tree.SpanKind.TOOL
)

chain_span = trace_tree.Span(
  name="LLM CHAIN 1", span_kind = trace_tree.SpanKind.CHAIN
)

llm_span = trace_tree.Span(
  name="LLM 1", span_kind = trace_tree.SpanKind.LLM
)

chain_span.add_child_span(llm_span)
agent_span.add_child_span(tool_span)
agent_span.add_child_span(chain_span)
```

### 3. Add the inputs and outputs

Populate spans with the input and output data

```python
tool_span.add_named_result(
  {"input": "search: google founded in year"}, 
  {"response": "1998"}
)

chain_span.add_named_result(
  {"input": "calculate: 2023 - 1998"}, 
  {"response": "25"}
)

llm_span.add_named_result(
  {"system": "you are a helpful assistant", 
    "input": "calculate: 2023 - 1998"}, 
  {"response": "25", "tokens_used": 218}
)

agent_span.add_named_result(
  {"user": "How old is google?"},
  {"response": "25 years old"}
)
```

### 4. Log the spans to Weights & Biases' Trace 

This will allow you to visualize the Trace Table, Trace Timeline, and Model Architecture.

```python
import wandb 

trace = trace_tree.WBTraceTree(agent_span)
run = wandb.init(project="wandb_prompts")
run.log({"trace": trace})
run.finish()
```
### 5. View the trace
Click on the W&B run link that gets generated to see the trace of your LLM.