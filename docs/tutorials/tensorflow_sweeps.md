
# TensorFlow Sweeps

[**Try in a Colab Notebook here â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/tensorflow/Hyperparameter_Optimization_in_TensorFlow_using_W&B_Sweeps.ipynb)

Weights & Biasesã‚’ä½¿ç”¨ã—ã¦ã€æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿé¨“ã®è¿½è·¡ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ãŠã‚ˆã³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚

<img src="http://wandb.me/mini-diagram" width="650" alt="Weights & Biases" />

Weights & Biases Sweepsã‚’ä½¿ç”¨ã—ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®æœ€é©åŒ–ã‚’è‡ªå‹•åŒ–ã—ã€å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ç©ºé–“ã‚’æ¢ç´¢ã—ã¾ã™ã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä»˜ãã§ã™ã€‚

![](https://i.imgur.com/AN0qnpC.png)

## ğŸ¤” Sweepsã‚’ä½¿ã†ç†ç”±

* **ç°¡å˜ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: ã‚ãšã‹æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã§W&B Sweepsã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚
* **é€æ˜æ€§**: å½“ç¤¾ãŒä½¿ç”¨ã—ã¦ã„ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å¼•ç”¨ã—ã¦ãŠã‚Šã€[ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ã™](https://github.com/wandb/client/tree/master/wandb/sweeps)ã€‚
* **å¼·åŠ›**: å½“ç¤¾ã®Sweepsã¯å®Œå…¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ã‹ã¤è¨­å®šå¯èƒ½ã§ã™ã€‚å¤šæ•°ã®ãƒã‚·ãƒ³ã§ã‚¹ã‚¤ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã€ãƒ©ãƒƒãƒ—ãƒˆãƒƒãƒ—ã§ã‚¹ã‚¤ãƒ¼ãƒ—ã‚’é–‹å§‹ã™ã‚‹ã®ã¨åŒã˜ãã‚‰ã„ç°¡å˜ã§ã™ã€‚

**[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ $\rightarrow$](https://docs.wandb.com/sweeps)**

## ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã‚«ãƒãƒ¼ã™ã‚‹å†…å®¹

* TensorFlowã§ã®ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨ã—ã¦ã€W&B Sweepã‚’å§‹ã‚ã‚‹ãŸã‚ã®ç°¡å˜ãªæ‰‹é †ã€‚
* ç”»åƒåˆ†é¡ã‚¿ã‚¹ã‚¯ã«æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã€‚

**æ³¨**: _Step_ ã§å§‹ã¾ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æ¢ç´¢ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«å¿…è¦ãªã™ã¹ã¦ã§ã™ã€‚ãã‚Œä»¥å¤–ã®ã‚³ãƒ¼ãƒ‰ã¯å˜ç´”ãªä¾‹ã‚’è¨­å®šã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚

# ğŸš€ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€ãƒ­ã‚°ã‚¤ãƒ³

### Step 0ï¸âƒ£: W&Bã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```python
%%capture
!pip install wandb
```

### Step 1ï¸âƒ£: W&Bã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ãƒ­ã‚°ã‚¤ãƒ³

```python
import tqdm
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.datasets import cifar10

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

```python
import wandb
from wandb.keras import WandbCallback

wandb.login()
```

> ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒˆ: åˆã‚ã¦W&Bã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãªã„å ´åˆã€`wandb.login()` ã‚’å®Ÿè¡Œã—ãŸå¾Œã«è¡¨ç¤ºã•ã‚Œã‚‹ãƒªãƒ³ã‚¯ã‹ã‚‰ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—/ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—ã¯æ•°å›ã®ã‚¯ãƒªãƒƒã‚¯ã§å®Œäº†ã—ã¾ã™ã€‚

# ğŸ‘©â€ğŸ³ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™

```python
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

x_train = x_train/255.
x_test = x_test/255.
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))
```

# ğŸ§  ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã®å®šç¾©

## ğŸ—ï¸ ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†é¡å™¨MLPã®æ§‹ç¯‰

```python
def Model():
    inputs = keras.Input(shape=(784,), name="digits")
    x1 = keras.layers.Dense(64, activation="relu")(inputs)
    x2 = keras.layers.Dense(64, activation="relu")(x1)
    outputs = keras.layers.Dense(10, name="predictions")(x2)

    return keras.Model(inputs=inputs, outputs=outputs)

    
def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)

    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))

    train_acc_metric.update_state(y, logits)

    return loss_value

    
def test_step(x, y, model, loss_fn, val_acc_metric):
    val_logits = model(x, training=False)
    loss_value = loss_fn(y, val_logits)
    val_acc_metric.update_state(y, val_logits)

    return loss_value
```

## ğŸ” ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã®ä½œæˆ

### Step 3ï¸âƒ£: `wandb.log`ã‚’ä½¿ã£ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã™ã‚‹

```python
def train(train_dataset,
          val_dataset, 
          model,
          optimizer,
          loss_fn,
          train_acc_metric,
          val_acc_metric,
          epochs=10, 
          log_step=200, 
          val_log_step=50):
  
    for epoch in range(epochs):
        print("\nStart of epoch %d" % (epoch,))

        train_loss = []   
        val_loss = []

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒƒãƒã‚’ç¹°ã‚Šè¿”ã—å‡¦ç†ã™ã‚‹
        for step, (x_batch_train, y_batch_train) in tqdm.tqdm(enumerate(train_dataset), total=len(train_dataset)):
            loss_value = train_step(x_batch_train, y_batch_train, 
                                    model, optimizer, 
                                    loss_fn, train_acc_metric)
            train_loss.append(float(loss_value))

        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ
        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):
            val_loss_value = test_step(x_batch_val, x_batch_val, 
                                       model, loss_fn, 
                                       val_acc_metric)
            val_loss.append(float(val_loss_value))
            
        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º
        train_acc = train_acc_metric.result()
        print("Training acc over epoch: %.4f" % (float(train_acc),))

        val_acc = val_acc_metric.result()
        print("Validation acc: %.4f" % (float(val_acc),))

        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
        train_acc_metric.reset_states()
        val_acc_metric.reset_states()

        # 3ï¸âƒ£ wandb.logã‚’ä½¿ã£ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã™ã‚‹
        wandb.log({'epochs': epoch,
                   'loss': np.mean(train_loss),
                   'acc': float(train_acc), 
                   'val_loss': np.mean(val_loss),
                   'val_acc': float(val_acc)})
```

# Step 4ï¸âƒ£: Sweepã®è¨­å®š

ã“ã“ã§ã¯ä»¥ä¸‹ã‚’è¡Œã„ã¾ã™ã€‚
* ã‚¹ã‚¤ãƒ¼ãƒ—ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å®šç¾©
* ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æä¾›ã€‚`random`ã€`grid`ã€`bayes`ãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚
* ç›®çš„ã¨ã™ã‚‹`metric`ãŠã‚ˆã³`bayes`ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ä¾‹ãˆã°`val_loss`ã‚’`minimize`ã™ã‚‹ã€‚
* ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ä½ã„runã®æ—©æœŸçµ‚äº†ã«ã¯`hyperband`ã‚’ä½¿ç”¨

#### [Sweep Configsã®è©³ç´°ã¯ã“ã¡ã‚‰â†’](https://docs.wandb.com/sweeps/configuration)

```python
sweep_config = {
  'method': 'random', 
  'metric': {
      'name': 'val_loss',
      'goal': 'minimize'
  },
  'early_terminate':{
      'type': 'hyperband',
      'min_iter': 5
  },
  'parameters': {
      'batch_size': {
          'values': [32, 64, 128, 256]
      },
      'learning_rate':{
          'values': [0.01, 0.005, 0.001, 0.0005, 0.0001]
      }
  }
}
```

# Step 5ï¸âƒ£: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ãƒ©ãƒƒãƒ—ã™ã‚‹

ä»¥ä¸‹ã®ã‚ˆã†ã«ã€`train`ãŒå‘¼ã³å‡ºã•ã‚Œã‚‹å‰ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’è¨­å®šã™ã‚‹ãŸã‚ã«`wandb.config`ã‚’ä½¿ç”¨ã™ã‚‹é–¢æ•°ãŒå¿…è¦ã§ã™ã€‚

```python
def sweep_train(config_defaults=None):
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
    config_defaults = {
        "batch_size": 64,
        "learning_rate": 0.01
    }
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã§wandbã‚’åˆæœŸåŒ–
    wandb.init(config=config_defaults)  # ã“ã‚Œã¯Sweepã§ä¸Šæ›¸ãã•ã‚Œã¾ã™

    # ä»–ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’è¨­å®šã«æŒ‡å®š
    wandb.config.epochs = 2
    wandb.config.log_step = 20
    wandb.config.val_log_step = 50
    wandb.config.architecture_name = "MLP"
    wandb.config.dataset_name = "MNIST"

    # tf.dataã‚’ä½¿ç”¨ã—ã¦å…¥åŠ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    train_dataset = (train_dataset.shuffle(buffer_size=1024)
                                  .batch(wandb.config.batch_size)
                                  .prefetch(buffer_size=tf.data.AUTOTUNE))

    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
    val_dataset = (val_dataset.batch(wandb.config.batch_size)
                              .prefetch(buffer_size=tf.data.AUTOTUNE))

    # ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    model = Model()

    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    optimizer = keras.optimizers.SGD(learning_rate=wandb.config.learning_rate)
    # ãƒ­ã‚¹é–¢æ•°ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æº–å‚™
    train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
    val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

    train(train_dataset,
          val_dataset, 
          model,
          optimizer,
          loss_fn,
          train_acc_metric,
          val_acc_metric,
          epochs=wandb.config.epochs, 
          log_step=wandb.config.log_step, 
          val_log_step=wandb.config.val_log_step)
```

# Step 6ï¸âƒ£: Sweepã®åˆæœŸåŒ–ã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œ

```python
sweep_id = wandb.sweep(sweep_config, project="sweeps-tensorflow")
```

`count`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã§ç·runæ•°ã‚’åˆ¶é™ã§ãã¾ã™ã€‚ã“ã“ã§ã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œã‚’æ—©ãã™ã‚‹ãŸã‚ã«10ã«åˆ¶é™ã—ã¾ã™ã€‚runæ•°ã‚’å¢—ã‚„ã—ã¦ä½•ãŒèµ·ã“ã‚‹ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
wandb.agent(sweep_id, function=sweep_train, count=10)
```

# ğŸ‘€ çµæœã®å¯è¦–åŒ–

ä¸Šè¨˜ã®**Sweep URL**ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ©ã‚¤ãƒ–çµæœã‚’ç¢ºèªã—ã¾ã™ã€‚

# ğŸ¨ ã‚®ãƒ£ãƒ©ãƒªãƒ¼ä¾‹

W&Bã§è¿½è·¡ãŠã‚ˆã³å¯è¦–åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¾‹ã¯[Gallery â†’](https://app.wandb.ai/gallery)ã‚’ã”è¦§ãã ã•ã„ã€‚

# ğŸ“ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
1. **Projects**: è¤‡æ•°ã®runã‚’ãƒ­ã‚°ã—ã¦æ¯”è¼ƒã—ã¾ã™ã€‚`wandb.init(project="project-name")`
2. **Groups**: è¤‡æ•°ãƒ—ãƒ­ã‚»ã‚¹ã‚„äº¤å·®æ¤œè¨¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®å ´åˆã€å„ãƒ—ãƒ­ã‚»ã‚¹ã‚’runsã¨ã—ã¦ãƒ­ã‚°ã—ã€ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¾ã™ã€‚`wandb.init(group='experiment-1')`
3. **Tags**: ç¾åœ¨ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¾ãŸã¯ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã«ã‚¿ã‚°ã‚’è¿½åŠ ã—ã¾ã™ã€‚
4. **Notes**: ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ¡ãƒ¢ã‚’å…¥åŠ›ã—ã¦runé–“ã®å¤‰æ›´ã‚’è¿½è·¡ã—ã¾ã™ã€‚
5. **Reports**: åŒåƒšã¨é€²æ—ã‚’å…±æœ‰ã™ã‚‹ãŸã‚ã®ã‚¯ã‚¤ãƒƒã‚¯ãƒ¡ãƒ¢ã‚’ä½œæˆã—ã€MLãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆåŒ–ã—ã¾ã™ã€‚

# ğŸ¤“ é«˜åº¦ãªè¨­å®š
1. [Environment variables](https://docs.wandb.com/library/environment-variables): APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¦ã€ç®¡ç†ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
2. [Offline mode](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): `dryrun`ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€å¾Œã§çµæœã‚’åŒæœŸã—ã¾ã™ã€‚
3. [On-prem](https://docs.wandb.com/self-hosted): ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚¯ãƒ©ã‚¦ãƒ‰ã‚„è‡ªç¤¾ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã®ã‚¨ã‚¢ã‚®ãƒ£ãƒƒãƒ—ã•ã‚ŒãŸã‚µãƒ¼ãƒãƒ¼ã«W&Bã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚å¤§å­¦ã‹ã‚‰ä¼æ¥­ãƒãƒ¼ãƒ ã¾ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚