
# TensorFlow Sweeps

[**Try in a Colab Notebook here â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/tensorflow/Hyperparameter_Optimization_in_TensorFlow_using_W&B_Sweeps.ipynb)

Weights & Biasesã‚’ä½¿ç”¨ã—ã¦æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿé¨“ç®¡ç†ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ãŠã‚ˆã³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å…±åŒä½œæ¥­ã‚’è¡Œã„ã¾ã™ã€‚

<img src="http://wandb.me/mini-diagram" width="650" alt="Weights & Biases" />

Weights & Biasesã®Sweepsã‚’ä½¿ã£ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®æœ€é©åŒ–ã‚’è‡ªå‹•åŒ–ã—ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ç©ºé–“ã‚’æ¢ç´¢ã—ã¾ã™ã€‚

![](https://i.imgur.com/AN0qnpC.png)


## ğŸ¤” ãªãœSweepsã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ

* **ã‚¯ã‚¤ãƒƒã‚¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã ã‘ã§W&Bã®sweepsã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚
* **é€æ˜æ€§**: ä½¿ç”¨ã—ã¦ã„ã‚‹å…¨ã¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ˜è¨˜ã—ã¦ãŠã‚Šã€[ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ã™](https://github.com/wandb/client/tree/master/wandb/sweeps)ã€‚
* **å¼·åŠ›**: Sweepsã¯å®Œå…¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒ»è¨­å®šå¯èƒ½ã§ã™ã€‚æ•°åã®ãƒã‚·ãƒ³ã«ã¾ãŸãŒã‚‹sweepã‚’èµ·å‹•ã™ã‚‹ã®ã‚‚ã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã§sweepã‚’é–‹å§‹ã™ã‚‹ã®ã¨åŒã˜ãã‚‰ã„ç°¡å˜ã§ã™ã€‚

**[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¦‹ã‚‹ $\rightarrow$](https://docs.wandb.com/sweeps)**


## ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã‚«ãƒãƒ¼ã™ã‚‹å†…å®¹

* TensorFlowã§ç‹¬è‡ªã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ã£ã¦W&B Sweepã‚’é–‹å§‹ã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªæ‰‹é †ã€‚
* ç”»åƒåˆ†é¡ã‚¿ã‚¹ã‚¯ã®æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚

**æ³¨æ„**: _Step_ã‹ã‚‰å§‹ã¾ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼sweepã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«å¿…è¦ãªã‚‚ã®ã§ã™ã€‚
ä»–ã®ã‚³ãƒ¼ãƒ‰ã¯å˜ç´”ãªä¾‹ã‚’è¨­å®šã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚

# ğŸš€ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€ãƒ­ã‚°ã‚¤ãƒ³

### Step 0ï¸âƒ£: W&Bã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«


```python
%%capture
!pip install wandb
```

### Step 1ï¸âƒ£: W&Bã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ãƒ­ã‚°ã‚¤ãƒ³


```python
import tqdm
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.datasets import cifar10

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```


```python
import wandb
from wandb.keras import WandbCallback

wandb.login()
```

> ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒˆ: ã“ã‚ŒãŒåˆã‚ã¦ã®W&Bã®ä½¿ç”¨ã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãªã„å ´åˆã€`wandb.login()`ã‚’å®Ÿè¡Œã—ãŸå¾Œã«è¡¨ç¤ºã•ã‚Œã‚‹ãƒªãƒ³ã‚¯ã§ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—/ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—ã¯æ•°ã‚¯ãƒªãƒƒã‚¯ã§å®Œäº†ã—ã¾ã™ã€‚

# ğŸ‘©â€ğŸ³ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™


```python
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

x_train = x_train/255.
x_test = x_test/255.
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))
```

# ğŸ§  ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã®å®šç¾©

## ğŸ—ï¸ ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†é¡å™¨MLPã‚’æ§‹ç¯‰


```python
def Model():
    inputs = keras.Input(shape=(784,), name="digits")
    x1 = keras.layers.Dense(64, activation="relu")(inputs)
    x2 = keras.layers.Dense(64, activation="relu")(x1)
    outputs = keras.layers.Dense(10, name="predictions")(x2)

    return keras.Model(inputs=inputs, outputs=outputs)

    
def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)

    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))

    train_acc_metric.update_state(y, logits)

    return loss_value

    
def test_step(x, y, model, loss_fn, val_acc_metric):
    val_logits = model(x, training=False)
    loss_value = loss_fn(y, val_logits)
    val_acc_metric.update_state(y, val_logits)

    return loss_value
```

## ğŸ” ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ›¸ã

### Step 3ï¸âƒ£: `wandb.log`ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°


```python
def train(train_dataset,
          val_dataset, 
          model,
          optimizer,
          loss_fn,
          train_acc_metric,
          val_acc_metric,
          epochs=10, 
          log_step=200, 
          val_log_step=50):
  
    for epoch in range(epochs):
        print("\nStart of epoch %d" % (epoch,))

        train_loss = []   
        val_loss = []

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒƒãƒã‚’ç¹°ã‚Šè¿”ã—å‡¦ç†
        for step, (x_batch_train, y_batch_train) in tqdm.tqdm(enumerate(train_dataset), total=len(train_dataset)):
            loss_value = train_step(x_batch_train, y_batch_train, 
                                    model, optimizer, 
                                    loss_fn, train_acc_metric)
            train_loss.append(float(loss_value))

        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ
        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):
            val_loss_value = test_step(x_batch_val, y_batch_val, 
                                       model, loss_fn, 
                                       val_acc_metric)
            val_loss.append(float(val_loss_value))
            
        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º
        train_acc = train_acc_metric.result()
        print("Training acc over epoch: %.4f" % (float(train_acc),))

        val_acc = val_acc_metric.result()
        print("Validation acc: %.4f" % (float(val_acc),))

        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
        train_acc_metric.reset_states()
        val_acc_metric.reset_states()

        # 3ï¸âƒ£ wandb.logã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°
        wandb.log({'epochs': epoch,
                   'loss': np.mean(train_loss),
                   'acc': float(train_acc), 
                   'val_loss': np.mean(val_loss),
                   'val_acc': float(val_acc)})
```

# Step 4ï¸âƒ£: Sweepã‚’è¨­å®šã™ã‚‹

ã“ã“ã§è¡Œã†ã“ã¨ã¯:
* æ¢ç´¢ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å®šç¾©
* ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®æœ€é©åŒ–æ–¹æ³•ã‚’æä¾›ã—ã¾ã™ã€‚ `random`, `grid`, `bayes` ãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚
* `bayes`ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ç›®çš„ã¨`metric`ã‚’æä¾›ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€`val_loss`ã‚’`æœ€å°åŒ–`ã™ã‚‹ã€‚
* `hyperband`ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ä½ã„runã‚’æ—©æœŸçµ‚äº†

#### [Sweepã®è¨­å®šã«é–¢ã™ã‚‹è©³ç´°ã¯ã“ã¡ã‚‰ $\rightarrow$](https://docs.wandb.com/sweeps/configuration)


```python
sweep_config = {
  'method': 'random', 
  'metric': {
      'name': 'val_loss',
      'goal': 'minimize'
  },
  'early_terminate':{
      'type': 'hyperband',
      'min_iter': 5
  },
  'parameters': {
      'batch_size': {
          'values': [32, 64, 128, 256]
      },
      'learning_rate':{
          'values': [0.01, 0.005, 0.001, 0.0005, 0.0001]
      }
  }
}
```

# Step 5ï¸âƒ£: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ãƒ©ãƒƒãƒ—

`sweep_train`ã®ã‚ˆã†ãªé–¢æ•°ãŒå¿…è¦ã§ã™ã€‚ã“ã®é–¢æ•°ã¯`wandb.config`ã‚’ä½¿ç”¨ã—ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’è¨­å®šã—ã€ãã®å¾Œã« `train` ãŒå‘¼ã³å‡ºã•ã‚Œã¾ã™ã€‚


```python
def sweep_train(config_defaults=None):
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
    config_defaults = {
        "batch_size": 64,
        "learning_rate": 0.01
    }
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã§wandbã‚’åˆæœŸåŒ–
    wandb.init(config=config_defaults)  # ã“ã‚Œã¯Sweepã§ä¸Šæ›¸ãã•ã‚Œã¾ã™

    # ãã®ä»–ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼è¨­å®šã‚’æŒ‡å®šã™ã‚‹å ´åˆ
    wandb.config.epochs = 2
    wandb.config.log_step = 20
    wandb.config.val_log_step = 50
    wandb.config.architecture_name = "MLP"
    wandb.config.dataset_name = "MNIST"

    # tf.dataã‚’ä½¿ç”¨ã—ã¦å…¥åŠ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    train_dataset = (train_dataset.shuffle(buffer_size=1024)
                                  .batch(wandb.config.batch_size)
                                  .prefetch(buffer_size=tf.data.AUTOTUNE))

    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
    val_dataset = (val_dataset.batch(wandb.config.batch_size)
                              .prefetch(buffer_size=tf.data.AUTOTUNE))

    # ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    model = Model()

    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    optimizer = keras.optimizers.SGD(learning_rate=wandb.config.learning_rate)
    # æå¤±é–¢æ•°ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æº–å‚™
    train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
    val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

    train(train_dataset,
          val_dataset, 
          model,
          optimizer,
          loss_fn,
          train_acc_metric,
          val_acc_metric,
          epochs=wandb.config.epochs, 
          log_step=wandb.config.log_step, 
          val_log_step=wandb.config.val_log_step)
```

# Step 6ï¸âƒ£: Sweepã‚’åˆæœŸåŒ–ã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ


```python
sweep_id = wandb.sweep(sweep_config, project="sweeps-tensorflow")
```

`count`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¦runã®ç·æ•°ã‚’åˆ¶é™ã§ãã¾ã™ã€‚ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æ—©ãå®Ÿè¡Œã™ã‚‹ãŸã‚ã«10ã«åˆ¶é™ã—ã¾ã™ã€‚runã®æ•°ã‚’å¢—ã‚„ã—ã¦çµæœã‚’ç¢ºèªã—ã¦ã¿ã¦ãã ã•ã„ã€‚


```python
wandb.agent(sweep_id, function=sweep_train, count=10)
```

# ğŸ‘€ çµæœã‚’å¯è¦–åŒ–

**Sweep URL**ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€ãƒ©ã‚¤ãƒ–çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

# ğŸ¨ ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã®ä¾‹

W&Bã§è¿½è·¡ãƒ»å¯è¦–åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¾‹ã‚’[ã‚®ãƒ£ãƒ©ãƒªãƒ¼ â†’](https://app.wandb.ai/gallery)ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚

# ğŸ“ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
1. **Projects**: è¤‡æ•°ã®runã‚’ãƒ­ã‚°ã—ã¦æ¯”è¼ƒã™ã‚‹ã€‚`wandb.init(project="project-name")`
2. **Groups**: è¤‡æ•°ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚„äº¤å·®æ¤œè¨¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®å ´åˆã€å„ãƒ—ãƒ­ã‚»ã‚¹ã‚’Runsã¨ã—ã¦ãƒ­ã‚°ã—ã€ä¸€ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«ã¾ã¨ã‚ã‚‹ã€‚`wandb.init(group='experiment-1')`
3. **Tags**: ç¾åœ¨ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚„ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã«ã‚¿ã‚°ã‚’è¿½åŠ ã—ã¾ã™ã€‚
4. **Notes**: ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã§ãƒ¡ãƒ¢ã‚’å…¥åŠ›ã—ã¦ã€runé–“ã®å¤‰æ›´ã‚’è¿½è·¡ã—ã¾ã™ã€‚
5. **Reports**: é€²æ—ã«é–¢ã™ã‚‹ãƒ¡ãƒ¢ã‚’å–ã£ã¦åŒåƒšã¨å…±æœ‰ã—ãŸã‚Šã€MLãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

# ğŸ¤“ é«˜åº¦ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
1. [ç’°å¢ƒå¤‰æ•°](https://docs.wandb.com/library/environment-variables): ç’°å¢ƒå¤‰æ•°ã«APIã‚­ãƒ¼ã‚’è¨­å®šã—ã€ç®¡ç†ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
2. [ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€å¾Œã§çµæœã‚’åŒæœŸã™ã‚‹ãŸã‚ã«`dryrun`ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
3. [ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹](https://docs.wandb.com/self-hosted): ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚¯ãƒ©ã‚¦ãƒ‰ã‚„ã‚¨ã‚¢ã‚®ãƒ£ãƒƒãƒ—ã•ã‚ŒãŸã‚µãƒ¼ãƒãƒ¼ã«W&Bã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚å­¦è¡“æ©Ÿé–¢ã‹ã‚‰ä¼æ¥­ã®Teamsã¾ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚