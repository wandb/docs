


# TensorFlow Sweeps

[**Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§è©¦ã™ â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/tensorflow/Hyperparameter_Optimization_in_TensorFlow_using_W&B_Sweeps.ipynb)

Weights & Biases ã‚’ä½¿ç”¨ã—ã¦ã€æ©Ÿæ¢°å­¦ç¿’å®Ÿé¨“ã®è¿½è·¡ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã¾ã™ã€‚

<img src="http://wandb.me/mini-diagram" width="650" alt="Weights & Biases" />

Weights & Biasesã®Sweepsã‚’ä½¿ç”¨ã—ã¦ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®æœ€é©åŒ–ã‚’è‡ªå‹•åŒ–ã—ã€å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ç©ºé–“ã‚’æ¢ç´¢ã§ãã¾ã™ã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒä»˜å±ã—ã¦ã„ã¾ã™ã€‚

![](https://i.imgur.com/AN0qnpC.png)


## ğŸ¤” ãªãœSweepsã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ

* **ç°¡å˜ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã§W&B sweepsã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚
* **é€æ˜æ€§**: ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã™ã¹ã¦å¼•ç”¨ã—ã¦ãŠã‚Šã€[ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ã™](https://github.com/wandb/client/tree/master/wandb/sweeps)ã€‚
* **å¼·åŠ›**: Sweepsã¯å®Œå…¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãŠã‚ˆã³è¨­å®šå¯èƒ½ã§ã™ã€‚æ•°åå°ã®ãƒã‚·ãƒ³ã§ã‚¹ã‚¤ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚‚ã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã§ã‚¹ã‚¤ãƒ¼ãƒ—ã‚’é–‹å§‹ã™ã‚‹ã“ã¨ã‚‚åŒã˜ãã‚‰ã„ç°¡å˜ã§ã™ã€‚

**[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ $\rightarrow$](https://docs.wandb.com/sweeps)**


## ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãŒã‚«ãƒãƒ¼ã™ã‚‹å†…å®¹

* TensorFlowã§ã®ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨ã—ãŸW&B Sweepã®ç°¡å˜ãªé–‹å§‹æ‰‹é †ã€‚
* ç”»åƒåˆ†é¡ã‚¿ã‚¹ã‚¯ã«æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚

**æ³¨æ„**: _Step_ ã§å§‹ã¾ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚¹ã‚¤ãƒ¼ãƒ—ã‚’è¡Œã†ãŸã‚ã«å¿…è¦ãªã‚‚ã®ã§ã™ã€‚æ®‹ã‚Šã®ã‚³ãƒ¼ãƒ‰ã¯ã‚·ãƒ³ãƒ—ãƒ«ãªä¾‹ã‚’è¨­å®šã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚


# ğŸš€ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€ãƒ­ã‚°ã‚¤ãƒ³

### Step 0ï¸âƒ£: W&Bã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹


```python
%%capture
!pip install wandb
```

### Step 1ï¸âƒ£: W&Bã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹


```python
import tqdm
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.datasets import cifar10

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```


```python
import wandb
from wandb.keras import WandbCallback

wandb.login()
```

> ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒˆ: ã“ã‚ŒãŒåˆã‚ã¦ã®W&Bã®ä½¿ç”¨ã§ã‚ã£ãŸã‚Šã€ã¾ã ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãªã„å ´åˆã€`wandb.login()`ã‚’å®Ÿè¡Œã—ãŸå¾Œã«è¡¨ç¤ºã•ã‚Œã‚‹ãƒªãƒ³ã‚¯ã‹ã‚‰ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—/ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ç§»å‹•ã—ã¾ã™ã€‚ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—ã¯æ•°å›ã®ã‚¯ãƒªãƒƒã‚¯ã ã‘ã§ç°¡å˜ã«ã§ãã¾ã™ã€‚

# ğŸ‘©â€ğŸ³ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™


```python
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

x_train = x_train/255.
x_test = x_test/255.
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))
```

# ğŸ§  ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã®å®šç¾©

## ğŸ—ï¸ ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†é¡å™¨MLPã®æ§‹ç¯‰


```python
def Model():
    inputs = keras.Input(shape=(784,), name="digits")
    x1 = keras.layers.Dense(64, activation="relu")(inputs)
    x2 = keras.layers.Dense(64, activation="relu")(x1)
    outputs = keras.layers.Dense(10, name="predictions")(x2)

    return keras.Model(inputs=inputs, outputs=outputs)

    
def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)

    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))

    train_acc_metric.update_state(y, logits)

    return loss_value

    
def test_step(x, y, model, loss_fn, val_acc_metric):
    val_logits = model(x, training=False)
    loss_value = loss_fn(y, val_logits)
    val_acc_metric.update_state(y, val_logits)

    return loss_value
```

## ğŸ” ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã®ä½œæˆ

### Step 3ï¸âƒ£: `wandb.log` ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã™ã‚‹


```python
def train(train_dataset,
          val_dataset, 
          model,
          optimizer,
          loss_fn,
          train_acc_metric,
          val_acc_metric,
          epochs=10, 
          log_step=200, 
          val_log_step=50):
  
    for epoch in range(epochs):
        print("\nStart of epoch %d" % (epoch,))

        train_loss = []   
        val_loss = []

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒƒãƒã‚’ç¹°ã‚Šè¿”ã—å‡¦ç†ã™ã‚‹
        for step, (x_batch_train, y_batch_train) in tqdm.tqdm(enumerate(train_dataset), total=len(train_dataset)):
            loss_value = train_step(x_batch_train, y_batch_train, 
                                    model, optimizer, 
                                    loss_fn, train_acc_metric)
            train_loss.append(float(loss_value))

        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã™ã‚‹
        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):
            val_loss_value = test_step(x_batch_val, y_batch_val, 
                                       model, loss_fn, 
                                       val_acc_metric)
            val_loss.append(float(val_loss_value))
            
        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤ºã™ã‚‹
        train_acc = train_acc_metric.result()
        print("Training acc over epoch: %.4f" % (float(train_acc),))

        val_acc = val_acc_metric.result()
        print("Validation acc: %.4f" % (float(val_acc),))

        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹
        train_acc_metric.reset_states()
        val_acc_metric.reset_states()

        # 3ï¸âƒ£ `wandb.log` ã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã™ã‚‹
        wandb.log({'epochs': epoch,
                   'loss': np.mean(train_loss),
                   'acc': float(train_acc), 
                   'val_loss': np.mean(val_loss),
                   'val_acc':float(val_acc)})
```

# Step 4ï¸âƒ£: Sweepã®è¨­å®š

ã“ã“ã§ã¯æ¬¡ã®ã“ã¨ã‚’è¡Œã„ã¾ã™:
* ã‚¹ã‚¤ãƒ¼ãƒ—ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®å®šç¾©
* ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–ã®ãƒ¡ã‚½ãƒƒãƒ‰ã®æŒ‡å®šã€‚`random`ã€`grid`ã€`bayes`ãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚
* `bayes`ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ç›®çš„ã¨`metric`ã®æŒ‡å®šã€ä¾‹ãˆã°`val_loss`ã‚’`minimize`ã™ã‚‹ã€‚
* ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ã„runã‚’æ—©æœŸçµ‚äº†ã™ã‚‹ãŸã‚ã«`hyperband`ã‚’ä½¿ç”¨

#### [Sweepã®è¨­å®šã«ã¤ã„ã¦ã‚‚ã£ã¨è¦‹ã‚‹ $\rightarrow$](https://docs.wandb.com/sweeps/configuration)


```python
sweep_config = {
  'method': 'random', 
  'metric': {
      'name': 'val_loss',
      'goal': 'minimize'
  },
  'early_terminate':{
      'type': 'hyperband',
      'min_iter': 5
  },
  'parameters': {
      'batch_size': {
          'values': [32, 64, 128, 256]
      },
      'learning_rate':{
          'values': [0.01, 0.005, 0.001, 0.0005, 0.0001]
      }
  }
}
```

# Step 5ï¸âƒ£: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ãƒ©ãƒƒãƒ—ã™ã‚‹

ä»¥ä¸‹ã®ã‚ˆã†ãªé–¢æ•°`sweep_train`ãŒå¿…è¦ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€`train`ãŒå‘¼ã³å‡ºã•ã‚Œã‚‹å‰ã«`wandb.config`ã‚’ä½¿ç”¨ã—ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãŒè¨­å®šã•ã‚Œã¾ã™ã€‚


```python
def sweep_train(config_defaults=None):
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
    config_defaults = {
        "batch_size": 64,
        "learning_rate": 0.01
    }
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã§wandbã‚’åˆæœŸåŒ–
    wandb.init(config=config_defaults)  # ã“ã‚Œã¯Sweepã§ä¸Šæ›¸ãã•ã‚Œã¾ã™

    # ãã®ä»–ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’è¨­å®šã«æŒ‡å®š
    wandb.config.epochs = 2
    wandb.config.log_step = 20
    wandb.config.val_log_step = 50
    wandb.config.architecture_name = "MLP"
    wandb.config.dataset_name = "MNIST"

    # tf.dataã‚’ä½¿ç”¨ã—ã¦å…¥åŠ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    train_dataset = (train_dataset.shuffle(buffer_size=1024)
                                  .batch(wandb.config.batch_size)
                                  .prefetch(buffer_size=tf.data.AUTOTUNE))

    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
    val_dataset = (val_dataset.batch(wandb.config.batch_size)
                              .prefetch(buffer_size=tf.data.AUTOTUNE))

    # ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    model = Model()

    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    optimizer = keras.optimizers.SGD(learning_rate=wandb.config.learning_rate)
    # ãƒ­ã‚¹é–¢æ•°ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æº–å‚™
    train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
    val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

    train(train_dataset,
          val_dataset, 
          model,
          optimizer,
          loss_fn,
          train_acc_metric,
          val_acc_metric,
          epochs=wandb.config.epochs, 
          log_step=wandb.config.log_step, 
          val_log_step=wandb.config.val_log_step)
```

# Step 6ï¸âƒ£: Sweepã‚’åˆæœŸåŒ–ã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ


```python
sweep_id = wandb.sweep(sweep_config, project="sweeps-tensorflow")
```

`count`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã§runã®åˆè¨ˆæ•°ã‚’åˆ¶é™ã§ãã¾ã™ã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã™ã°ã‚„ãå®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã€ã“ã“ã§ã¯10ã«åˆ¶é™ã—ã¦ã„ã¾ã™ãŒã€runã®æ•°ã‚’å¢—ã‚„ã—ã¦ä½•ãŒèµ·ã“ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚


```python
wandb.agent(sweep_id, function=sweep_train, count=10)
```

# ğŸ‘€ çµæœã®å¯è¦–åŒ–

ä¸Šè¨˜ã®**Sweep URL**ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€ãƒ©ã‚¤ãƒ–çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚


# ğŸ¨ ä¾‹ã‚®ãƒ£ãƒ©ãƒªãƒ¼

W&Bã‚’ä½¿ç”¨ã—ã¦è¿½è·¡ãŠã‚ˆã³å¯è¦–åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¾‹ã‚’[ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã§è¦‹ã‚‹ â†’](https://app.wandb.ai/gallery)

# ğŸ“ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
1. **Projects**: è¤‡æ•°ã®runã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãƒ­ã‚°ã—ã¦æ¯”è¼ƒã—ã¾ã™ã€‚`wandb.init(project="project-name")`
2. **Groups**: è¤‡æ•°ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚„äº¤å·®æ¤œè¨¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®ãŸã‚ã«ã€å„ãƒ—ãƒ­ã‚»ã‚¹ã‚’runã¨ã—ã¦ãƒ­ã‚°ã—ã€ãã‚Œã‚‰ã‚’ã‚°ãƒ«ãƒ¼ãƒ—ã«ã¾ã¨ã‚ã¾ã™ã€‚`wandb.init(group='experiment-1')`
3. **Tags**: ç¾åœ¨ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚„ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã«ã‚¿ã‚°ã‚’è¿½åŠ ã—ã¾ã™ã€‚
4. **Notes**: èµ°è¡Œé–“ã®å¤‰æ›´ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã«ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ¡ãƒ¢ã‚’å…¥åŠ›ã—ã¾ã™ã€‚
5. **Reports**: é€²æ—çŠ¶æ³ã«é–¢ã™ã‚‹ã‚¯ã‚¤ãƒƒã‚¯ãƒ¡ãƒ¢ã‚’åŒåƒšã¨å…±æœ‰ã—ã€MLãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

# ğŸ¤“ é«˜åº¦ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
1. [ç’°å¢ƒå¤‰æ•°](https://docs.wandb.com/library/environment-variables): ç®¡ç†ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸Šã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ç’°å¢ƒå¤‰æ•°ã«APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¾ã™ã€‚
2. [ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): `dryrun`ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã€å¾Œã§çµæœã‚’åŒæœŸã—ã¾ã™ã€‚
3. [ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹](https://docs.wandb.com/self-hosted): ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚¯ãƒ©ã‚¦ãƒ‰ã¾ãŸã¯ã‚¨ã‚¢ã‚®ãƒ£ãƒƒãƒ—ã‚µãƒ¼ãƒãƒ¼ã«W&Bã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚å­¦è¡“æ©Ÿé–¢ã‹ã‚‰ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒãƒ¼ãƒ ã¾ã§ã€ã™ã¹ã¦ã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚