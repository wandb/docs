---
title: TensorFlow Sweeps
---
import { CTAButtons } from '@site/src/components/CTAButtons/CTAButtons.tsx'

<CTAButtons colabLink='https://colab.research.google.com/github/wandb/examples/blob/master/colabs/tensorflow/Hyperparameter_Optimization_in_TensorFlow_using_W&B_Sweeps.ipynb'/>

Weights & Biasesë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ê³„í•™ìŠµ ì‹¤í—˜ ì¶”ì , ë°ì´í„°ì…‹ ë²„ì „ ê´€ë¦¬ ë° í”„ë¡œì íŠ¸ í˜‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

![](/images/tutorials/huggingface-why.png)

Weights & Biasesì˜ Sweepsë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìë™í™”í•˜ê³  ê°€ëŠ¥í•œ ëª¨ë¸ì˜ ê³µê°„ì„ íƒìƒ‰í•˜ì„¸ìš”. ì´ì™€ ê°™ì€ ì¸í„°ë™í‹°ë¸Œí•œ ëŒ€ì‹œë³´ë“œë„ ì œê³µí•©ë‹ˆë‹¤:

![](/images/tutorials/tensorflow/sweeps.png)

## ğŸ¤” Sweepsë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?

* **ë¹ ë¥¸ ì„¤ì •**: ëª‡ ì¤„ì˜ ì½”ë“œë§Œìœ¼ë¡œ W&B sweepsë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **íˆ¬ëª…ì„±**: ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì„ ëª…ì‹œí•˜ê³  ìˆìœ¼ë©°, [ìš°ë¦¬ì˜ ì½”ë“œëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ì…ë‹ˆë‹¤](https://github.com/wandb/client/tree/master/wandb/sweeps).
* **ê°•ë ¥í•¨**: ìš°ë¦¬ì˜ sweepsëŠ” ì™„ì „íˆ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ ê°€ëŠ¥í•˜ê³  ì„¤ì • ê°€ëŠ¥í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ëŒ€ì˜ ê¸°ê³„ì—ì„œ ìŠ¤ìœ•ì„ ì‹œì‘í•´ë„ ê°œì¸ ë…¸íŠ¸ë¶ì—ì„œ í•˜ëŠ” ê²ƒë§Œí¼ ì‰½ìŠµë‹ˆë‹¤.

**[ê³µì‹ ë¬¸ì„œ ë³´ê¸°](/guides/sweeps)**

## ì´ ë…¸íŠ¸ë¶ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©

* TensorFlowì—ì„œ ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë‹ ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ W&B Sweepì„ ì‹œì‘í•˜ëŠ” ê°„ë‹¨í•œ ë‹¨ê³„ë“¤.
* ìš°ë¦¬ì˜ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…ì„ ìœ„í•œ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

**ì°¸ê³ **: _Step_ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì„¹ì…˜ë“¤ì€ ê¸°ì¡´ ì½”ë“œì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ê²ƒì…ë‹ˆë‹¤.
ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë‹¨ìˆœí•œ ì˜ˆì œë¥¼ êµ¬ì„±í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.

## ğŸš€ ì„¤ì¹˜, ê°€ì ¸ì˜¤ê¸° ë° ë¡œê·¸ì¸

### Step 0ï¸âƒ£: W&B ì„¤ì¹˜

```python
%%capture
!pip install wandb
```

### Step 1ï¸âƒ£: W&B ê°€ì ¸ì˜¤ê¸° ë° ë¡œê·¸ì¸

```python
import tqdm
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.datasets import cifar10

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

```python
import wandb
from wandb.integration.keras import WandbMetricsLogger

wandb.login()
```

> ë³´ì¶© ì„¤ëª…: W&Bë¥¼ ì²˜ìŒ ì‚¬ìš©í•˜ì‹œê±°ë‚˜ ë¡œê·¸ì¸ë˜ì§€ ì•Šì€ ê²½ìš° `wandb.login()`ì„ ì‹¤í–‰í•œ í›„ ë‚˜íƒ€ë‚˜ëŠ” ë§í¬ê°€ íšŒì›ê°€ì…/ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™ì‹œì¼œ ì¤ë‹ˆë‹¤. íšŒì›ê°€ì…ì€ ëª‡ ë²ˆì˜ í´ë¦­ë§Œìœ¼ë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ‘©â€ğŸ³ ë°ì´í„°ì…‹ ì¤€ë¹„

```python
# íŠ¸ë ˆì´ë‹ ë°ì´í„°ì…‹ ì¤€ë¹„
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

x_train = x_train/255.
x_test = x_test/255.
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))
```

## ğŸ§  ëª¨ë¸ ë° íŠ¸ë ˆì´ë‹ ë£¨í”„ ì •ì˜í•˜ê¸°

## ğŸ—ï¸ ê°„ë‹¨í•œ ë¶„ë¥˜ MLP êµ¬ì¶•í•˜ê¸°

```python
def Model():
    inputs = keras.Input(shape=(784,), name="digits")
    x1 = keras.layers.Dense(64, activation="relu")(inputs)
    x2 = keras.layers.Dense(64, activation="relu")(x1)
    outputs = keras.layers.Dense(10, name="predictions")(x2)

    return keras.Model(inputs=inputs, outputs=outputs)

    
def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)

    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))

    train_acc_metric.update_state(y, logits)

    return loss_value

    
def test_step(x, y, model, loss_fn, val_acc_metric):
    val_logits = model(x, training=False)
    loss_value = loss_fn(y, val_logits)
    val_acc_metric.update_state(y, val_logits)

    return loss_value
```

## ğŸ” íŠ¸ë ˆì´ë‹ ë£¨í”„ ì‘ì„±í•˜ê¸°

### Step 3ï¸âƒ£: `wandb.log`ë¡œ ë©”íŠ¸ë¦­ ê¸°ë¡í•˜ê¸°

```python
def train(train_dataset,
          val_dataset, 
          model,
          optimizer,
          loss_fn,
          train_acc_metric,
          val_acc_metric,
          epochs=10, 
          log_step=200, 
          val_log_step=50):
  
    for epoch in range(epochs):
        print("\nStart of epoch %d" % (epoch,))

        train_loss = []   
        val_loss = []

        # ë°ì´í„°ì…‹ì˜ ë°°ì¹˜ë¥¼ ë°˜ë³µ ì²˜ë¦¬
        for step, (x_batch_train, y_batch_train) in tqdm.tqdm(enumerate(train_dataset), total=len(train_dataset)):
            loss_value = train_step(x_batch_train, y_batch_train, 
                                    model, optimizer, 
                                    loss_fn, train_acc_metric)
            train_loss.append(float(loss_value))

        # ì—í¬í¬ ì¢…ë£Œ ì‹œ ê²€ì¦ ë£¨í”„ ì‹¤í–‰
        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):
            val_loss_value = test_step(x_batch_val, y_batch_val, 
                                       model, loss_fn, 
                                       val_acc_metric)
            val_loss.append(float(val_loss_value))
            
        # ì—í¬í¬ ì¢…ë£Œ ì‹œ ë©”íŠ¸ë¦­ í‘œì‹œ
        train_acc = train_acc_metric.result()
        print("Training acc over epoch: %.4f" % (float(train_acc),))

        val_acc = val_acc_metric.result()
        print("Validation acc: %.4f" % (float(val_acc),))

        # ì—í¬í¬ ì¢…ë£Œ ì‹œ ë©”íŠ¸ë¦­ ë¦¬ì…‹
        train_acc_metric.reset_states()
        val_acc_metric.reset_states()

        # 3ï¸âƒ£ wandb.logë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”íŠ¸ë¦­ ê¸°ë¡
        wandb.log({'epochs': epoch,
                   'loss': np.mean(train_loss),
                   'acc': float(train_acc), 
                   'val_loss': np.mean(val_loss),
                   'val_acc':float(val_acc)})
```

### Step 4ï¸âƒ£: Sweep ì„¤ì •í•˜ê¸°

ì—¬ê¸°ì„œëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:
* íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
* í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë©”ì†Œë“œ ì œê³µ. `random`, `grid`, `bayes` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* `bayes`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° `metric`ê³¼ `ëª©í‘œ`ë¥¼ ì œê³µí•˜ì—¬, ì˜ˆë¥¼ ë“¤ì–´ `val_loss`ë¥¼ `minimize`í•˜ë„ë¡ ì„¤ì •.
* ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì€ ì‹¤í–‰ì˜ ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•´ `hyperband` ì‚¬ìš©

#### [ìì„¸í•œ Sweep ì„¤ì • ì •ë³´ ë³´ê¸°](/guides/sweeps/define-sweep-configuration)

```python
sweep_config = {
  'method': 'random', 
  'metric': {
      'name': 'val_loss',
      'goal': 'minimize'
  },
  'early_terminate':{
      'type': 'hyperband',
      'min_iter': 5
  },
  'parameters': {
      'batch_size': {
          'values': [32, 64, 128, 256]
      },
      'learning_rate':{
          'values': [0.01, 0.005, 0.001, 0.0005, 0.0001]
      }
  }
}
```

### Step 5ï¸âƒ£: íŠ¸ë ˆì´ë‹ ë£¨í”„ ê°ì‹¸ê¸°

`train`ì´ í˜¸ì¶œë˜ê¸° ì „ì— í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ê¸° ìœ„í•´ `wandb.config`ë¥¼ ì‚¬ìš©í•˜ëŠ”, ì•„ë˜ì˜ `sweep_train`ê³¼ ê°™ì€ í•¨ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.

```python
def sweep_train(config_defaults=None):
    # ê¸°ë³¸ ê°’ ì„¤ì •
    config_defaults = {
        "batch_size": 64,
        "learning_rate": 0.01
    }
    # ì˜ˆì œ í”„ë¡œì íŠ¸ ì´ë¦„ìœ¼ë¡œ wandb ì´ˆê¸°í™”
    wandb.init(config=config_defaults)  # ìŠ¤ìœ•ì—ì„œ ë®ì–´ì”ë‹ˆë‹¤.

    # ê¸°íƒ€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •ì— ëª…ì‹œ
    wandb.config.epochs = 2
    wandb.config.log_step = 20
    wandb.config.val_log_step = 50
    wandb.config.architecture_name = "MLP"
    wandb.config.dataset_name = "MNIST"

    # tf.dataë¥¼ ì‚¬ìš©í•´ ì…ë ¥ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    train_dataset = (train_dataset.shuffle(buffer_size=1024)
                                  .batch(wandb.config.batch_size)
                                  .prefetch(buffer_size=tf.data.AUTOTUNE))

    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
    val_dataset = (val_dataset.batch(wandb.config.batch_size)
                              .prefetch(buffer_size=tf.data.AUTOTUNE))

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = Model()

    # ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹ í•  ì˜µí‹°ë§ˆì´ì € ì¸ìŠ¤í„´ìŠ¤í™”
    optimizer = keras.optimizers.SGD(learning_rate=wandb.config.learning_rate)
    # ì†ì‹¤ í•¨ìˆ˜ ì¸ìŠ¤í„´ìŠ¤í™”
    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    # ë©”íŠ¸ë¦­ ì¤€ë¹„.
    train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
    val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

    train(train_dataset,
          val_dataset, 
          model,
          optimizer,
          loss_fn,
          train_acc_metric,
          val_acc_metric,
          epochs=wandb.config.epochs, 
          log_step=wandb.config.log_step, 
          val_log_step=wandb.config.val_log_step)
```

### Step 6ï¸âƒ£: Sweep ì´ˆê¸°í™” ë° ì—ì´ì „íŠ¸ ì‹¤í–‰

```python
sweep_id = wandb.sweep(sweep_config, project="sweeps-tensorflow")
```

`count` íŒŒë¼ë¯¸í„°ë¡œ ì´ ì‹¤í–‰ íšŸìˆ˜ë¥¼ ì œí•œí•  ìˆ˜ ìˆìœ¼ë©°, ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¹ ë¥´ê²Œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ 10ìœ¼ë¡œ ì œí•œí•©ë‹ˆë‹¤. ì‹¤í–‰ íšŸìˆ˜ë¥¼ ëŠ˜ë ¤ë³´ê³  ì–´ë–¤ ë³€í™”ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

```python
wandb.agent(sweep_id, function=sweep_train, count=10)
```

## ğŸ‘€ ê²°ê³¼ ì‹œê°í™”

ìœ„ì˜ **Sweep URL** ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ ë¼ì´ë¸Œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.

## ğŸ¨ ì˜ˆì œ ê°¤ëŸ¬ë¦¬

W&Bë¡œ ì¶”ì í•˜ê³  ì‹œê°í™”ëœ í”„ë¡œì íŠ¸ì˜ ì˜ˆì œë¥¼ [ê°¤ëŸ¬ë¦¬ì—ì„œ â†’](https://app.wandb.ai/gallery) í™•ì¸í•˜ì„¸ìš”.

## ğŸ“ ëª¨ë²” ì‚¬ë¡€
1. **Projects**: ì—¬ëŸ¬ runsë¥¼ ë¡œê·¸í•˜ì—¬ í”„ë¡œì íŠ¸ì—ì„œ ë¹„êµí•˜ì„¸ìš”. `wandb.init(project="project-name")`
2. **Groups**: ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ë‚˜ êµì°¨ê²€ì¦ì„ ìœ„í•´, ê°ê°ì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ runìœ¼ë¡œ ë¡œê·¸í•˜ê³  ê·¸ë£¹í™”í•˜ì„¸ìš”. `wandb.init(group='experiment-1')`
3. **Tags**: í˜„ì¬ ë² ì´ìŠ¤ë¼ì¸ì´ë‚˜ í”„ë¡œë•ì…˜ ëª¨ë¸ì„ ì¶”ì í•˜ê¸° ìœ„í•´ íƒœê·¸ ì¶”ê°€.
4. **Notes**: runs ê°„ì˜ ë³€ê²½ ì‚¬í•­ì„ ì¶”ì í•˜ê¸° ìœ„í•´ í…Œì´ë¸”ì— ë…¸íŠ¸ ì‘ì„±.
5. **Reports**: ë™ë£Œì™€ ê³µìœ í•˜ê¸° ìœ„í•´ ì§„í–‰ ìƒí™©ì— ëŒ€í•œ ë¹ ë¥¸ ë…¸íŠ¸ë¥¼ ì‘ì„±í•˜ê³  ëŒ€ì‹œë³´ë“œ ë° ML í”„ë¡œì íŠ¸ì˜ ìŠ¤ëƒ…ìƒ· ìƒì„±.

## ğŸ¤“ ê³ ê¸‰ ì„¤ì •
1. [í™˜ê²½ ë³€ìˆ˜](/guides/hosting/env-vars): ê´€ë¦¬ë˜ëŠ” í´ëŸ¬ìŠ¤í„°ì—ì„œ íŠ¸ë ˆì´ë‹ì„ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •.
2. [ì˜¤í”„ë¼ì¸ ëª¨ë“œ](/guides/technical-faq/setup/#can-i-run-wandb-offline)
3. [ì˜¨í”„ë ˆë¯¸ìŠ¤](/guides/hosting/hosting-options/self-managed): í”„ë¼ì´ë¹— í´ë¼ìš°ë“œ ë˜ëŠ” ìì²´ ì¸í”„ë¼ì˜ ì—ì–´ê°­ ì„œë²„ì— W&B ì„¤ì¹˜. í•™ê³„ë¶€í„° ê¸°ì—… íŒ€ê¹Œì§€ ëª¨ë“  ì‚¬ëŒì„ ìœ„í•œ ë¡œì»¬ ì„¤ì¹˜ ì œê³µ.