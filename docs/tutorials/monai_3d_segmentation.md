---
title: 3D brain tumor segmentation with MONAI
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import { CTAButtons } from '@site/src/components/CTAButtons/CTAButtons.tsx';

<CTAButtons colabLink="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/monai/3d_brain_tumor_segmentation.ipynb"></CTAButtons>

ì´ íŠœí† ë¦¬ì–¼ì€ [MONAI](https://github.com/Project-MONAI/MONAI)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì¤‘ ë ˆì´ë¸” 3D ë‡Œì¢…ì–‘ ë¶„í•  ì‘ì—…ì˜ íŠ¸ë ˆì´ë‹ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•˜ê³  [Weights & Biases](https://wandb.ai/site)ì˜ ì‹¤í—˜ ì¶”ì  ë° ë°ì´í„° ì‹œê°í™” ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

1. Weights & Biases runì„ ì´ˆê¸°í™”í•˜ê³  ì¬í˜„ì„±ì„ ìœ„í•´ runê³¼ ì—°ê´€ëœ ëª¨ë“  ì„¤ì •ì„ ë™ê¸°í™”í•©ë‹ˆë‹¤.
2. MONAI transform API:
    1. ì‚¬ì „ í˜•ì‹ ë°ì´í„°ì— ëŒ€í•œ MONAI Transforms.
    2. MONAI `transforms` APIì— ë§ì¶° ìƒˆë¡œìš´ transformì„ ì •ì˜í•˜ëŠ” ë°©ë²•.
    3. ë°ì´í„° ì¦ê°•ì„ ìœ„í•´ ê°•ë„ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¡°ì •í•˜ëŠ” ë°©ë²•.
3. ë°ì´í„° ë¡œë”© ë° ì‹œê°í™”:
    1. ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ `Nifti` ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ê³  ì´ë¯¸ì§€ë¥¼ ëª©ë¡ìœ¼ë¡œ ë¡œë“œí•˜ì—¬ ìŠ¤íƒí•©ë‹ˆë‹¤.
    2. íŠ¸ë ˆì´ë‹ ë° ê²€ì¦ì„ ê°€ì†í™”í•˜ê¸° ìœ„í•´ IO ë° ë³€í™˜ì„ ìºì‹œí•©ë‹ˆë‹¤.
    3. `wandb.Table`ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ê³  Weights & Biasesì—ì„œ ìƒí˜¸ì‘ìš© ë¶„í•  ì˜¤ë²„ë ˆì´ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
4. 3D `SegResNet` ëª¨ë¸ íŠ¸ë ˆì´ë‹
    1. MONAIì˜ `networks`, `losses`, `metrics` API ì‚¬ìš©.
    2. PyTorch íŠ¸ë ˆì´ë‹ ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ 3D `SegResNet` ëª¨ë¸ íŠ¸ë ˆì´ë‹.
    3. Weights & Biasesë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ë ˆì´ë‹ ì‹¤í—˜ì„ ì¶”ì í•©ë‹ˆë‹¤.
    4. Weights & Biasesì—ì„œ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë¡œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œê·¸í•˜ê³  ë²„ì „ ê´€ë¦¬í•©ë‹ˆë‹¤.
5. `wandb.Table`ì„ ì‚¬ìš©í•˜ê³  Weights & Biasesì—ì„œ ìƒí˜¸ì‘ìš© ë¶„í•  ì˜¤ë²„ë ˆì´ë¥¼ í†µí•´ ê²€ì¦ ë°ì´í„°ì…‹ì˜ ì˜ˆì¸¡ì„ ì‹œê°í™”í•˜ê³  ë¹„êµí•©ë‹ˆë‹¤.

## ğŸŒ´ ì„¤ì • ë° ì„¤ì¹˜

ë¨¼ì € MONAIì™€ Weights & Biasesì˜ ìµœì‹  ë²„ì „ì„ ì„¤ì¹˜í•˜ì„¸ìš”.

```python
!python -c "import monai" || pip install -q -U "monai[nibabel, tqdm]"
!python -c "import wandb" || pip install -q -U wandb
```

```python
import os

import numpy as np
from tqdm.auto import tqdm
import wandb

from monai.apps import DecathlonDataset
from monai.data import DataLoader, decollate_batch
from monai.losses import DiceLoss
from monai.inferers import sliding_window_inference
from monai.metrics import DiceMetric
from monai.networks.nets import SegResNet
from monai.transforms import (
    Activations,
    AsDiscrete,
    Compose,
    LoadImaged,
    MapTransform,
    NormalizeIntensityd,
    Orientationd,
    RandFlipd,
    RandScaleIntensityd,
    RandShiftIntensityd,
    RandSpatialCropd,
    Spacingd,
    EnsureTyped,
    EnsureChannelFirstd,
)
from monai.utils import set_determinism

import torch
```

ê·¸ëŸ° ë‹¤ìŒ, W&Bë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ Colab ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¸ì¦í•˜ì„¸ìš”.

```python
wandb.login()
```

## ğŸŒ³ W&B Run ì´ˆê¸°í™”

ì‹¤í—˜ ì¶”ì ì„ ì‹œì‘í•˜ë ¤ë©´ ìƒˆë¡œìš´ W&B runì„ ì‹œì‘í•˜ì„¸ìš”.

```python
wandb.init(project="monai-brain-tumor-segmentation")
```

ì ì ˆí•œ êµ¬ì„± ì‹œìŠ¤í…œì˜ ì‚¬ìš©ì€ ì¬í˜„ ê°€ëŠ¥í•œ ê¸°ê³„í•™ìŠµì„ ìœ„í•œ ê¶Œì¥ ìš°ìˆ˜ ì‚¬ë¡€ì…ë‹ˆë‹¤. W&Bë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì‹¤í—˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
config = wandb.config
config.seed = 0
config.roi_size = [224, 224, 144]
config.batch_size = 1
config.num_workers = 4
config.max_train_images_visualized = 20
config.max_val_images_visualized = 20
config.dice_loss_smoothen_numerator = 0
config.dice_loss_smoothen_denominator = 1e-5
config.dice_loss_squared_prediction = True
config.dice_loss_target_onehot = False
config.dice_loss_apply_sigmoid = True
config.initial_learning_rate = 1e-4
config.weight_decay = 1e-5
config.max_train_epochs = 50
config.validation_intervals = 1
config.dataset_dir = "./dataset/"
config.checkpoint_dir = "./checkpoints"
config.inference_roi_size = (128, 128, 64)
config.max_prediction_images_visualized = 20
```

ëª¨ë“ˆì˜ ëœë¤ ì‹œë“œë¥¼ ì„¤ì •í•´ì•¼ í•˜ë©°, ì´ëŠ” ê²°ì •ë¡ ì  íŠ¸ë ˆì´ë‹ì„ í™œì„±í™” ë˜ëŠ” ë¹„í™œì„±í™”í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

```python
set_determinism(seed=config.seed)

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(config.dataset_dir, exist_ok=True)
os.makedirs(config.checkpoint_dir, exist_ok=True)
```

## ğŸ’¿ ë°ì´í„° ë¡œë”© ë° ë³€í™˜

ì—¬ê¸°ì—ì„œëŠ” `monai.transforms` APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„í•  ì‘ì—…ì˜ ì›í•« í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì»¤ìŠ¤í…€ ë³€í™˜ì„ ìƒì„±í•©ë‹ˆë‹¤.

```python
class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):
    """
    brats í´ë˜ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë ˆì´ë¸”ì„ ë‹¤ì¤‘ ì±„ë„ë¡œ ë³€í™˜:
    ë ˆì´ë¸” 1ì€ ì¢…ì–‘ ì£¼ìœ„ ë¶€ì¢…
    ë ˆì´ë¸” 2ëŠ” GD-ì¦ê°• ì¢…ì–‘
    ë ˆì´ë¸” 3ì€ ê´´ì‚¬ì„± ë° ë¹„ì¦ê°• ì¢…ì–‘ ì¤‘ì‹¬
    ê°€ëŠ¥í•œ í´ë˜ìŠ¤ëŠ” TC(ì¢…ì–‘ ì¤‘ì‹¬), WT(ì „ì²´ ì¢…ì–‘) ë° ET(ì¦ê°• ì¢…ì–‘)ì…ë‹ˆë‹¤.

    ì°¸ê³ : https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb

    """

    def __call__(self, data):
        d = dict(data)
        for key in self.keys:
            result = []
            # ë ˆì´ë¸” 2ì™€ ë ˆì´ë¸” 3ì„ ë³‘í•©í•˜ì—¬ TCë¥¼ êµ¬ì„±
            result.append(torch.logical_or(d[key] == 2, d[key] == 3))
            # ë ˆì´ë¸” 1, 2, 3ì„ ë³‘í•©í•˜ì—¬ WTë¥¼ êµ¬ì„±
            result.append(
                torch.logical_or(
                    torch.logical_or(d[key] == 2, d[key] == 3), d[key] == 1
                )
            )
            # ë ˆì´ë¸” 2ëŠ” ETì…ë‹ˆë‹¤.
            result.append(d[key] == 2)
            d[key] = torch.stack(result, axis=0).float()
        return d
```

ë‹¤ìŒìœ¼ë¡œ íŠ¸ë ˆì´ë‹ ë° ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ ë³€í™˜ì„ ê°ê° ì„¤ì •í•˜ì„¸ìš”.

```python
train_transform = Compose(
    [
        # 4ê°œì˜ Nifti ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ê³  í•¨ê»˜ ìŠ¤íƒ
        LoadImaged(keys=["image", "label"]),
        EnsureChannelFirstd(keys="image"),
        EnsureTyped(keys=["image", "label"]),
        ConvertToMultiChannelBasedOnBratsClassesd(keys="label"),
        Orientationd(keys=["image", "label"], axcodes="RAS"),
        Spacingd(
            keys=["image", "label"],
            pixdim=(1.0, 1.0, 1.0),
            mode=("bilinear", "nearest"),
        ),
        RandSpatialCropd(
            keys=["image", "label"], roi_size=config.roi_size, random_size=False
        ),
        RandFlipd(keys=["image", "label"], prob=0.5, spatial_axis=0),
        RandFlipd(keys=["image", "label"], prob=0.5, spatial_axis=1),
        RandFlipd(keys=["image", "label"], prob=0.5, spatial_axis=2),
        NormalizeIntensityd(keys="image", nonzero=True, channel_wise=True),
        RandScaleIntensityd(keys="image", factors=0.1, prob=1.0),
        RandShiftIntensityd(keys="image", offsets=0.1, prob=1.0),
    ]
)
val_transform = Compose(
    [
        LoadImaged(keys=["image", "label"]),
        EnsureChannelFirstd(keys="image"),
        EnsureTyped(keys=["image", "label"]),
        ConvertToMultiChannelBasedOnBratsClassesd(keys="label"),
        Orientationd(keys=["image", "label"], axcodes="RAS"),
        Spacingd(
            keys=["image", "label"],
            pixdim=(1.0, 1.0, 1.0),
            mode=("bilinear", "nearest"),
        ),
        NormalizeIntensityd(keys="image", nonzero=True, channel_wise=True),
    ]
)
```

### ğŸ ë°ì´í„°ì…‹

ì´ ì‹¤í—˜ì— ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì€ http://medicaldecathlon.com/ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤. ë‹¤ì¤‘ ëª¨ë‹¬ ë‹¤ì¤‘ ì‚¬ì´íŠ¸ MRI ë°ì´í„° (FLAIR, T1w, T1gd, T2w)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸€ë¦¬ì˜¤ë§ˆ, ê´´ì‚¬ì„±/í™œì„± ì¢…ì–‘ ë° ë¶€ì¢…ì„ ë¶„í• í•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ 750ê°œì˜ 4D ë³¼ë¥¨ (484 íŠ¸ë ˆì´ë‹ + 266 í…ŒìŠ¤íŠ¸)ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

`DecathlonDataset`ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì„ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ê³  ì¶”ì¶œí•©ë‹ˆë‹¤. MONAI `CacheDataset`ì„ ìƒì†ë°›ì•„, ë©”ëª¨ë¦¬ í¬ê¸°ì— ë”°ë¼ íŠ¸ë ˆì´ë‹ì„ ìœ„í•´ `cache_num=N`ìœ¼ë¡œ `N`ê°œ í•­ëª©ì„ ìºì‹œí•˜ê±°ë‚˜ ê¸°ë³¸ ì¸ìˆ˜ë¡œ ê²€ì¦ì„ ìœ„í•´ ëª¨ë“  í•­ëª©ì„ ìºì‹œí•˜ë„ë¡ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
train_dataset = DecathlonDataset(
    root_dir=config.dataset_dir,
    task="Task01_BrainTumour",
    transform=val_transform,
    section="training",
    download=True,
    cache_rate=0.0,
    num_workers=4,
)
val_dataset = DecathlonDataset(
    root_dir=config.dataset_dir,
    task="Task01_BrainTumour",
    transform=val_transform,
    section="validation",
    download=False,
    cache_rate=0.0,
    num_workers=4,
)
```

:::info
**ì°¸ê³ :** `train_dataset`ì— `train_transform`ì„ ì ìš©í•˜ëŠ” ëŒ€ì‹ , ë°ì´í„°ì…‹ì˜ ë‘ ë¶„í• ë¡œë¶€í„° ìƒ˜í”Œì„ ì‹œê°í™”í•˜ê¸° ì „ì— `val_transform`ì„ íŠ¸ë ˆì´ë‹ ë° ê²€ì¦ ë°ì´í„°ì…‹ ëª¨ë‘ì— ì ìš©í•˜ì„¸ìš”.
:::

### ğŸ“¸ ë°ì´í„°ì…‹ ì‹œê°í™”

Weights & BiasesëŠ” ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤ ë“±ì„ ì§€ì›í•©ë‹ˆë‹¤. ê²°ê³¼ë¥¼ íƒìƒ‰í•˜ê³  ìš°ë¦¬ì˜ runs, models, datasetsë¥¼ ì‹œê°ì ìœ¼ë¡œ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ ë¦¬ì¹˜ ë¯¸ë””ì–´ë¥¼ ë¡œê·¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [ë¶„í•  ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ ì‹œìŠ¤í…œ](/guides/track/log/media#image-overlays-in-tables)ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë³¼ë¥¨ì„ ì‹œê°í™”í•˜ì„¸ìš”. [í…Œì´ë¸”](/guides/tables)ì— ë¶„í•  ë§ˆìŠ¤í¬ë¥¼ ê¸°ë¡í•˜ë ¤ë©´ í…Œì´ë¸”ì˜ ê° í–‰ì— `wandb.Image` ì˜¤ë¸Œì íŠ¸ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

ì˜ˆì œëŠ” ì•„ë˜ì˜ ì˜ì‚¬ ì½”ë“œì— ì œê³µë©ë‹ˆë‹¤:

```python
table = wandb.Table(columns=["ID", "Image"])

for id, img, label in zip(ids, images, labels):
    mask_img = wandb.Image(
        img,
        masks={
            "prediction": {"mask_data": label, "class_labels": class_labels}
            # ...
        },
    )

    table.add_data(id, img)

wandb.log({"Table": table})
```

ì´ì œ ê°„ë‹¨í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì—¬ ìƒ˜í”Œ ì´ë¯¸ì§€, ë ˆì´ë¸”, `wandb.Table` ì˜¤ë¸Œì íŠ¸ì™€ ì¼ë¶€ ê´€ë ¨ ë©”íƒ€ë°ì´í„°ë¥¼ í…Œì´ë¸”ì˜ í–‰ì— ì±„ìš°ê³  ì´ë¥¼ Weights & Biases ëŒ€ì‹œë³´ë“œì— ê¸°ë¡í•©ë‹ˆë‹¤.

```python
def log_data_samples_into_tables(
    sample_image: np.array,
    sample_label: np.array,
    split: str = None,
    data_idx: int = None,
    table: wandb.Table = None,
):
    num_channels, _, _, num_slices = sample_image.shape
    with tqdm(total=num_slices, leave=False) as progress_bar:
        for slice_idx in range(num_slices):
            ground_truth_wandb_images = []
            for channel_idx in range(num_channels):
                ground_truth_wandb_images.append(
                    masks = {
                        "ground-truth/Tumor-Core": {
                            "mask_data": sample_label[0, :, :, slice_idx],
                            "class_labels": {0: "background", 1: "Tumor Core"},
                        },
                        "ground-truth/Whole-Tumor": {
                            "mask_data": sample_label[1, :, :, slice_idx] * 2,
                            "class_labels": {0: "background", 2: "Whole Tumor"},
                        },
                        "ground-truth/Enhancing-Tumor": {
                            "mask_data": sample_label[2, :, :, slice_idx] * 3,
                            "class_labels": {0: "background", 3: "Enhancing Tumor"},
                        },
                    }
                    wandb.Image(
                        sample_image[channel_idx, :, :, slice_idx],
                        masks=masks,
                    )
                )
            table.add_data(split, data_idx, slice_idx, *ground_truth_wandb_images)
            progress_bar.update(1)
    return table
```

ë‹¤ìŒìœ¼ë¡œ, ë°ì´í„° ì‹œê°í™”ë¥¼ ìœ„í•´ ë°ì´í„°ë¡œ êµ¬ì„±ëœ í–‰ì„ í…Œì´ë¸”ì— ì±„ìš°ê¸° ìœ„í•´ `wandb.Table` ì˜¤ë¸Œì íŠ¸ì™€ ê·¸ê²ƒì´ í¬í•¨í•  ì—´ì„ ì •ì˜í•˜ì„¸ìš”.

```python
table = wandb.Table(
    columns=[
        "Split",
        "Data Index",
        "Slice Index",
        "Image-Channel-0",
        "Image-Channel-1",
        "Image-Channel-2",
        "Image-Channel-3",
    ]
)
```

ê·¸ëŸ° ë‹¤ìŒ `train_dataset`ê³¼ `val_dataset`ì„ ê°ê° ìˆœíšŒí•˜ì—¬ ë°ì´í„° ìƒ˜í”Œì˜ ì‹œê°í™”ë¥¼ ìƒì„±í•˜ê³  ëŒ€ì‹œë³´ë“œì— ë¡œê·¸ë  í…Œì´ë¸”ì˜ í–‰ì„ ì±„ì›ë‹ˆë‹¤.

```python
# train_datasetì— ëŒ€í•œ ì‹œê°í™” ìƒì„±
max_samples = (
    min(config.max_train_images_visualized, len(train_dataset))
    if config.max_train_images_visualized > 0
    else len(train_dataset)
)
progress_bar = tqdm(
    enumerate(train_dataset[:max_samples]),
    total=max_samples,
    desc="Generating Train Dataset Visualizations:",
)
for data_idx, sample in progress_bar:
    sample_image = sample["image"].detach().cpu().numpy()
    sample_label = sample["label"].detach().cpu().numpy()
    table = log_data_samples_into_tables(
        sample_image,
        sample_label,
        split="train",
        data_idx=data_idx,
        table=table,
    )

# val_datasetì— ëŒ€í•œ ì‹œê°í™” ìƒì„±
max_samples = (
    min(config.max_val_images_visualized, len(val_dataset))
    if config.max_val_images_visualized > 0
    else len(val_dataset)
)
progress_bar = tqdm(
    enumerate(val_dataset[:max_samples]),
    total=max_samples,
    desc="Generating Validation Dataset Visualizations:",
)
for data_idx, sample in progress_bar:
    sample_image = sample["image"].detach().cpu().numpy()
    sample_label = sample["label"].detach().cpu().numpy()
    table = log_data_samples_into_tables(
        sample_image,
        sample_label,
        split="val",
        data_idx=data_idx,
        table=table,
    )

# ëŒ€ì‹œë³´ë“œì— í…Œì´ë¸” ê¸°ë¡
wandb.log({"Tumor-Segmentation-Data": table})
```

ë°ì´í„°ëŠ” W&B ëŒ€ì‹œë³´ë“œì—ì„œ ì¸í„°ë™í‹°ë¸Œí•œ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ë°ì´í„° ë³¼ë¥¨ì˜ íŠ¹ì • ìŠ¬ë¼ì´ìŠ¤ì—ì„œ ê° ì±„ë„ì„ ë³¼ ìˆ˜ ìˆìœ¼ë©°, ê° í–‰ì—ëŠ” í•´ë‹¹í•˜ëŠ” ë¶„í•  ë§ˆìŠ¤í¬ê°€ ì˜¤ë²„ë ˆì´ë©ë‹ˆë‹¤. [Weave ì¿¼ë¦¬](/guides/weave)ë¥¼ ì‘ì„±í•˜ì—¬ í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ê³  íŠ¹ì • í–‰ì— ì§‘ì¤‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

| ![ë¡œê·¸ëœ í…Œì´ë¸” ë°ì´í„° ì˜ˆì œ.](/images/tutorials/monai/viz-1.gif) | 
|:--:| 
| **ë¡œê·¸ëœ í…Œì´ë¸” ë°ì´í„° ì˜ˆì œ.** |

ì´ë¯¸ì§€ë¥¼ ì—´ê³  ì¸í„°ë™í‹°ë¸Œ ì˜¤ë²„ë ˆì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë¶„í•  ë§ˆìŠ¤í¬ì™€ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.

| ![ì‹œê°í™”ëœ ë¶„í•  ì§€ë„ ì˜ˆì œ.](/images/tutorials/monai/viz-2.gif) | 
|:--:| 
| **ì‹œê°í™”ëœ ë¶„í•  ì§€ë„ ì˜ˆì œ.* |

:::info
**ì°¸ê³ :** ë°ì´í„°ì…‹ì˜ ë ˆì´ë¸”ì€ í´ë˜ìŠ¤ ê°„ ê²¹ì¹˜ì§€ ì•ŠëŠ” ë§ˆìŠ¤í¬ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì˜¤ë²„ë ˆì´ëŠ” ì˜¤ë²„ë ˆì´ì˜ ì„œë¡œ ë‹¤ë¥¸ ë§ˆìŠ¤í¬ë¡œ ë ˆì´ë¸”ì„ ê¸°ë¡í•©ë‹ˆë‹¤.
:::

### ğŸ›« ë°ì´í„° ë¡œë”©

ë°ì´í„°ì…‹ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•œ PyTorch DataLoadersë¥¼ ë§Œë“­ë‹ˆë‹¤. DataLoadersë¥¼ ìƒì„±í•˜ê¸° ì „ì—, íŠ¸ë ˆì´ë‹ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ë³€í™˜í•˜ê¸° ìœ„í•´ `train_transform`ì„ `train_dataset`ì— ì„¤ì •í•˜ì„¸ìš”.

```python
# íŠ¸ë ˆì´ë‹ ë°ì´í„°ì…‹ì— train_transforms ì ìš©
train_dataset.transform = train_transform

# train_loader ë§Œë“¤ê¸°
train_loader = DataLoader(
    train_dataset,
    batch_size=config.batch_size,
    shuffle=True,
    num_workers=config.num_workers,
)

# val_loader ë§Œë“¤ê¸°
val_loader = DataLoader(
    val_dataset,
    batch_size=config.batch_size,
    shuffle=False,
    num_workers=config.num_workers,
)
```

## ğŸ¤– ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ìƒì„±

ì´ íŠœí† ë¦¬ì–¼ì€ [3D MRI brain tumor segmentation using auto-encoder regularization](https://arxiv.org/pdf/1810.11654.pdf) ë…¼ë¬¸ì— ë”°ë¼ `SegResNet` ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤. `SegResNet` ëª¨ë¸ì€ `monai.networks` APIì˜ ì¼ë¶€ë¡œ PyTorch ëª¨ë“ˆë¡œ êµ¬í˜„ë˜ì–´ ìˆìœ¼ë©°, ì˜µí‹°ë§ˆì´ì €ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë„ í¬í•¨í•©ë‹ˆë‹¤.

```python
device = torch.device("cuda:0")

# ëª¨ë¸ ìƒì„±
model = SegResNet(
    blocks_down=[1, 2, 2, 4],
    blocks_up=[1, 1, 1],
    init_filters=16,
    in_channels=4,
    out_channels=3,
    dropout_prob=0.2,
).to(device)

# ì˜µí‹°ë§ˆì´ì € ìƒì„±
optimizer = torch.optim.Adam(
    model.parameters(),
    config.initial_learning_rate,
    weight_decay=config.weight_decay,
)

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=config.max_train_epochs
)
```

ì†ì‹¤ì„ ë‹¤ì¤‘ ë ˆì´ë¸” `DiceLoss`ë¡œ ì •ì˜í•˜ê³  `monai.losses` APIì™€ `monai.metrics` APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ Dice ë©”íŠ¸ë¦­ì„ ì •ì˜í•©ë‹ˆë‹¤.

```python
loss_function = DiceLoss(
    smooth_nr=config.dice_loss_smoothen_numerator,
    smooth_dr=config.dice_loss_smoothen_denominator,
    squared_pred=config.dice_loss_squared_prediction,
    to_onehot_y=config.dice_loss_target_onehot,
    sigmoid=config.dice_loss_apply_sigmoid,
)

dice_metric = DiceMetric(include_background=True, reduction="mean")
dice_metric_batch = DiceMetric(include_background=True, reduction="mean_batch")
post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])

# ìë™ í˜¼í•© ì •ë°€ë„ ì‚¬ìš©í•˜ì—¬ íŠ¸ë ˆì´ë‹ ê°€ì†í™”
scaler = torch.cuda.amp.GradScaler()
torch.backends.cudnn.benchmark = True
```

í˜¼í•© ì •ë°€ ì¶”ë¡ ì„ ìœ„í•œ ì‘ì€ ìœ í‹¸ë¦¬í‹°ë¥¼ ì •ì˜í•˜ì„¸ìš”. ì´ëŠ” íŠ¸ë ˆì´ë‹ í”„ë¡œì„¸ìŠ¤ì˜ ê²€ì¦ ë‹¨ê³„ ë° íŠ¸ë ˆì´ë‹ í›„ ëª¨ë¸ì„ ì‹¤í–‰í•˜ë ¤ê³  í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.

```python
def inference(model, input):
    def _compute(input):
        return sliding_window_inference(
            inputs=input,
            roi_size=(240, 240, 160),
            sw_batch_size=1,
            predictor=model,
            overlap=0.5,
        )

    with torch.cuda.amp.autocast():
        return _compute(input)
```

## ğŸš íŠ¸ë ˆì´ë‹ ë° ê²€ì¦

íŠ¸ë ˆì´ë‹ ì „ì— `wandb.log()`ì™€ í•¨ê»˜ ë‚˜ì¤‘ì— ë¡œê¹…ë  ë©”íŠ¸ë¦­ ì†ì„±ì„ ì •ì˜í•˜ì—¬ íŠ¸ë ˆì´ë‹ ë° ê²€ì¦ ì‹¤í—˜ì„ ì¶”ì í•©ë‹ˆë‹¤.

```python
wandb.define_metric("epoch/epoch_step")
wandb.define_metric("epoch/*", step_metric="epoch/epoch_step")
wandb.define_metric("batch/batch_step")
wandb.define_metric("batch/*", step_metric="batch/batch_step")
wandb.define_metric("validation/validation_step")
wandb.define_metric("validation/*", step_metric="validation/validation_step")

batch_step = 0
validation_step = 0
metric_values = []
metric_values_tumor_core = []
metric_values_whole_tumor = []
metric_values_enhanced_tumor = []
```

### ğŸ­ í‘œì¤€ PyTorch íŠ¸ë ˆì´ë‹ ë£¨í”„ ì‹¤í–‰

```python
# W&B Artifact ì˜¤ë¸Œì íŠ¸ ì •ì˜
artifact = wandb.Artifact(
    name=f"{wandb.run.id}-checkpoint", type="model"
)

epoch_progress_bar = tqdm(range(config.max_train_epochs), desc="Training:")

for epoch in epoch_progress_bar:
    model.train()
    epoch_loss = 0

    total_batch_steps = len(train_dataset) // train_loader.batch_size
    batch_progress_bar = tqdm(train_loader, total=total_batch_steps, leave=False)
    
    # íŠ¸ë ˆì´ë‹ ìŠ¤í…
    for batch_data in batch_progress_bar:
        inputs, labels = (
            batch_data["image"].to(device),
            batch_data["label"].to(device),
        )
        optimizer.zero_grad()
        with torch.cuda.amp.autocast():
            outputs = model(inputs)
            loss = loss_function(outputs, labels)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        epoch_loss += loss.item()
        batch_progress_bar.set_description(f"train_loss: {loss.item():.4f}:")
        ## ë°°ì¹˜ ë‹¨ìœ„ íŠ¸ë ˆì´ë‹ ì†ì‹¤ì„ W&Bì— ë¡œê·¸
        wandb.log({"batch/batch_step": batch_step, "batch/train_loss": loss.item()})
        batch_step += 1

    lr_scheduler.step()
    epoch_loss /= total_batch_steps
    ## ë°°ì¹˜ ë‹¨ìœ„ íŠ¸ë ˆì´ë‹ ì†ì‹¤ê³¼ í•™ìŠµë¥ ì„ W&Bì— ë¡œê·¸
    wandb.log(
        {
            "epoch/epoch_step": epoch,
            "epoch/mean_train_loss": epoch_loss,
            "epoch/learning_rate": lr_scheduler.get_last_lr()[0],
        }
    )
    epoch_progress_bar.set_description(f"Training: train_loss: {epoch_loss:.4f}:")

    # ê²€ì¦ ë° ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë‹¨ê³„
    if (epoch + 1) % config.validation_intervals == 0:
        model.eval()
        with torch.no_grad():
            for val_data in val_loader:
                val_inputs, val_labels = (
                    val_data["image"].to(device),
                    val_data["label"].to(device),
                )
                val_outputs = inference(model, val_inputs)
                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]
                dice_metric(y_pred=val_outputs, y=val_labels)
                dice_metric_batch(y_pred=val_outputs, y=val_labels)

            metric_values.append(dice_metric.aggregate().item())
            metric_batch = dice_metric_batch.aggregate()
            metric_values_tumor_core.append(metric_batch[0].item())
            metric_values_whole_tumor.append(metric_batch[1].item())
            metric_values_enhanced_tumor.append(metric_batch[2].item())
            dice_metric.reset()
            dice_metric_batch.reset()

            checkpoint_path = os.path.join(config.checkpoint_dir, "model.pth")
            torch.save(model.state_dict(), checkpoint_path)
            
            # W&B artifactsë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œê·¸ ë° ë²„ì „ ê´€ë¦¬í•©ë‹ˆë‹¤.
            artifact.add_file(local_path=checkpoint_path)
            wandb.log_artifact(artifact, aliases=[f"epoch_{epoch}"])

            # W&B ëŒ€ì‹œë³´ë“œì— ê²€ì¦ ë©”íŠ¸ë¦­ì„ ë¡œê·¸í•©ë‹ˆë‹¤.
            wandb.log(
                {
                    "validation/validation_step": validation_step,
                    "validation/mean_dice": metric_values[-1],
                    "validation/mean_dice_tumor_core": metric_values_tumor_core[-1],
                    "validation/mean_dice_whole_tumor": metric_values_whole_tumor[-1],
                    "validation/mean_dice_enhanced_tumor": metric_values_enhanced_tumor[-1],
                }
            )
            validation_step += 1


# ì´ ì•„í‹°íŒ©íŠ¸ê°€ ë¡œê·¸ ë§ˆì¹˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
artifact.wait()
```

`wandb.log`ë¡œ ì½”ë“œë¥¼ ê¸°ì…í•¨ìœ¼ë¡œì¨ íŠ¸ë ˆì´ë‹ ë° ê²€ì¦ í”„ë¡œì„¸ìŠ¤ì™€ ê´€ë ¨ëœ ëª¨ë“  ë©”íŠ¸ë¦­ë¿ë§Œ ì•„ë‹ˆë¼ W&B ëŒ€ì‹œë³´ë“œì—ì„œ ëª¨ë“  ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ (ì´ ê²½ìš° ìš°ë¦¬ì˜ CPU ë° GPU)ë„ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

| ![W&Bì—ì„œì˜ íŠ¸ë ˆì´ë‹ ë° ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì¶”ì  ì˜ˆì œ.](/images/tutorials/monai/viz-3.gif) | 
|:--:| 
| **W&Bì—ì„œì˜ íŠ¸ë ˆì´ë‹ ë° ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì¶”ì  ì˜ˆì œ.** |

íŠ¸ë ˆì´ë‹ ì¤‘ì— ê¸°ë¡ëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì•„í‹°íŒ©íŠ¸ì˜ ë‹¤ì–‘í•œ ë²„ì „ì— ì—‘ì„¸ìŠ¤í•˜ë ¤ë©´ W&B run ëŒ€ì‹œë³´ë“œì˜ ì•„í‹°íŒ©íŠ¸ íƒ­ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.

| ![W&Bì—ì„œì˜ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê¸°ë¡ ë° ë²„ì „ ê´€ë¦¬ ì˜ˆì œ.](/images/tutorials/monai/viz-4.gif) | 
|:--:| 
| **W&Bì—ì„œì˜ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê¸°ë¡ ë° ë²„ì „ ê´€ë¦¬ ì˜ˆì œ.** |

## ğŸ”± ì¶”ë¡ 

ì•„í‹°íŒ©íŠ¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„í‹°íŒ©íŠ¸ì˜ ë²„ì „ ì¤‘ ì–´ë–¤ ê²ƒì´ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì¸ì§€ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°, í‰ê·  ì—í¬í¬ë³„ íŠ¸ë ˆì´ë‹ ì†ì‹¤ì…ë‹ˆë‹¤. ì•„í‹°íŒ©íŠ¸ì˜ ì „ì²´ ê³„ë³´ë¥¼ íƒìƒ‰í•˜ê³  í•„ìš”í•œ ë²„ì „ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

| ![W&Bì—ì„œì˜ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì¶”ì  ì˜ˆì œ.](/images/tutorials/monai/viz-5.gif) | 
|:--:| 
| **W&Bì—ì„œì˜ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì¶”ì  ì˜ˆì œ.** |

ì—í¬í¬ë³„ í‰ê·  íŠ¸ë ˆì´ë‹ ì†ì‹¤ì´ ìµœìƒì˜ ë²„ì „ì¸ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ì— ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ ì‚¬ì „ì„ ë¡œë“œí•©ë‹ˆë‹¤.

```python
model_artifact = wandb.use_artifact(
    "geekyrakshit/monai-brain-tumor-segmentation/d5ex6n4a-checkpoint:v49",
    type="model",
)
model_artifact_dir = model_artifact.download()
model.load_state_dict(torch.load(os.path.join(model_artifact_dir, "model.pth")))
model.eval()
```

### ğŸ“¸ ì˜ˆì¸¡ ì‹œê°í™” ë° ê·¸ë¼ìš´ë“œ íŠ¸ë£¨ìŠ¤ ë ˆì´ë¸”ê³¼ ë¹„êµ

ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì‹œê°í™”í•˜ê³ , ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ìƒí˜¸ì‘ìš© ë¶„í•  ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹í•˜ëŠ” ê·¸ë¼ìš´ë“œ íŠ¸ë£¨ìŠ¤ ë¶„í•  ë§ˆìŠ¤í¬ì™€ ë¹„êµí•˜ëŠ” ë˜ ë‹¤ë¥¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.

```python
def log_predictions_into_tables(
    sample_image: np.array,
    sample_label: np.array,
    predicted_label: np.array,
    split: str = None,
    data_idx: int = None,
    table: wandb.Table = None,
):
    num_channels, _, _, num_slices = sample_image.shape
    with tqdm(total=num_slices, leave=False) as progress_bar:
        for slice_idx in range(num_slices):
            wandb_images = []
            for channel_idx in range(num_channels):
                wandb_images += [
                    wandb.Image(
                        sample_image[channel_idx, :, :, slice_idx],
                        masks={
                            "ground-truth/Tumor-Core": {
                                "mask_data": sample_label[0, :, :, slice_idx],
                                "class_labels": {0: "background", 1: "Tumor Core"},
                            },
                            "prediction/Tumor-Core": {
                                "mask_data": predicted_label[0, :, :, slice_idx] * 2,
                                "class_labels": {0: "background", 2: "Tumor Core"},
                            },
                        },
                    ),
                    wandb.Image(
                        sample_image[channel_idx, :, :, slice_idx],
                        masks={
                            "ground-truth/Whole-Tumor": {
                                "mask_data": sample_label[1, :, :, slice_idx],
                                "class_labels": {0: "background", 1: "Whole Tumor"},
                            },
                            "prediction/Whole-Tumor": {
                                "mask_data": predicted_label[1, :, :, slice_idx] * 2,
                                "class_labels": {0: "background", 2: "Whole Tumor"},
                            },
                        },
                    ),
                    wandb.Image(
                        sample_image[channel_idx, :, :, slice_idx],
                        masks={
                            "ground-truth/Enhancing-Tumor": {
                                "mask_data": sample_label[2, :, :, slice_idx],
                                "class_labels": {0: "background", 1: "Enhancing Tumor"},
                            },
                            "prediction/Enhancing-Tumor": {
                                "mask_data": predicted_label[2, :, :, slice_idx] * 2,
                                "class_labels": {0: "background", 2: "Enhancing Tumor"},
                            },
                        },
                    ),
                ]
            table.add_data(split, data_idx, slice_idx, *wandb_images)
            progress_bar.update(1)
    return table
```

ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì˜ˆì¸¡ í…Œì´ë¸”ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤.

```python
# ì˜ˆì¸¡ í…Œì´ë¸” ìƒì„±
prediction_table = wandb.Table(
    columns=[
        "Split",
        "Data Index",
        "Slice Index",
        "Image-Channel-0/Tumor-Core",
        "Image-Channel-1/Tumor-Core",
        "Image-Channel-2/Tumor-Core",
        "Image-Channel-3/Tumor-Core",
        "Image-Channel-0/Whole-Tumor",
        "Image-Channel-1/Whole-Tumor",
        "Image-Channel-2/Whole-Tumor",
        "Image-Channel-3/Whole-Tumor",
        "Image-Channel-0/Enhancing-Tumor",
        "Image-Channel-1/Enhancing-Tumor",
        "Image-Channel-2/Enhancing-Tumor",
        "Image-Channel-3/Enhancing-Tumor",
    ]
)

# ì¶”ë¡  ë° ì‹œê°í™” ìˆ˜í–‰
with torch.no_grad():
    config.max_prediction_images_visualized
    max_samples = (
        min(config.max_prediction_images_visualized, len(val_dataset))
        if config.max_prediction_images_visualized > 0
        else len(val_dataset)
    )
    progress_bar = tqdm(
        enumerate(val_dataset[:max_samples]),
        total=max_samples,
        desc="Generating Predictions:",
    )
    for data_idx, sample in progress_bar:
        val_input = sample["image"].unsqueeze(0).to(device)
        val_output = inference(model, val_input)
        val_output = post_trans(val_output[0])
        prediction_table = log_predictions_into_tables(
            sample_image=sample["image"].cpu().numpy(),
            sample_label=sample["label"].cpu().numpy(),
            predicted_label=val_output.cpu().numpy(),
            data_idx=data_idx,
            split="validation",
            table=prediction_table,
        )

    wandb.log({"Predictions/Tumor-Segmentation-Data": prediction_table})


# ì‹¤í—˜ ì¢…ë£Œ
wandb.finish()
```

ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ê·¸ë¼ìš´ë“œ íŠ¸ë£¨ìŠ¤ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë¶„í•  ë§ˆìŠ¤í¬ë¥¼ ë¶„ì„í•˜ê³  ë¹„êµí•˜ê¸° ìœ„í•´ ìƒí˜¸ì‘ìš© ë¶„í•  ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

| ![W&Bì—ì„œì˜ ì˜ˆì¸¡ ë° ê·¸ë¼ìš´ë“œ íŠ¸ë£¨ìŠ¤ ì‹œê°í™” ì˜ˆì œ.](/images/tutorials/monai/viz-6.gif) | 
|:--:| 
| **W&Bì—ì„œì˜ ì˜ˆì¸¡ ë° ê·¸ë¼ìš´ë“œ íŠ¸ë£¨ìŠ¤ ì‹œê°í™” ì˜ˆì œ.** |

## ê°ì‚¬ ë° ì¶”ê°€ ìë£Œ

* [MONAI íŠœí† ë¦¬ì–¼: Brain tumor 3D segmentation with MONAI](https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb)
* [WandB ë¦¬í¬íŠ¸: Brain Tumor Segmentation using MONAI and WandB](https://wandb.ai/geekyrakshit/brain-tumor-segmentation/reports/Brain-Tumor-Segmentation-using-MONAI-and-WandB---Vmlldzo0MjUzODIw)
```