import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import { CTAButtons } from '@site/src/components/CTAButtons/CTAButtons.tsx';


# 3D brain tumor segmentation with MONAI

<CTAButtons colabLink="https://colab.research.google.com/github/wandb/examples/blob/main/colabs/monai/3d_brain_tumor_segmentation.ipynb"></CTAButtons>

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€[MONAI](https://github.com/Project-MONAI/MONAI) ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«ã®3Dè„³è…«ç˜ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã—ã€[Weights & Biases](https://wandb.ai/site) ã®å®Ÿé¨“ç®¡ç†ã¨ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã«ã¯æ¬¡ã®æ©Ÿèƒ½ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

1. Weights & Biases ã® run ã‚’åˆæœŸåŒ–ã—ã€å†ç¾æ€§ã®ãŸã‚ã« run ã«é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®è¨­å®šã‚’åŒæœŸã—ã¾ã™ã€‚
2. MONAI transform API:
    1. è¾æ›¸å½¢å¼ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚ã® MONAI Transformsã€‚
    2. MONAI `transforms` API ã«å¾“ã£ã¦æ–°ã—ã„ transform ã‚’å®šç¾©ã™ã‚‹æ–¹æ³•ã€‚
    3. ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®ãŸã‚ã«å¼·åº¦ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«èª¿æ•´ã™ã‚‹æ–¹æ³•ã€‚
3. ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ã¨å¯è¦–åŒ–:
    1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ã `Nifti` ç”»åƒã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ç”»åƒãƒªã‚¹ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã‚Œã‚‰ã‚’ã‚¹ã‚¿ãƒƒã‚¯ã—ã¾ã™ã€‚
    2. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¤œè¨¼ã‚’åŠ é€Ÿã™ã‚‹ãŸã‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ IO ã¨ transformsã€‚
    3. `wandb.Table` ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–ã—ã€Weights & Biases ä¸Šã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã‚’è¡Œã„ã¾ã™ã€‚
4. 3D `SegResNet` ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°:
    1. MONAI ã® `networks`ã€`losses`ã€`metrics` APIs ã‚’ä½¿ç”¨ã€‚
    2. PyTorch ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨ã—ã¦ 3D `SegResNet` ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚
    3. Weights & Biases ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿé¨“ã‚’è¿½è·¡ã—ã¾ã™ã€‚
    4. ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ Weights & Biases ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦ãƒ­ã‚°ãŠã‚ˆã³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã—ã¾ã™ã€‚
5. `wandb.Table` ãŠã‚ˆã³ Weights & Biases ä¸Šã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã‚’ä½¿ç”¨ã—ã¦ã€æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸Šã®äºˆæ¸¬ã‚’å¯è¦–åŒ–ãŠã‚ˆã³æ¯”è¼ƒã—ã¾ã™ã€‚

## ğŸŒ´ Set ğŸŒ»ã‚¢ãƒƒãƒ—ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³
æœ€åˆã«ã€æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã® MONAI ã¨ Weights & Biases ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

```python
!python -c "import monai" || pip install -q -U "monai[nibabel, tqdm]"
!python -c "import wandb" || pip install -q -U wandb
```

```python
import os

import numpy as np
from tqdm.auto import tqdm
import wandb

from monai.apps import DecathlonDataset
from monai.data import DataLoader, decollate_batch
from monai.losses import DiceLoss
from monai.inferers import sliding_window_inference
from monai.metrics import DiceMetric
from monai.networks.nets import SegResNet
from monai.transforms import (
    Activations,
    AsDiscrete,
    Compose,
    LoadImaged,
    MapTransform,
    NormalizeIntensityd,
    Orientationd,
    RandFlipd,
    RandScaleIntensityd,
    RandShiftIntensityd,
    RandSpatialCropd,
    Spacingd,
    EnsureTyped,
    EnsureChannelFirstd,
)
from monai.utils import set_determinism

import torch
```

æ¬¡ã«ã€Colab ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’èªè¨¼ã—ã¦ W&B ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```python
wandb.login()
```

## ğŸŒ³ W&B Run ã®åˆæœŸåŒ–

æ–°ã—ã„ W&B run ã‚’é–‹å§‹ã—ã¦å®Ÿé¨“ã‚’è¿½è·¡ã—ã¾ã™ã€‚

```python
wandb.init(project="monai-brain-tumor-segmentation")
```

é©åˆ‡ãª config ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯ã€å†ç¾æ€§ã®ã‚ã‚‹æ©Ÿæ¢°å­¦ç¿’ã®ãŸã‚ã®æ¨å¥¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã§ã™ã€‚W&B ã‚’ä½¿ç”¨ã—ã¦å„å®Ÿé¨“ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’è¿½è·¡ã§ãã¾ã™ã€‚

```python
config = wandb.config
config.seed = 0
config.roi_size = [224, 224, 144]
config.batch_size = 1
config.num_workers = 4
config.max_train_images_visualized = 20
config.max_val_images_visualized = 20
config.dice_loss_smoothen_numerator = 0
config.dice_loss_smoothen_denominator = 1e-5
config.dice_loss_squared_prediction = True
config.dice_loss_target_onehot = False
config.dice_loss_apply_sigmoid = True
config.initial_learning_rate = 1e-4
config.weight_decay = 1e-5
config.max_train_epochs = 50
config.validation_intervals = 1
config.dataset_dir = "./dataset/"
config.checkpoint_dir = "./checkpoints"
config.inference_roi_size = (128, 128, 64)
config.max_prediction_images_visualized = 20
```

ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®šã—ã¦ã€æ±ºå®šçš„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æœ‰åŠ¹ã¾ãŸã¯ç„¡åŠ¹ã«ã™ã‚‹å¿…è¦ã‚‚ã‚ã‚Šã¾ã™ã€‚

```python
set_determinism(seed=config.seed)

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
os.makedirs(config.dataset_dir, exist_ok=True)
os.makedirs(config.checkpoint_dir, exist_ok=True)
```

## ğŸ’¿ ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ã¨å¤‰æ›

ã“ã“ã§ã¯ã€`monai.transforms` API ã‚’ä½¿ç”¨ã—ã¦ã€ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’ one-hot å½¢å¼ã®ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã«å¤‰æ›ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ ã‚’ä½œæˆã—ã¾ã™ã€‚

```python
class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):
    """
    brats ã‚¯ãƒ©ã‚¹ã«åŸºã¥ã„ã¦ãƒ©ãƒ™ãƒ«ã‚’ãƒãƒ«ãƒãƒãƒ£ãƒ³ãƒãƒ«ã«å¤‰æ›ã—ã¾ã™ï¼š
    ãƒ©ãƒ™ãƒ« 1 ã¯å‘¨å›²ã®æµ®è…«
    ãƒ©ãƒ™ãƒ« 2 ã¯ GD å¢—å¼·è…«ç˜
    ãƒ©ãƒ™ãƒ« 3 ã¯å£Šæ­»ã¨éå¢—å¼·è…«ç˜ã‚³ã‚¢
    ã‚¯ãƒ©ã‚¹ã¯ TC (è…«ç˜ã‚³ã‚¢)ã€WT (å…¨è…«ç˜)ã€ET (å¢—å¼·è…«ç˜) ã§ã™ã€‚

    å‚è€ƒ: https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb
    """

    def __call__(self, data):
        d = dict(data)
        for key in self.keys:
            result = []
            # ãƒ©ãƒ™ãƒ« 2 ã¨ãƒ©ãƒ™ãƒ« 3 ã‚’çµåˆã—ã¦ TC ã‚’æ§‹ç¯‰
            result.append(torch.logical_or(d[key] == 2, d[key] == 3))
            # ãƒ©ãƒ™ãƒ« 1, 2 ã¨ 3 ã‚’çµåˆã—ã¦ WT ã‚’æ§‹ç¯‰
            result.append(
                torch.logical_or(
                    torch.logical_or(d[key] == 2, d[key] == 3), d[key] == 1
                )
            )
            # ãƒ©ãƒ™ãƒ« 2 ã¯ ET
            result.append(d[key] == 2)
            d[key] = torch.stack(result, axis=0).float()
        return d
```

æ¬¡ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãŸã‚ã® transform ã‚’è¨­å®šã—ã¾ã™ã€‚

```python
train_transform = Compose(
    [
        # 4 ã¤ã® Nifti ç”»åƒã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¸€ç·’ã«ã‚¹ã‚¿ãƒƒã‚¯
        LoadImaged(keys=["image", "label"]),
        EnsureChannelFirstd(keys="image"),
        EnsureTyped(keys=["image", "label"]),
        ConvertToMultiChannelBasedOnBratsClassesd(keys="label"),
        Orientationd(keys=["image", "label"], axcodes="RAS"),
        Spacingd(
            keys=["image", "label"],
            pixdim=(1.0, 1.0, 1.0),
            mode=("bilinear", "nearest"),
        ),
        RandSpatialCropd(
            keys=["image", "label"], roi_size=config.roi_size, random_size=False
        ),
        RandFlipd(keys=["image", "label"], prob=0.5, spatial_axis=0),
        RandFlipd(keys=["image", "label"], prob=0.5, spatial_axis=1),
        RandFlipd(keys=["image", "label"], prob=0.5, spatial_axis=2),
        NormalizeIntensityd(keys="image", nonzero=True, channel_wise=True),
        RandScaleIntensityd(keys="image", factors=0.1, prob=1.0),
        RandShiftIntensityd(keys="image", offsets=0.1, prob=1.0),
    ]
)
val_transform = Compose(
    [
        LoadImaged(keys=["image", "label"]),
        EnsureChannelFirstd(keys="image"),
        EnsureTyped(keys=["image", "label"]),
        ConvertToMultiChannelBasedOnBratsClassesd(keys="label"),
        Orientationd(keys=["image", "label"], axcodes="RAS"),
        Spacingd(
            keys=["image", "label"],
            pixdim=(1.0, 1.0, 1.0),
            mode=("bilinear", "nearest"),
        ),
        NormalizeIntensityd(keys="image", nonzero=True, channel_wise=True),
    ]
)
```

### ğŸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

ã“ã®å®Ÿé¨“ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ http://medicaldecathlon.com/ ã‹ã‚‰å–å¾—ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€FLAIRã€T1wã€T1gdã€T2w ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ»ãƒãƒ«ãƒã‚µã‚¤ãƒˆ MRI ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€ã‚°ãƒªã‚ªãƒ¼ãƒã€å£Šæ­»/æ´»å‹•ä¸­ã®è…«ç˜ã€ãŠã‚ˆã³æµ®è…«ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ 750 ã® 4D ãƒœãƒªãƒ¥ãƒ¼ãƒ  (484 ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° + 266 ãƒ†ã‚¹ãƒˆ) ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚

`DecathlonDataset` ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªå‹•çš„ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŠã‚ˆã³æŠ½å‡ºã—ã¾ã™ã€‚ ã“ã‚Œã¯ MONAI `CacheDataset` ã‚’ç¶™æ‰¿ã—ã¦ãŠã‚Šã€ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºã«å¿œã˜ã¦ `cache_num=N` ã‚’è¨­å®šã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã« `N` å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€æ¤œè¨¼ç”¨ã«ã™ã¹ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚

```python
train_dataset = DecathlonDataset(
    root_dir=config.dataset_dir,
    task="Task01_BrainTumour",
    transform=val_transform,
    section="training",
    download=True,
    cache_rate=0.0,
    num_workers=4,
)
val_dataset = DecathlonDataset(
    root_dir=config.dataset_dir,
    task="Task01_BrainTumour",
    transform=val_transform,
    section="validation",
    download=False,
    cache_rate=0.0,
    num_workers=4,
)
```

:::info
**Note:**  `train_dataset` ã« `train_transform` ã‚’é©ç”¨ã™ã‚‹ä»£ã‚ã‚Šã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ã« `val_transform` ã‚’é©ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸¡æ–¹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å¯è¦–åŒ–ã™ã‚‹ãŸã‚ã§ã™ã€‚
:::

### ğŸ“¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¯è¦–åŒ–

Weights & Biases ã¯ç”»åƒã€ãƒ“ãƒ‡ã‚ªã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãªã©ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ãƒªãƒƒãƒãƒ¡ãƒ‡ã‚£ã‚¢ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã—ã¦çµæœã‚’æ¢ç´¢ã—ã€runã€modelã€ãŠã‚ˆã³ dataset ã‚’è¦–è¦šçš„ã«æ¯”è¼ƒã§ãã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚’è¦–è¦šåŒ–ã™ã‚‹ãŸã‚ã®[ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã‚·ã‚¹ãƒ†ãƒ ](https://docs.wandb.ai/guides/track/log/media#image-overlays-in-tables) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚[tables](https://docs.wandb.ai/guides/tables) ã«ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹ã«ã¯ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã®å„è¡Œã« `wandb.Image` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æä¾›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

æ“¬ä¼¼ã‚³ãƒ¼ãƒ‰ã®ä¾‹ã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™:

```python
table = wandb.Table(columns=["ID", "Image"])

for id, img, label in zip(ids, images, labels):
    mask_img = wandb.Image(
        img,
        masks={
            "prediction": {"mask_data": label, "class_labels": class_labels}
            # ...
        },
    )

    table.add_data(id, img)

wandb.log({"Table": table})
```

æ¬¡ã«ã€ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã€ãƒ©ãƒ™ãƒ«ã€`wandb.Table` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€ãŠã‚ˆã³ã„ãã¤ã‹ã®é–¢é€£ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€Weights & Biases ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ãƒ­ã‚°ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œã‚’åŸ‹ã‚ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã‚’æ›¸ãã¾ã™ã€‚

```python
def log_data_samples_into_tables(
    sample_image: np.array,
    sample_label: np.array,
    split: str = None,
    data_idx: int = None,
    table: wandb.Table = None,
):
    num_channels, _, _, num_slices = sample_image.shape
    with tqdm(total=num_slices, leave=False) as progress_bar:
        for slice_idx in range(num_slices):
            ground_truth_wandb_images = []
            for channel_idx in range(num_channels):
                ground_truth_wandb_images.append(
                    masks = {
                        "ground-truth/Tumor-Core": {
                            "mask_data": sample_label[0, :, :, slice_idx],
                            "class_labels": {0: "background", 1: "Tumor Core"},
                        },
                        "ground-truth/Whole-Tumor": {
                            "mask_data": sample_label[1, :, :, slice_idx] * 2,
                            "class_labels": {0: "background", 2: "Whole Tumor"},
                        },
                        "ground-truth/Enhancing-Tumor": {
                            "mask_data": sample_label[2, :, :, slice_idx] * 3,
                            "class_labels": {0: "background", 3: "Enhancing Tumor"},
                        },
                    }
                    wandb.Image(
                        sample_image[channel_idx, :, :, slice_idx],
                        masks=masks,
                    )
                )
            table.add_data(split, data_idx, slice_idx, *ground_truth_wandb_images)
            progress_bar.update(1)
    return table
```

æ¬¡ã«ã€`wandb.Table` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ãã®åˆ—ã‚’å®šç¾©ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã§åŸ‹ã‚ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```python
table = wandb.Table(
    columns=[
        "Split",
        "Data Index",
        "Slice Index",
        "Image-Channel-0",
        "Image-Channel-1",
        "Image-Channel-2",
        "Image-Channel-3",
    ]
)
```

æ¬¡ã«ã€ãã‚Œãã‚Œã® `train_dataset` ã¨ `val_dataset` ã‚’ãƒ«ãƒ¼ãƒ—ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã®å¯è¦–åŒ–ã‚’ç”Ÿæˆã—ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œã‚’åŸ‹ã‚ã¦ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ãƒ­ã‚°ã—ã¾ã™ã€‚

```python
# train_dataset ã®å¯è¦–åŒ–ã‚’ç”Ÿæˆ
max_samples = (
    min(config.max_train_images_visualized, len(train_dataset))
    if config.max_train_images_visualized > 0
    else len(train_dataset)
)
progress_bar = tqdm(
    enumerate(train_dataset[:max_samples]),
    total=max_samples,
    desc="Generating Train Dataset Visualizations:",
)
for data_idx, sample in progress_bar:
    sample_image = sample["image"].detach().cpu().numpy()
    sample_label = sample["label"].detach().cpu().numpy()
    table = log_data_samples_into_tables(
        sample_image,
        sample_label,
        split="train",
        data_idx=data_idx,
        table=table,
    )

# val_dataset ã®å¯è¦–åŒ–ã‚’ç”Ÿæˆ
max_samples = (
    min(config.max_val_images_visualized, len(val_dataset))
    if config.max_val_images_visualized > 0
    else len(val_dataset)
)
progress_bar = tqdm(
    enumerate(val_dataset[:max_samples]),
    total=max_samples,
    desc="Generating Validation Dataset Visualizations:",
)
for data_idx, sample in progress_bar:
    sample_image = sample["image"].detach().cpu().numpy()
    sample_label = sample["label"].detach().cpu().numpy()
    table = log_data_samples_into_tables(
        sample_image,
        sample_label,
        split="val",
        data_idx=data_idx,
        table=table,
    )

# ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ãƒ­ã‚°
wandb.log({"Tumor-Segmentation-Data": table})
```

ãƒ‡ãƒ¼ã‚¿ãŒ W&B ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªè¡¨å½¢å¼ã§è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒœãƒªãƒ¥ãƒ¼ãƒ ã®ç‰¹å®šã®ã‚¹ãƒ©ã‚¤ã‚¹ã®å„ãƒãƒ£ãƒ³ãƒãƒ«ã«å¯¾å¿œã™ã‚‹è¡Œã§ã€ãã‚Œãã‚Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ãŒé‡ã­ã‚‰ã‚ŒãŸçŠ¶æ…‹ã‚’ç¢ºèªã§ãã¾ã™ã€‚ [Weave ã‚¯ã‚¨ãƒª](https://docs.wandb.ai/guides/weave) ã‚’è¨˜è¿°ã—ã¦ã€è¡¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã€ç‰¹å®šã®è¡Œã«æ³¨ç›®ã§ãã¾ã™ã€‚

| ![An example of logged table data.](@site/static/images/tutorials/monai/viz-1.gif) | 
|:--:| 
| **An example of logged table data.** |

ç”»åƒã‚’é–‹ã„ã¦ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã‚’ä½¿ç”¨ã—ã¦å„ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’ã©ã®ã‚ˆã†ã«æ“ä½œã§ãã‚‹ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

| ![An example of visualized segmentation maps.](@site/static/images/tutorials/monai/viz-2.gif) | 
|:--:| 
| **An example of visualized segmentation maps.* |

:::info
**Note:** ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ãƒ™ãƒ«ã¯ã‚¯ãƒ©ã‚¹é–“ã§éé‡è¤‡ãƒã‚¹ã‚¯ã‚’å«ã¿ã¾ã™ã€‚ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã¯ãƒ©ãƒ™ãƒ«ã‚’å€‹åˆ¥ã®ãƒã‚¹ã‚¯ã¨ã—ã¦ãƒ­ã‚°ã«è¨˜éŒ²ã—ã¾ã™ã€‚
:::

### ğŸ›« ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã® PyTorch DataLoader ã‚’ä½œæˆã—ã¾ã™ã€‚DataLoader ã‚’ä½œæˆã™ã‚‹å‰ã«ã€ `train_transform` ã‚’ ` train_dataset` ã«è¨­å®šã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ãŠã‚ˆã³å¤‰æ›ã—ã¾ã™ã€‚

```python
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã« train_transforms ã‚’é©ç”¨
train_dataset.transform = train_transform

# train_loader ã‚’ä½œæˆ
train_loader = DataLoader(
    train_dataset,
    batch_size=config.batch_size,
    shuffle=True,
   

## ğŸ¤– Creating the Model, Loss, and Optimizer

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€[3D MRI brain tumor segmentation using auto-encoder regularization](https://arxiv.org/pdf/1810.11654.pdf) ã¨ã„ã†è«–æ–‡ã«åŸºã¥ã„ã¦ `SegResNet` ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚ã“ã® `SegResNet` ãƒ¢ãƒ‡ãƒ«ã¯ã€`monai.networks` API ã®ä¸€éƒ¨ã¨ã—ã¦ PyTorch Module ã¨ã—ã¦å®Ÿè£…ã•ã‚Œã¦ãŠã‚Šã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚„å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚‚å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

```python
device = torch.device("cuda:0")

# model ã‚’ä½œæˆ
model = SegResNet(
    blocks_down=[1, 2, 2, 4],
    blocks_up=[1, 1, 1],
    init_filters=16,
    in_channels=4,
    out_channels=3,
    dropout_prob=0.2,
).to(device)

# optimizer ã‚’ä½œæˆ
optimizer = torch.optim.Adam(
    model.parameters(),
    config.initial_learning_rate,
    weight_decay=config.weight_decay,
)

# å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’ä½œæˆ
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=config.max_train_epochs
)
```

`monai.losses` API ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«ã® `DiceLoss` ã‚’æå¤±é–¢æ•°ã¨ã—ã¦å®šç¾©ã—ã€å¯¾å¿œã™ã‚‹ãƒ€ã‚¤ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ `monai.metrics` API ã‚’ä½¿ã£ã¦å®šç¾©ã—ã¾ã™ã€‚

```python
loss_function = DiceLoss(
    smooth_nr=config.dice_loss_smoothen_numerator,
    smooth_dr=config.dice_loss_smoothen_denominator,
    squared_pred=config.dice_loss_squared_prediction,
    to_onehot_y=config.dice_loss_target_onehot,
    sigmoid=config.dice_loss_apply_sigmoid,
)

dice_metric = DiceMetric(include_background=True, reduction="mean")
dice_metric_batch = DiceMetric(include_background=True, reduction="mean_batch")
post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])

# è‡ªå‹•æ··åˆç²¾åº¦ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–
scaler = torch.cuda.amp.GradScaler()
torch.backends.cudnn.benchmark = True
```

æ··åˆç²¾åº¦æ¨è«–ã®ãŸã‚ã®å°ã•ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’å®šç¾©ã—ã¾ã™ã€‚ã“ã‚Œã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã®æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—ãŠã‚ˆã³è¨“ç·´å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹éš›ã«å½¹ç«‹ã¡ã¾ã™ã€‚

```python
def inference(model, input):
    def _compute(input):
        return sliding_window_inference(
            inputs=input,
            roi_size=(240, 240, 160),
            sw_batch_size=1,
            predictor=model,
            overlap=0.5,
        )

    with torch.cuda.amp.autocast():
        return _compute(input)
```

## ğŸš Training and Validation

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å‰ã«ã€`wandb.log()` ã‚’ä½¿ç”¨ã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼å®Ÿé¨“ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã®ãŸã‚ã«å¾Œã§ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’å®šç¾©ã—ã¾ã™ã€‚

```python
wandb.define_metric("epoch/epoch_step")
wandb.define_metric("epoch/*", step_metric="epoch/epoch_step")
wandb.define_metric("batch/batch_step")
wandb.define_metric("batch/*", step_metric="batch/batch_step")
wandb.define_metric("validation/validation_step")
wandb.define_metric("validation/*", step_metric="validation/validation_step")

batch_step = 0
validation_step = 0
metric_values = []
metric_values_tumor_core = []
metric_values_whole_tumor = []
metric_values_enhanced_tumor = []
```

### ğŸ­ Execute Standard PyTorch Training Loop

```python
# W&B Artifact ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å®šç¾©
artifact = wandb.Artifact(
    name=f"{wandb.run.id}-checkpoint", type="model"
)

epoch_progress_bar = tqdm(range(config.max_train_epochs), desc="Training:")

for epoch in epoch_progress_bar:
    model.train()
    epoch_loss = 0

    total_batch_steps = len(train_dataset) // train_loader.batch_size
    batch_progress_bar = tqdm(train_loader, total=total_batch_steps, leave=False)
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—
    for batch_data in batch_progress_bar:
        inputs, labels = (
            batch_data["image"].to(device),
            batch_data["label"].to(device),
        )
        optimizer.zero_grad()
        with torch.cuda.amp.autocast():
            outputs = model(inputs)
            loss = loss_function(outputs, labels)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        epoch_loss += loss.item()
        batch_progress_bar.set_description(f"train_loss: {loss.item():.4f}:")
        # ãƒãƒƒãƒå˜ä½ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æå¤±ã‚’W&Bã«ãƒ­ã‚°
        wandb.log({"batch/batch_step": batch_step, "batch/train_loss": loss.item()})
        batch_step += 1

    lr_scheduler.step()
    epoch_loss /= total_batch_steps
    # ã‚¨ãƒãƒƒã‚¯å˜ä½ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æå¤±ã¨å­¦ç¿’ç‡ã‚’W&Bã«ãƒ­ã‚°
    wandb.log(
        {
            "epoch/epoch_step": epoch,
            "epoch/mean_train_loss": epoch_loss,
            "epoch/learning_rate": lr_scheduler.get_last_lr()[0],
        }
    )
    epoch_progress_bar.set_description(f"Training: train_loss: {epoch_loss:.4f}:")

    # æ¤œè¨¼ãŠã‚ˆã³ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚¹ãƒ†ãƒƒãƒ—
    if (epoch + 1) % config.validation_intervals == 0:
        model.eval()
        with torch.no_grad():
            for val_data in val_loader:
                val_inputs, val_labels = (
                    val_data["image"].to(device),
                    val_data["label"].to(device),
                )
                val_outputs = inference(model, val_inputs)
                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]
                dice_metric(y_pred=val_outputs, y=val_labels)
                dice_metric_batch(y_pred=val_outputs, y=val_labels)

            metric_values.append(dice_metric.aggregate().item())
            metric_batch = dice_metric_batch.aggregate()
            metric_values_tumor_core.append(metric_batch[0].item())
            metric_values_whole_tumor.append(metric_batch[1].item())
            metric_values_enhanced_tumor.append(metric_batch[2].item())
            dice_metric.reset()
            dice_metric_batch.reset()

            checkpoint_path = os.path.join(config.checkpoint_dir, "model.pth")
            torch.save(model.state_dict(), checkpoint_path)
            
            # ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’W&Bã®Artifactã‚’ä½¿ç”¨ã—ã¦ãƒ­ã‚°ãŠã‚ˆã³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
            artifact.add_file(local_path=checkpoint_path)
            wandb.log_artifact(artifact, aliases=[f"epoch_{epoch}"])

            # æ¤œè¨¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’W&Bãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ãƒ­ã‚°
            wandb.log(
                {
                    "validation/validation_step": validation_step,
                    "validation/mean_dice": metric_values[-1],
                    "validation/mean_dice_tumor_core": metric_values_tumor_core[-1],
                    "validation/mean_dice_whole_tumor": metric_values_whole_tumor[-1],
                    "validation/mean_dice_enhanced_tumor": metric_values_enhanced_tumor[-1],
                }
            )
            validation_step += 1


# ã“ã®Artifactã®ãƒ­ã‚°ãŒå®Œäº†ã™ã‚‹ã®ã‚’å¾…ã¤
artifact.wait()
```

`wandb.log` ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ—ãƒ­ã‚»ã‚¹ã«é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã ã‘ã§ãªãã€W&Bãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä¸Šã®ã™ã¹ã¦ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆã“ã®å ´åˆã€ç§ãŸã¡ã®CPUã¨GPUï¼‰ã‚‚ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

| ![An example of training and validation process tracking on W&B.](@site/static/images/tutorials/monai/viz-3.gif) | 
|:--:| 
| **An example of training and validation process tracking on W&B.** |

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ãƒ­ã‚°ã«è¨˜éŒ²ã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ç•°ãªã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã«ã¯ã€W&B run ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚¿ãƒ–ã«ç§»å‹•ã—ã¾ã™ã€‚

| ![An example of model checkpoints logging and versioning on W&B.](@site/static/images/tutorials/monai/viz-4.gif) | 
|:--:| 
| **An example of model checkpoints logging and versioning on W&B.** |

## ğŸ”± Inference

Artifactsã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã€ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®ã©ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒæœ€é«˜ã®ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã§ã‚ã‚‹ã‹ã‚’é¸æŠã§ãã¾ã™ã€‚ã“ã®å ´åˆã€ã‚¨ãƒãƒƒã‚¯å˜ä½ã®å¹³å‡ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æå¤±ãŒã“ã‚Œã«å½“ãŸã‚Šã¾ã™ã€‚ã¾ãŸã€ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®å…¨ãƒªãƒãƒ¼ã‚¸ã‚’æ¢ç´¢ã—ã€å¿…è¦ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

| ![An example of model artifact tracking on W&B.](@site/static/images/tutorials/monai/viz-5.gif) | 
|:--:| 
| **An example of model artifact tracking on W&B.** |

ã‚¨ãƒãƒƒã‚¯å˜ä½ã®å¹³å‡ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æå¤±ãŒæœ€ã‚‚è‰¯ã„ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—ã—ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¹ãƒ†ãƒ¼ãƒˆè¾æ›¸ã‚’ãƒ¢ãƒ‡ãƒ«ã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```python
model_artifact = wandb.use_artifact(
    "geekyrakshit/monai-brain-tumor-segmentation/d5ex6n4a-checkpoint:v49",
    type="model",
)
model_artifact_dir = model_artifact.download()
model.load_state_dict(torch.load(os.path.join(model_artifact_dir, "model.pth")))
model.eval()
```

### ğŸ“¸ Visualizing Predictions and Comparing with the Ground Truth Labels

å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæœã‚’å¯è¦–åŒ–ã—ã€å¯¾å¿œã™ã‚‹æ­£è§£ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã¨æ¯”è¼ƒã™ã‚‹ãŸã‚ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚

```python
def log_predictions_into_tables(
    sample_image: np.array,
    sample_label: np.array,
    predicted_label: np.array,
    split: str = None,
    data_idx: int = None,
    table: wandb.Table = None,
):
    num_channels, _, _, num_slices = sample_image.shape
    with tqdm(total=num_slices, leave=False) as progress_bar:
        for slice_idx in range(num_slices):
            wandb_images = []
            for channel_idx in range(num_channels):
                wandb_images += [
                    wandb.Image(
                        sample_image[channel_idx, :, :, slice_idx],
                        masks={
                            "ground-truth/Tumor-Core": {
                                "mask_data": sample_label[0, :, :, slice_idx],
                                "class_labels": {0: "background", 1: "Tumor Core"},
                            },
                            "prediction/Tumor-Core": {
                                "mask_data": predicted_label[0, :, :, slice_idx] * 2,
                                "class_labels": {0: "background", 2: "Tumor Core"},
                            },
                        },
                    ),
                    wandb.Image(
                        sample_image[channel_idx, :, :, slice_idx],
                        masks={
                            "ground-truth/Whole-Tumor": {
                                "mask_data": sample_label[1, :, :, slice_idx],
                                "class_labels": {0: "background", 1: "Whole Tumor"},
                            },
                            "prediction/Whole-Tumor": {
                                "mask_data": predicted_label[1, :, :, slice_idx] * 2,
                                "class_labels": {0: "background", 2: "Whole Tumor"},
                            },
                        },
                    ),
                    wandb.Image(
                        sample_image[channel_idx, :, :, slice_idx],
                        masks={
                            "ground-truth/Enhancing-Tumor": {
                                "mask_data": sample_label[2, :, :, slice_idx],
                                "class_labels": {0: "background", 1: "Enhancing Tumor"},
                            },
                            "prediction/Enhancing-Tumor": {
                                "mask_data": predicted_label[2, :, :, slice_idx] * 2,
                                "class_labels": {0: "background", 2: "Enhancing Tumor"},
                            },
                        },
                    ),
                ]
            table.add_data(split, data_idx, slice_idx, *wandb_images)
            progress_bar.update(1)
    return table
```

äºˆæ¸¬çµæœã‚’äºˆæ¸¬ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ­ã‚°ã—ã¾ã™ã€‚

```python
# äºˆæ¸¬ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ
prediction_table = wandb.Table(
    columns=[
        "Split",
        "Data Index",
        "Slice Index",
        "Image-Channel-0/Tumor-Core",
        "Image-Channel-1/Tumor-Core",
        "Image-Channel-2/Tumor-Core",
        "Image-Channel-3/Tumor-Core",
        "Image-Channel-0/Whole-Tumor",
        "Image-Channel-1/Whole-Tumor",
        "Image-Channel-2/Whole-Tumor",
        "Image-Channel-3/Whole-Tumor",
        "Image-Channel-0/Enhancing-Tumor",
        "Image-Channel-1/Enhancing-Tumor",
        "Image-Channel-2/Enhancing-Tumor",
        "Image-Channel-3/Enhancing-Tumor",
    ]
)

# æ¨è«–ãŠã‚ˆã³å¯è¦–åŒ–ã‚’å®Ÿè¡Œ
with torch.no_grad():
    config.max_prediction_images_visualized
    max_samples = (
        min(config.max_prediction_images_visualized, len(val_dataset))
        if config.max_prediction_images_visualized > 0
        else len(val_dataset)
    )
    progress_bar = tqdm(
        enumerate(val_dataset[:max_samples]),
        total=max_samples,
        desc="Generating Predictions:",
    )
    for data_idx, sample in progress_bar:
        val_input = sample["image"].unsqueeze(0).to(device)
        val_output = inference(model, val_input)
        val_output = post_trans(val_output[0])
        prediction_table = log_predictions_into_tables(
            sample_image=sample["image"].cpu().numpy(),
            sample_label=sample["label"].cpu().numpy(),
            predicted_label=val_output.cpu().numpy(),
            data_idx=data_idx,
            split="validation",
            table=prediction_table,
        )

    wandb.log({"Predictions/Tumor-Segmentation-Data": prediction_table})


# å®Ÿé¨“ã‚’çµ‚äº†
wandb.finish()
```

ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã‚’ä½¿ç”¨ã—ã¦ã€å„ã‚¯ãƒ©ã‚¹ã®äºˆæ¸¬ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã¨æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’åˆ†æãŠã‚ˆã³æ¯”è¼ƒã—ã¾ã™ã€‚

| ![An example of predictions and ground-truth visualization on W&B.](@site/static/images/tutorials/monai/viz-6.gif) | 
|:--:| 
| **An example of predictions and ground-truth visualization on W&B.** |

## Acknowledgements and more resources

* [MONAI Tutorial: Brain tumor 3D segmentation with MONAI](https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb)
* [WandB Report: Brain Tumor Segmentation using MONAI and WandB](https://wandb.ai/geekyrakshit/brain-tumor-segmentation/reports/Brain-Tumor-Segmentation-using-MONAI-and-WandB---Vmlldzo0MjUzODIw)