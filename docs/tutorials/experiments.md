
# ì‹¤í—˜ ì¶”ì í•˜ê¸°

[**ì—¬ê¸°ì—ì„œ Colab ë…¸íŠ¸ë¶ìœ¼ë¡œ ì‹œë„í•´ë³´ì„¸ìš” â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_&_Biases.ipynb)

ë¹ ë¥¸ ì‹¤í—˜ì€ ë¨¸ì‹  ëŸ¬ë‹ì— ìˆì–´ ê¸°ë³¸ì ì…ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” W&Bë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ì„ ì¶”ì í•˜ê³  ì‹œê°í™”í•˜ì—¬ ê²°ê³¼ë¥¼ ë¹ ë¥´ê²Œ ì´í•´í•˜ê³  ë°˜ë³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¤© ì‹¤í—˜ì„ ìœ„í•œ ê³µìœ  ëŒ€ì‹œë³´ë“œ

ëª‡ ì¤„ì˜ ì½”ë“œë§Œìœ¼ë¡œ,
ì—¬ëŸ¬ë¶„ì€ í’ë¶€í•˜ê³  ìƒí˜¸ì‘ìš© ê°€ëŠ¥í•˜ë©° ê³µìœ í•  ìˆ˜ ìˆëŠ” ëŒ€ì‹œë³´ë“œë¥¼ ì–»ê²Œ ë©ë‹ˆë‹¤ [ì—¬ê¸°ì—ì„œ ì§ì ‘ í™•ì¸í•´ë³´ì„¸ìš”](https://wandb.ai/wandb/wandb_example).
![](https://i.imgur.com/Pell4Oo.png)

## ğŸ”’ ë°ì´í„° ë° ê°œì¸ ì •ë³´ ë³´í˜¸

ìš°ë¦¬ëŠ” ë³´ì•ˆì„ ë§¤ìš° ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ë©°, í´ë¼ìš°ë“œ í˜¸ìŠ¤íŒ… ëŒ€ì‹œë³´ë“œëŠ” ì•”í˜¸í™”ì— ëŒ€í•œ ì—…ê³„ í‘œì¤€ ëª¨ë²” ì‚¬ë¡€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê¸°ì—… í´ëŸ¬ìŠ¤í„°ë¥¼ ë– ë‚  ìˆ˜ ì—†ëŠ” ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, [ì˜¨-í”„ë ˆë¯¸ìŠ¤](https://docs.wandb.com/self-hosted) ì„¤ì¹˜ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ë°ì´í„°ë¥¼ ëª¨ë‘ ë‹¤ìš´ë¡œë“œí•˜ê³  ë‹¤ë¥¸ ë„êµ¬ë¡œ ë‚´ë³´ë‚´ëŠ” ê²ƒë„ ì‰½ìŠµë‹ˆë‹¤ â€” ì˜ˆë¥¼ ë“¤ì–´ Jupyter ë…¸íŠ¸ë¶ì—ì„œ ë§ì¶¤ ë¶„ì„ê³¼ ê°™ì€ ì‘ì—…ì…ë‹ˆë‹¤. ì—¬ê¸°ì—ì„œ [ìš°ë¦¬ APIì— ëŒ€í•œ ìì„¸í•œ ì •ë³´](https://docs.wandb.com/library/api)ë¥¼ í™•ì¸í•˜ì„¸ìš”.

---

## ğŸª„ `wandb` ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ë¡œê·¸ì¸


ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  ë¬´ë£Œ ê³„ì •ì— ë¡œê·¸ì¸í•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”.




```python
!pip install wandb -qU
```


```python
# W&B ê³„ì •ì— ë¡œê·¸ì¸
import wandb
wandb.login()
```

## ğŸ‘Ÿ ì‹¤í—˜ ì‹¤í–‰í•˜ê¸°
1ï¸âƒ£. **ìƒˆë¡œìš´ ì‹¤í–‰ì„ ì‹œì‘**í•˜ê³  ì¶”ì í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì „ë‹¬í•˜ì„¸ìš”

2ï¸âƒ£. **í•™ìŠµ ë˜ëŠ” í‰ê°€ì—ì„œ ë©”íŠ¸ë¦­ì„ ê¸°ë¡**í•˜ì„¸ìš”

3ï¸âƒ£. **ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ë¥¼ ì‹œê°í™”**í•˜ì„¸ìš”


```python
import random

# 5ê°œì˜ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í—˜ ì‹œì‘
total_runs = 5
for run in range(total_runs):
  # ğŸ 1ï¸âƒ£ ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¶”ì í•  ìƒˆë¡œìš´ ì‹¤í–‰ì„ ì‹œì‘í•©ë‹ˆë‹¤
  wandb.init(
      # ì´ ì‹¤í–‰ì´ ê¸°ë¡ë  í”„ë¡œì íŠ¸ ì„¤ì •
      project="basic-intro", 
      # ì‹¤í–‰ ì´ë¦„ì„ ì „ë‹¬í•©ë‹ˆë‹¤ (ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë¬´ì‘ìœ„ë¡œ í• ë‹¹ë©ë‹ˆë‹¤, ì˜ˆ: sunshine-lollypop-10)
      name=f"experiment_{run}", 
      # í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì‹¤í–‰ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì 
      config={
      "learning_rate": 0.02,
      "architecture": "CNN",
      "dataset": "CIFAR-100",
      "epochs": 10,
      })
  
  # ì´ ê°„ë‹¨í•œ ë¸”ë¡ì€ ë©”íŠ¸ë¦­ì„ ê¸°ë¡í•˜ëŠ” í•™ìŠµ ë£¨í”„ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤
  epochs = 10
  offset = random.random() / 5
  for epoch in range(2, epochs):
      acc = 1 - 2 ** -epoch - random.random() / epoch - offset
      loss = 2 ** -epoch + random.random() / epoch + offset
      
      # ğŸ 2ï¸âƒ£ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ W&Bì— ë©”íŠ¸ë¦­ì„ ê¸°ë¡í•©ë‹ˆë‹¤
      wandb.log({"acc": acc, "loss": loss})
      
  # ì‹¤í–‰ì„ ì™„ë£Œë¡œ í‘œì‹œí•©ë‹ˆë‹¤
  wandb.finish()
```

3ï¸âƒ£ ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ìœ„ì— ìˆëŠ” ğŸ‘† wandb ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ ëŒ€í™”í˜• ëŒ€ì‹œë³´ë“œë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ğŸ”¥ ê°„ë‹¨í•œ Pytorch ì‹ ê²½ë§

ğŸ’ª ì´ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ ê°„ë‹¨í•œ MNIST ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµì‹œí‚¤ê³ , í”„ë¡œì íŠ¸ í˜ì´ì§€ ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ W&B í”„ë¡œì íŠ¸ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ê²°ê³¼ê°€ ìŠ¤íŠ¸ë¦¬ë°ë˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì„¸ìš”.


`wandb`ì—ì„œì˜ ëª¨ë“  ì‹¤í–‰ì€ ìë™ì ìœ¼ë¡œ [ë©”íŠ¸ë¦­](https://docs.wandb.ai/ref/app/pages/run-page#charts-tab),
[ì‹œìŠ¤í…œ ì •ë³´](https://docs.wandb.ai/ref/app/pages/run-page#system-tab),
[í•˜ì´í¼íŒŒë¼ë¯¸í„°](https://docs.wandb.ai/ref/app/pages/run-page#overview-tab),
[í„°ë¯¸ë„ ì¶œë ¥](https://docs.wandb.ai/ref/app/pages/run-page#logs-tab)ì„ ê¸°ë¡í•˜ë©°,
ëª¨ë¸ ì…ë ¥ê³¼ ì¶œë ¥ì´ ìˆëŠ” [ëŒ€í™”í˜• í…Œì´ë¸”](https://docs.wandb.ai/guides/tables)ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ë°ì´í„°ë¡œë” ì„¤ì •

ì´ ì˜ˆì œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤. Google Colabì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì´ë¯¸ ì‚¬ì „ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 


```python
!pip install torch torchvision
```


```python
import wandb
import math
import random
import torch, torchvision
import torch.nn as nn
import torchvision.transforms as T

device = "cuda:0" if torch.cuda.is_available() else "cpu"

def get_dataloader(is_train, batch_size, slice=5):
    "í•™ìŠµìš© ë°ì´í„°ë¡œë”ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤"
    full_dataset = torchvision.datasets.MNIST(root=".", train=is_train, transform=T.ToTensor(), download=True)
    sub_dataset = torch.utils.data.Subset(full_dataset, indices=range(0, len(full_dataset), slice))
    loader = torch.utils.data.DataLoader(dataset=sub_dataset, 
                                         batch_size=batch_size, 
                                         shuffle=True if is_train else False, 
                                         pin_memory=True, num_workers=2)
    return loader

def get_model(dropout):
    "ê°„ë‹¨í•œ ëª¨ë¸"
    model = nn.Sequential(nn.Flatten(),
                         nn.Linear(28*28, 256),
                         nn.BatchNorm1d(256),
                         nn.ReLU(),
                         nn.Dropout(dropout),
                         nn.Linear(256,10)).to(device)
    return model

def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):
    "ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê³„ì‚°í•˜ê³  wandb.Tableì„ ê¸°ë¡í•˜ì—¬ ë¡œê·¸í•©ë‹ˆë‹¤"
    model.eval()
    val_loss = 0.
    with torch.inference_mode():
        correct = 0
        for i, (images, labels) in enumerate(valid_dl):
            images, labels = images.to(device), labels.to(device)

            # ìˆœë°©í–¥ ì „ë‹¬ â¡
            outputs = model(images)
            val_loss += loss_func(outputs, labels)*labels.size(0)

            # ì •í™•ë„ ê³„ì‚° ë° ì¶•ì 
            _, predicted = torch.max(outputs.data, 1)
            correct += (predicted == labels).sum().item()

            # ëŒ€ì‹œë³´ë“œì— í•œ ë°°ì¹˜ì˜ ì´ë¯¸ì§€ë¥¼ ë¡œê·¸í•˜ë©°, í•­ìƒ ê°™ì€ batch_idxì…ë‹ˆë‹¤.
            if i==batch_idx and log_images:
                log_image_table(images, predicted, labels, outputs.softmax(dim=1))
    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)

def log_image_table(images, predicted, labels, probs):
    "ì´ë¯¸ì§€, ë ˆì´ë¸” ë° ì˜ˆì¸¡ì„ í¬í•¨í•˜ëŠ” wandb.Tableì„ ë¡œê·¸í•©ë‹ˆë‹¤"
    # ğŸ ì´ë¯¸ì§€, ë ˆì´ë¸”, ì˜ˆì¸¡ì„ ë¡œê·¸í•˜ê¸° ìœ„í•œ wandb Tableì„ ìƒì„±í•©ë‹ˆë‹¤
    table = wandb.Table(columns=["image", "pred", "target"]+[f"score_{i}" for i in range(10)])
    for img, pred, targ, prob in zip(images.to("cpu"), predicted.to("cpu"), labels.to("cpu"), probs.to("cpu")):
        table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())
    wandb.log({"predictions_table":table}, commit=False)
```

## ëª¨ë¸ í•™ìŠµí•˜ê¸°


```python
# ë‹¤ì–‘í•œ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ì„ ì‹œë„í•˜ë©° 5ê°œì˜ ì‹¤í—˜ì„ ì‹œì‘í•©ë‹ˆë‹¤
for _ in range(5):
    # ğŸ wandb ì‹¤í–‰ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤
    wandb.init(
        project="pytorch-intro",
        config={
            "epochs": 10,
            "batch_size": 128,
            "lr": 1e-3,
            "dropout": random.uniform(0.01, 0.80),
            })
    
    # ì„¤ì •ì„ ë³µì‚¬í•©ë‹ˆë‹¤
    config = wandb.config

    # ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤
    train_dl = get_dataloader(is_train=True, batch_size=config.batch_size)
    valid_dl = get_dataloader(is_train=False, batch_size=2*config.batch_size)
    n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)
    
    # ê°„ë‹¨í•œ MLP ëª¨ë¸
    model = get_model(config.dropout)

    # ì†ì‹¤ê³¼ ì˜µí‹°ë§ˆì´ì €ë¥¼ ë§Œë“­ë‹ˆë‹¤
    loss_func = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)

   # í•™ìŠµ
    example_ct = 0
    step_ct = 0
    for epoch in range(config.epochs):
        model.train()
        for step, (images, labels) in enumerate(train_dl):
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            train_loss = loss_func(outputs, labels)
            optimizer.zero_grad()
            train_loss.backward()
            optimizer.step()
            
            example_ct += len(images)
            metrics = {"train/train_loss": train_loss, 
                       "train/epoch": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, 
                       "train/example_ct": example_ct}
            
            if step + 1 < n_steps_per_epoch:
                # ğŸ í•™ìŠµ ë©”íŠ¸ë¦­ì„ wandbì— ê¸°ë¡í•©ë‹ˆë‹¤
                wandb.log(metrics)
                
            step_ct += 1

        val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=(epoch==(config.epochs-1)))

        # ğŸ í•™ìŠµ ë° ê²€ì¦ ë©”íŠ¸ë¦­ì„ wandbì— ê¸°ë¡í•©ë‹ˆë‹¤
        val_metrics = {"val/val_loss": val_loss, 
                       "val/val_accuracy": accuracy}
        wandb.log({**metrics, **val_metrics})
        
        print(f"Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, Accuracy: {accuracy:.2f}")

    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ê°€ ìˆì—ˆë‹¤ë©´, ì´ë ‡ê²Œ ìš”ì•½ ë©”íŠ¸ë¦­ìœ¼ë¡œ ê¸°ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    wandb.summary['test_accuracy'] = 0.8

    # ğŸ wandb ì‹¤í–‰ì„ ë§ˆì¹©ë‹ˆë‹¤
    wandb.finish()
```

ì´ì œ wandbë¥¼ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ ëª¨ë¸ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤! ğŸ‘† ìœ„ì˜ wandb ë§í¬ë¥¼ í´ë¦­í•˜ì—¬ ë©”íŠ¸ë¦­ì„ í™•ì¸í•˜ì„¸ìš”

# ğŸ”” W&B ì•Œë¦¼ ì‹œë„í•˜ê¸°

**[W&B ì•Œë¦¼](https://docs.wandb.ai/guides/track/alert)**ì€ Python ì½”ë“œì—ì„œ íŠ¸ë¦¬ê±°ëœ ê²½ê³ ë¥¼ Slackì´ë‚˜ ì´ë©”ì¼ë¡œ ë³´ë‚¼ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ì½”ë“œì—ì„œ Slackì´ë‚˜ ì´ë©”ì¼ ê²½ê³ ë¥¼ íŠ¸ë¦¬ê±°í•˜ê³  ì‹¶ì€ ì²« ë²ˆì§¸ ì‹œë„ì—ëŠ” 2ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:

1) W&B [ì‚¬ìš©ì ì„¤ì •](https://wandb.ai/settings)ì—ì„œ ì•Œë¦¼ì„ ì¼œì„¸ìš”

2) ì½”ë“œì— `wandb.alert()`ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:

```python
wandb.alert(
    title="ì •í™•ë„ ë‚®ìŒ", 
    text=f"ì •í™•ë„ê°€ í—ˆìš© ê°€ëŠ¥í•œ ì„ê³„ê°’ ì•„ë˜ì…ë‹ˆë‹¤"
)
```

**[W&B ì•Œë¦¼](https://docs.wandb.ai/guides/track/alert)**ì— ëŒ€í•œ ì „ì²´ ë¬¸ì„œì—ì„œ `wandb.alert`ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ í™•ì¸í•˜ì„¸ìš”


```python
# wandb ì‹¤í–‰ì„ ì‹œì‘í•©ë‹ˆë‹¤
wandb.init(project="pytorch-intro")

# ëª¨ë¸ í•™ìŠµ ë£¨í”„ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤
acc_threshold = 0.3
for training_step in range(1000):

    # ì •í™•ë„ë¥¼ ìœ„í•œ ì„ì˜ì˜ ìˆ«ì ìƒì„±
    accuracy = round(random.random() + random.random(), 3)
    print(f'Accuracy is: {accuracy}, {acc_threshold}')
    
    # ğŸ wandbì— ì •í™•ë„ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤
    wandb.log({"Accuracy": accuracy})

    # ğŸ”” ì •í™•ë„ê°€ ì„ê³„ê°’ ì´í•˜ì¸ ê²½ìš° W&B ì•Œë¦¼ì„ ë°œìƒì‹œí‚¤ê³  ì‹¤í–‰ì„ ì¤‘ì§€í•©ë‹ˆë‹¤
    if accuracy <= acc_threshold:
        # ğŸ wandb ì•Œë¦¼ì„ ë³´ëƒ…ë‹ˆë‹¤
        wandb.alert(
            title='Low Accuracy',
            text=f'Accuracy {accuracy} at step {training_step} is below the acceptable theshold, {acc_threshold}',
        )
        print('Alert triggered')
        break

# ì‹¤í–‰ì„ ë§ˆì³¤ìŒì„ í‘œì‹œí•©ë‹ˆë‹¤ (ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œ ìœ ìš©í•©ë‹ˆë‹¤)
wandb.finish()
```

# ë‹¤ìŒ ë‹¨ê³„ëŠ”?
ë‹¤ìŒ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” W&B í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì˜ˆì¸¡ì„ ë³´ê³  ë¶„ì„í•˜ëŠ” ë°©ë²•ì„ ë°°ìš¸ ê²ƒì…ë‹ˆë‹¤:

## ğŸ‘‰ [ëª¨ë¸ ì˜ˆì¸¡ ë³´ê¸° ë° ë¶„ì„í•˜ê¸°](tables)