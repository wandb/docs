
# å®Ÿé¨“ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹

[**Colab ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§è©¦ã™ â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_&_Biases.ipynb)

æ€¥é€Ÿãªå®Ÿé¨“ã¯æ©Ÿæ¢°å­¦ç¿’ã«ã¨ã£ã¦åŸºæœ¬çš„ã§ã™ã€‚ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€W&B ã‚’ä½¿ç”¨ã—ã¦å®Ÿé¨“ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãŠã‚ˆã³å¯è¦–åŒ–ã—ã€çµæœã‚’è¿…é€Ÿã«åå¾©ã—ç†è§£ã—ã¾ã™ã€‚

## ğŸ¤© å®Ÿé¨“ã®ãŸã‚ã®å…±æœ‰ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ã€
[ã‚ãªãŸè‡ªèº«ã§ã“ã“ã§è¦‹ã‚‰ã‚Œã‚‹](https://wandb.ai/wandb/wandb_example)è±Šå¯Œã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªå…±æœ‰å¯èƒ½ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’æ‰‹ã«å…¥ã‚Œã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
![](https://i.imgur.com/Pell4Oo.png)

## ğŸ”’ ãƒ‡ãƒ¼ã‚¿ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼

ç§ãŸã¡ã¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’éå¸¸ã«é‡è¦–ã—ã¦ãŠã‚Šã€ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ›ã‚¹ãƒˆã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¯æš—å·åŒ–ã®æ¥­ç•Œæ¨™æº–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’å¤–éƒ¨ã«å‡ºã‚‰ã‚Œãªã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‰±ã£ã¦ã„ã‚‹å ´åˆã¯ã€[ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹](https://docs.wandb.com/self-hosted)ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚‚åˆ©ç”¨å¯èƒ½ã§ã™ã€‚

ã¾ãŸã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç°¡å˜ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä»–ã®ãƒ„ãƒ¼ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ â€” Jupyterãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã®ã‚«ã‚¹ã‚¿ãƒ åˆ†æã®ã‚ˆã†ã«ã€‚ã“ã¡ã‚‰ã«[APIã«ã¤ã„ã¦ã®è©³ç´°](https://docs.wandb.com/library/api)ãŒã‚ã‚Šã¾ã™ã€‚

---

## ğŸª„ `wandb` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹

ã¾ãšã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ç„¡æ–™ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚

```python
!pip install wandb -qU
```

```python
# W&B ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³
import wandb
wandb.login()
```

## ğŸ‘Ÿ å®Ÿé¨“ã‚’å®Ÿè¡Œã™ã‚‹
1ï¸âƒ£. **æ–°ã—ã„ run ã‚’é–‹å§‹**ã—ã€ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’æ¸¡ã™

2ï¸âƒ£. **ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¾ãŸã¯è©•ä¾¡ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°**ã™ã‚‹

3ï¸âƒ£. **çµæœã‚’ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§å¯è¦–åŒ–**ã™ã‚‹

```python
import random

# 5ã¤ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã‚’é–‹å§‹
total_runs = 5
for run in range(total_runs):
  # ğŸ 1ï¸âƒ£ ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«æ–°ã—ã„ run ã‚’é–‹å§‹
  wandb.init(
      # ã“ã® run ã‚’ãƒ­ã‚°ã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è¨­å®š
      project="basic-intro", 
      # run ã®åå‰ã‚’è¨­å®šï¼ˆã•ã‚‚ãªã‘ã‚Œã°ãƒ©ãƒ³ãƒ€ãƒ ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¾ã™ã€‚ä¾‹: sunshine-lollypop-10ï¼‰
      name=f"experiment_{run}", 
      # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã¨ run ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
      config={
      "learning_rate": 0.02,
      "architecture": "CNN",
      "dataset": "CIFAR-100",
      "epochs": 10,
      })

  # ã“ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ–ãƒ­ãƒƒã‚¯ã¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã¾ã™
  epochs = 10
  offset = random.random() / 5
  for epoch in range(2, epochs):
      acc = 1 - 2 ** -epoch - random.random() / epoch - offset
      loss = 2 ** -epoch + random.random() / epoch + offset
      
      # ğŸ 2ï¸âƒ£ ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰ W&B ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°
      wandb.log({"acc": acc, "loss": loss})
      
  # run ã‚’çµ‚äº†ã¨ã—ã¦ãƒãƒ¼ã‚¯
  wandb.finish()
```

3ï¸âƒ£ ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ä¸Šè¨˜ã®ã„ãšã‚Œã‹ã® wandb ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

# ğŸ”¥ ã‚·ãƒ³ãƒ—ãƒ«ãª Pytorch ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

ğŸ’ª ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’èµ°ã‚‰ã›ã¦ã‚·ãƒ³ãƒ—ãƒ«ãª MNIST ã‚¯ãƒ©ã‚¹åˆ†é¡å™¨ã‚’è¨“ç·´ã—ã€çµæœãŒ W&B ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã•ã‚Œã‚‹ã®ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚

`wandb` ã®ã„ã‹ãªã‚‹ run ã«ãŠã„ã¦ã‚‚è‡ªå‹•çš„ã«[ãƒ¡ãƒˆãƒªã‚¯ã‚¹](https://docs.wandb.ai/ref/app/pages/run-page#charts-tab)ã€[ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±](https://docs.wandb.ai/ref/app/pages/run-page#system-tab)ã€[ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼](https://docs.wandb.ai/ref/app/pages/run-page#overview-tab)ã€[ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›](https://docs.wandb.ai/ref/app/pages/run-page#logs-tab) ãŒãƒ­ã‚°ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ã¨å‡ºåŠ›ãŒå«ã¾ã‚ŒãŸ[ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªè¡¨](https://docs.wandb.ai/guides/tables)ã‚’è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

## Dataloader ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

ã“ã®ä¾‹ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€PyTorch ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚Google Colab ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€ã™ã§ã«ãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™ã€‚

```python
!pip install torch torchvision
```

```python
import wandb
import math
import random
import torch, torchvision
import torch.nn as nn
import torchvision.transforms as T

device = "cuda:0" if torch.cuda.is_available() else "cpu"

def get_dataloader(is_train, batch_size, slice=5):
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å–å¾—"
    full_dataset = torchvision.datasets.MNIST(root=".", train=is_train, transform=T.ToTensor(), download=True)
    sub_dataset = torch.utils.data.Subset(full_dataset, indices=range(0, len(full_dataset), slice))
    loader = torch.utils.data.DataLoader(dataset=sub_dataset, 
                                         batch_size=batch_size, 
                                         shuffle=True if is_train else False, 
                                         pin_memory=True, num_workers=2)
    return loader

def get_model(dropout):
    "ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«"
    model = nn.Sequential(nn.Flatten(),
                         nn.Linear(28*28, 256),
                         nn.BatchNorm1d(256),
                         nn.ReLU(),
                         nn.Dropout(dropout),
                         nn.Linear(256,10)).to(device)
    return model

def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):
    "ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è©•ä¾¡ã—ã€wandb.Table ã«ãƒ­ã‚°"
    model.eval()
    val_loss = 0.
    with torch.inference_mode():
        correct = 0
        for i, (images, labels) in enumerate(valid_dl):
            images, labels = images.to(device), labels.to(device)

            # Forward pass â¡
            outputs = model(images)
            val_loss += loss_func(outputs, labels)*labels.size(0)

            # ç²¾åº¦ã‚’è¨ˆç®—ã—ã¦è“„ç©
            _, predicted = torch.max(outputs.data, 1)
            correct += (predicted == labels).sum().item()

            # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«1ãƒãƒƒãƒã®ç”»åƒã‚’ãƒ­ã‚°ã€å¸¸ã«åŒã˜ batch_idx
            if i==batch_idx and log_images:
                log_image_table(images, predicted, labels, outputs.softmax(dim=1))
    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)

def log_image_table(images, predicted, labels, probs):
    "ï¼ˆç”»åƒã€äºˆæ¸¬ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã€ã‚¹ã‚³ã‚¢ï¼‰ã‚’å«ã‚€ wandb.Table ã‚’ãƒ­ã‚°"
    # ğŸ ç”»åƒã€ãƒ©ãƒ™ãƒ«ã€äºˆæ¸¬ã‚’ãƒ­ã‚°ã™ã‚‹ãŸã‚ã® wandb Table ã‚’ä½œæˆ
    table = wandb.Table(columns=["image", "pred", "target"]+[f"score_{i}" for i in range(10)])
    for img, pred, targ, prob in zip(images.to("cpu"), predicted.to("cpu"), labels.to("cpu"), probs.to("cpu")):
        table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())
    wandb.log({"predictions_table":table}, commit=False)
```

## ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

```python
# 5ã¤ã®å®Ÿé¨“ã‚’é–‹å§‹ã—ã€ç•°ãªã‚‹ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ã‚’è©¦ã™
for _ in range(5):
    # ğŸ wandb run ã‚’åˆæœŸåŒ–
    wandb.init(
        project="pytorch-intro",
        config={
            "epochs": 10,
            "batch_size": 128,
            "lr": 1e-3,
            "dropout": random.uniform(0.01, 0.80),
            })
    
    # ã‚³ãƒ³ãƒ•ã‚£ã‚°ã‚’ã‚³ãƒ”ãƒ¼
    config = wandb.config

    # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    train_dl = get_dataloader(is_train=True, batch_size=config.batch_size)
    valid_dl = get_dataloader(is_train=False, batch_size=2*config.batch_size)
    n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªMLPãƒ¢ãƒ‡ãƒ«
    model = get_model(config.dropout)

    # æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
    loss_func = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)

   # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
    example_ct = 0
    step_ct = 0
    for epoch in range(config.epochs):
        model.train()
        for step, (images, labels) in enumerate(train_dl):
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            train_loss = loss_func(outputs, labels)
            optimizer.zero_grad()
            train_loss.backward()
            optimizer.step()
            
            example_ct += len(images)
            metrics = {"train/train_loss": train_loss, 
                       "train/epoch": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, 
                       "train/example_ct": example_ct}
            
            if step + 1 < n_steps_per_epoch:
                # ğŸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ wandb ã«ãƒ­ã‚°
                wandb.log(metrics)
                
            step_ct += 1

        val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=(epoch==(config.epochs-1)))

        # ğŸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¤œè¨¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ wandb ã«ãƒ­ã‚°
        val_metrics = {"val/val_loss": val_loss, 
                       "val/val_accuracy": accuracy}
        wandb.log({**metrics, **val_metrics})
        
        print(f"Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, Accuracy: {accuracy:.2f}")

    # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆãŒã‚ã‚Œã°ã€ã“ã‚ŒãŒ Summary ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã—ã¦ãƒ­ã‚°ã™ã‚‹æ–¹æ³•ã§ã™
    wandb.summary['test_accuracy'] = 0.8

    # ğŸ wandb run ã‚’çµ‚äº†
    wandb.finish()
```

ã“ã‚Œã§ wandb ã‚’ä½¿ã£ã¦æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¾ã—ãŸï¼ ğŸ‘†ä¸Šè¨˜ã® wandb ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

# ğŸ”” W&B Alerts ã‚’è©¦ã—ã¦ã¿ã‚‹

**[W&B Alerts](https://docs.wandb.ai/guides/track/alert)**ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€Python ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ãƒˆãƒªã‚¬ãƒ¼ã•ã‚ŒãŸã‚¢ãƒ©ãƒ¼ãƒˆã‚’ Slack ã‚„ãƒ¡ãƒ¼ãƒ«ã«é€ä¿¡ã§ãã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ãƒˆãƒªã‚¬ãƒ¼ã•ã‚ŒãŸ Slack ã‚„ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡ã—ãŸã„å ´åˆã€åˆã‚ã¦è¡Œã†éš›ã«ã¯æ¬¡ã® 2 ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

1) W&B ã® [ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š](https://wandb.ai/settings)ã§ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ã‚ªãƒ³ã«ã™ã‚‹

2) ã‚³ãƒ¼ãƒ‰ã« `wandb.alert()`ã‚’è¿½åŠ ã™ã‚‹:

```python
wandb.alert(
    title="Low accuracy", 
    text=f"Accuracy is below the acceptable threshold"
)
```

ä»¥ä¸‹ã®æœ€å°ä¾‹ã‚’å‚ç…§ã—ã¦ `wandb.alert` ã®ä½¿ã„æ–¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚**[W&B Alerts](https://docs.wandb.ai/guides/track/alert)** ã®å®Œå…¨ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”è¦§ã„ãŸã ã‘ã¾ã™ã€‚

```python
# wandb run ã‚’é–‹å§‹
wandb.init(project="pytorch-intro")

# ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
acc_threshold = 0.3
for training_step in range(1000):

    # ç²¾åº¦ã®ãŸã‚ã®ãƒ©ãƒ³ãƒ€ãƒ æ•°ã‚’ç”Ÿæˆã™ã‚‹
    accuracy = round(random.random() + random.random(), 3)
    print(f'Accuracy is: {accuracy}, {acc_threshold}')
    
    # ğŸ ç²¾åº¦ã‚’ wandb ã«ãƒ­ã‚°
    wandb.log({"Accuracy": accuracy})

    # ğŸ”” ç²¾åº¦ãŒé–¾å€¤ã‚’ä¸‹å›ã£ãŸå ´åˆã€W&B ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ç™ºç«ã•ã›ã¦ run ã‚’åœæ­¢
    if accuracy <= acc_threshold:
        # ğŸ wandb ã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡
        wandb.alert(
            title='Low Accuracy',
            text=f'Accuracy {accuracy} at step {training_step} is below the acceptable theshold, {acc_threshold}',
        )
        print('Alert triggered')
        break

# run ã‚’çµ‚äº†ã¨ã—ã¦ãƒãƒ¼ã‚¯ï¼ˆJupyterãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯æœ‰ç”¨ï¼‰
wandb.finish()
```

# æ¬¡ã¯ï¼Ÿ
æ¬¡ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€W&B Tables ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’è¡¨ç¤ºãŠã‚ˆã³åˆ†æã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ï¼š
## ğŸ‘‰ [View & Analyze Model Predictions](tables)