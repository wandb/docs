
# TensorFlow

[**Try in a Colab Notebook here â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/tensorflow/Simple_TensorFlow_Integration.ipynb)

Weights & Biases ã‚’ä½¿ç”¨ã—ã¦ã€æ©Ÿæ¢°å­¦ç¿’å®Ÿé¨“ã®è¿½è·¡ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚

<div><img /></div>

<img src="http://wandb.me/mini-diagram" width="650" alt="Weights & Biases" />

<div><img /></div>

## ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã‚«ãƒãƒ¼ã™ã‚‹å†…å®¹

* Weights and Biases ã¨ TensorFlow ã®ç°¡å˜ãªã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹å®Ÿé¨“ç®¡ç†
* `keras.metrics` ã‚’ç”¨ã„ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹æ–¹æ³•
* `wandb.log` ã‚’ä½¿ã£ã¦ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã™ã‚‹æ–¹æ³•

## ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãª W&B ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¯ã“ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

![dashboard](/images/tutorials/tensorflow/dashboard.png)

**æ³¨æ„**: _Step_ ã§å§‹ã¾ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ W&B ã‚’æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã«çµ„ã¿è¾¼ã‚€ãŸã‚ã«å¿…è¦ãªéƒ¨åˆ†ã§ã™ã€‚ãã®ä»–ã®éƒ¨åˆ†ã¯æ¨™æº–çš„ãª MNIST ã®ä¾‹ã«éãã¾ã›ã‚“ã€‚

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.datasets import cifar10

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

# ğŸš€ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€ãƒ­ã‚°ã‚¤ãƒ³

## Step 0ï¸âƒ£: W&B ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹

```python
%%capture
!pip install wandb
```

## Step 1ï¸âƒ£: W&B ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹

```python
import wandb
from wandb.keras import WandbCallback

wandb.login()
```

> è£œè¶³: ã‚‚ã— W&B ã‚’åˆã‚ã¦åˆ©ç”¨ã™ã‚‹å ´åˆã‚„ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãªã„å ´åˆã¯ã€`wandb.login()` å®Ÿè¡Œå¾Œã«è¡¨ç¤ºã•ã‚Œã‚‹ãƒªãƒ³ã‚¯ã‹ã‚‰ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—/ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ç§»å‹•ã§ãã¾ã™ã€‚ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—ã¯ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã§ç°¡å˜ã«å®Œäº†ã—ã¾ã™ã€‚

# ğŸ‘©â€ğŸ³ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™

```python
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
BATCH_SIZE = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

# tf.data ã‚’ä½¿ã£ã¦å…¥åŠ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)

val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
val_dataset = val_dataset.batch(BATCH_SIZE)
```

# ğŸ§  ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã®å®šç¾©

```python
def make_model():
    inputs = keras.Input(shape=(784,), name="digits")
    x1 = keras.layers.Dense(64, activation="relu")(inputs)
    x2 = keras.layers.Dense(64, activation="relu")(x1)
    outputs = keras.layers.Dense(10, name="predictions")(x2)

    return keras.Model(inputs=inputs, outputs=outputs)
```

```python
def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)

    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))

    train_acc_metric.update_state(y, logits)

    return loss_value
```

```python
def test_step(x, y, model, loss_fn, val_acc_metric):
    val_logits = model(x, training=False)
    loss_value = loss_fn(y, val_logits)
    val_acc_metric.update_state(y, val_logits)

    return loss_value
```

## Step 2ï¸âƒ£: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã« `wandb.log` ã‚’è¿½åŠ ã™ã‚‹

```python
def train(train_dataset, val_dataset, model, optimizer,
          train_acc_metric, val_acc_metric,
          epochs=10, log_step=200, val_log_step=50):
  
    for epoch in range(epochs):
        print("\nStart of epoch %d" % (epoch,))

        train_loss = []   
        val_loss = []

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒƒãƒã‚’ç¹°ã‚Šè¿”ã—å‡¦ç†
        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
            loss_value = train_step(x_batch_train, y_batch_train, 
                                    model, optimizer, 
                                    loss_fn, train_acc_metric)
            train_loss.append(float(loss_value))

        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ
        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):
            val_loss_value = test_step(x_batch_val, y_batch_val, 
                                       model, loss_fn, 
                                       val_acc_metric)
            val_loss.append(float(val_loss_value))
            
        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º
        train_acc = train_acc_metric.result()
        print("Training acc over epoch: %.4f" % (float(train_acc),))

        val_acc = val_acc_metric.result()
        print("Validation acc: %.4f" % (float(val_acc),))

        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
        train_acc_metric.reset_states()
        val_acc_metric.reset_states()

        # â­: wandb.log ã‚’ä½¿ã£ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°
        wandb.log({'epochs': epoch,
                   'loss': np.mean(train_loss),
                   'acc': float(train_acc), 
                   'val_loss': np.mean(val_loss),
                   'val_acc': float(val_acc)})
```

# ğŸ‘Ÿ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ

## Step 3ï¸âƒ£: run ã‚’é–‹å§‹ã™ã‚‹ãŸã‚ã« `wandb.init` ã‚’å‘¼ã³å‡ºã™

ã“ã‚Œã«ã‚ˆã‚Šã€å®Ÿé¨“ã‚’é–‹å§‹ã—ã€ä¸€æ„ã®IDã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’æä¾›ã—ã¾ã™ã€‚

[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã“ã¡ã‚‰ $\rightarrow$](https://docs.wandb.com/library/init)

```python
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§è¨­å®šã‚’æŒ‡å®šã—ã¦ wandb ã‚’åˆæœŸåŒ–
# è¨­å®šå€¤ã‚’å¤‰æ›´ã—ã€wandb ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§çµæœã‚’ç¢ºèª
config = {
              "learning_rate": 0.001,
              "epochs": 10,
              "batch_size": 64,
              "log_step": 200,
              "val_log_step": 50,
              "architecture": "CNN",
              "dataset": "CIFAR-10"
           }

run = wandb.init(project='my-tf-integration', config=config)
config = wandb.config

# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
model = make_model()

# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
optimizer = keras.optimizers.SGD(learning_rate=config.learning_rate)
# æå¤±é–¢æ•°ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æº–å‚™
train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

train(train_dataset,
      val_dataset, 
      model,
      optimizer,
      train_acc_metric,
      val_acc_metric,
      epochs=config.epochs, 
      log_step=config.log_step, 
      val_log_step=config.val_log_step)

run.finish()  # Jupyter/Colab ã§ã¯ã€çµ‚äº†ã‚’çŸ¥ã‚‰ã›ã‚‹ãŸã‚ã«ï¼
```

# ğŸ‘€ çµæœã‚’è¦–è¦šåŒ–

ãƒ©ã‚¤ãƒ–çµæœã‚’è¦‹ã‚‹ã«ã¯ã€ä¸Šè¨˜ã® [**run ãƒšãƒ¼ã‚¸**](https://docs.wandb.ai/ref/app/pages/run-page) ã®ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

# ğŸ§¹ Sweep 101

Weights & Biases Sweeps ã‚’ä½¿ç”¨ã—ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®æœ€é©åŒ–ã‚’è‡ªå‹•åŒ–ã—ã€è€ƒãˆã‚‰ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã®ç©ºé–“ã‚’æ¢ç´¢ã—ã¾ã—ã‚‡ã†ã€‚

## [W&B Sweeps ã‚’ä½¿ç”¨ã—ãŸ TensorFlow ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–ã®è©³ç´°ã¯ã“ã¡ã‚‰ $\rightarrow$](http://wandb.me/tf-sweeps-colab)

### W&B Sweeps ä½¿ç”¨ã®åˆ©ç‚¹

* **è¿…é€Ÿãªè¨­å®š**: ã‚ãšã‹æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ W&B sweeps ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚
* **é€æ˜æ€§**: ä½¿ç”¨ã—ã¦ã„ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å¼•ç”¨ã—ã¦ãŠã‚Šã€[ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ã™](https://github.com/wandb/client/tree/master/wandb/sweeps)ã€‚
* **å¼·åŠ›**: sweeps ã¯å®Œå…¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãŠã‚ˆã³è¨­å®šå¯èƒ½ã§ã™ã€‚æ•°åå°ã®ãƒã‚·ãƒ³ã«ã‚ãŸã£ã¦ sweep ã‚’èµ·å‹•ã™ã‚‹ã®ã‚‚ã€ãƒ©ãƒƒãƒ—ãƒˆãƒƒãƒ—ã§ sweep ã‚’é–‹å§‹ã™ã‚‹ã®ã¨åŒã˜ãã‚‰ã„ç°¡å˜ã§ã™ã€‚

<img src="https://i.imgur.com/6eWHZhg.png" alt="Sweep Result" />

# ğŸ¨ ä¾‹ã®ã‚®ãƒ£ãƒ©ãƒªãƒ¼

W&B ã§è¿½è·¡ãƒ»å¯è¦–åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¾‹ã‚’ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã§ã”è¦§ãã ã•ã„ã€‚[Fully Connected â†’](https://wandb.me/fc)

# ğŸ“ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
1. **Projects**: è¤‡æ•°ã® run ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãƒ­ã‚°ã—ã¦æ¯”è¼ƒã—ã¾ã™ã€‚`wandb.init(project="project-name")`
2. **Groups**: è¤‡æ•°ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚„äº¤å·®æ¤œè¨¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®ãŸã‚ã«ã€å„ãƒ—ãƒ­ã‚»ã‚¹ã‚’ runs ã¨ã—ã¦ãƒ­ã‚°ã—ã€ä¸€æ‹¬ã‚Šã«ã—ã¾ã™ã€‚`wandb.init(group='experiment-1')`
3. **Tags**: ç¾åœ¨ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚„ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã«ã‚¿ã‚°ã‚’è¿½åŠ ã—ã¾ã™ã€‚
4. **Notes**: ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ¡ãƒ¢ã‚’æ›¸ãè¾¼ã¿ã€run é–“ã®å¤‰æ›´ã‚’è¿½è·¡ã—ã¾ã™ã€‚
5. **Reports**: åŒåƒšã¨é€²æ—ã‚’å…±æœ‰ã™ã‚‹ãŸã‚ã«ãƒ¬ãƒãƒ¼ãƒˆã‚’æ›¸ãè¾¼ã¿ã€ML ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

## ğŸ¤“ é«˜åº¦ãªè¨­å®š
1. [ç’°å¢ƒå¤‰æ•°](https://docs.wandb.com/library/environment-variables): ç’°å¢ƒå¤‰æ•°ã« API ã‚­ãƒ¼ã‚’è¨­å®šã—ã€ç®¡ç†ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚
2. [ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): `dryrun` ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã€å¾Œã§çµæœã‚’åŒæœŸã—ã¾ã™ã€‚
3. [ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹](https://docs.wandb.com/self-hosted): ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚¯ãƒ©ã‚¦ãƒ‰ã‚„ã‚¨ã‚¢ã‚®ãƒ£ãƒƒãƒ—ã‚µãƒ¼ãƒãƒ¼ã®ã‚¤ãƒ³ãƒ•ãƒ©ã« W&B ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚ã‚¢ã‚«ãƒ‡ãƒŸã‚¢ã‹ã‚‰ä¼æ¥­ãƒãƒ¼ãƒ ã¾ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
4. [Artifacts](http://wandb.me/artifacts-colab): ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½è·¡åŠã³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã—ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãªãŒã‚‰è‡ªå‹•çš„ã«å–å¾—ã—ã¾ã™ã€‚