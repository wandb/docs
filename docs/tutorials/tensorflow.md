
# TensorFlow

[**Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§è©¦ã™ â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/tensorflow/Simple_TensorFlow_Integration.ipynb)

Weights & Biasesã‚’ä½¿ç”¨ã—ã¦æ©Ÿæ¢°å­¦ç¿’å®Ÿé¨“ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã¾ã™ã€‚

<div><img /></div>

<img src="http://wandb.me/mini-diagram" width="650" alt="Weights & Biases" />

<div><img /></div>

## ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã‚«ãƒãƒ¼ã™ã‚‹å†…å®¹

* TensorFlowãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«Weights and Biasesã‚’ç°¡å˜ã«çµ±åˆã—ã¦å®Ÿé¨“ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ã¾ã™ã€‚
* `keras.metrics`ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
* ç‹¬è‡ªã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã§`wandb.log`ã‚’ä½¿ç”¨ã—ã¦ãã‚Œã‚‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã—ã¾ã™ã€‚

## ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªW&Bãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¯ã“ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

![dashboard](/images/tutorials/tensorflow/dashboard.png)

**æ³¨æ„**: _Step_ ã§å§‹ã¾ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€W&Bã‚’æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã«çµ±åˆã™ã‚‹ãŸã‚ã«å¿…è¦ãªã™ã¹ã¦ã§ã™ã€‚ãã‚Œä»¥å¤–ã¯æ¨™æº–çš„ãªMNISTã®ä¾‹ã§ã™ã€‚

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.datasets import cifar10

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

# ğŸš€ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€ãƒ­ã‚°ã‚¤ãƒ³

## Step 0ï¸âƒ£: W&Bã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```python
%%capture
!pip install wandb
```

## Step 1ï¸âƒ£: W&Bã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ãƒ­ã‚°ã‚¤ãƒ³

```python
import wandb
from wandb.keras import WandbCallback

wandb.login()
```

> ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒˆ: ã“ã‚ŒãŒåˆã‚ã¦ã®W&Bä½¿ç”¨ã‹ã€ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãªã„å ´åˆã€`wandb.login()`ã‚’å®Ÿè¡Œã—ãŸå¾Œã«è¡¨ç¤ºã•ã‚Œã‚‹ãƒªãƒ³ã‚¯ã‹ã‚‰ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—/ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«é€²ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—ã¯ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã§ç°¡å˜ã§ã™ã€‚

# ğŸ‘©â€ğŸ³ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™

```python
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã—ã¾ã™
BATCH_SIZE = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

# tf.dataã‚’ä½¿ç”¨ã—ã¦å…¥åŠ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)

val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
val_dataset = val_dataset.batch(BATCH_SIZE)
```

# ğŸ§  ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã®å®šç¾©

```python
def make_model():
    inputs = keras.Input(shape=(784,), name="digits")
    x1 = keras.layers.Dense(64, activation="relu")(inputs)
    x2 = keras.layers.Dense(64, activation="relu")(x1)
    outputs = keras.layers.Dense(10, name="predictions")(x2)

    return keras.Model(inputs=inputs, outputs=outputs)
```

```python
def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)

    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))

    train_acc_metric.update_state(y, logits)

    return loss_value
```

```python
def test_step(x, y, model, loss_fn, val_acc_metric):
    val_logits = model(x, training=False)
    loss_value = loss_fn(y, val_logits)
    val_acc_metric.update_state(y, val_logits)

    return loss_value
```

## Step 2ï¸âƒ£: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã«`wandb.log`ã‚’è¿½åŠ 

```python
def train(train_dataset, val_dataset, model, optimizer,
          train_acc_metric, val_acc_metric,
          epochs=10, log_step=200, val_log_step=50):
  
    for epoch in range(epochs):
        print("\nStart of epoch %d" % (epoch,))

        train_loss = []   
        val_loss = []

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒƒãƒã‚’ç¹°ã‚Šè¿”ã—ã¾ã™
        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
            loss_value = train_step(x_batch_train, y_batch_train, 
                                    model, optimizer, 
                                    loss_fn, train_acc_metric)
            train_loss.append(float(loss_value))

        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™
        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):
            val_loss_value = test_step(x_batch_val, y_batch_val, 
                                       model, loss_fn, 
                                       val_acc_metric)
            val_loss.append(float(val_loss_value))
            
        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤ºã—ã¾ã™
        train_acc = train_acc_metric.result()
        print("Training acc over epoch: %.4f" % (float(train_acc),))

        val_acc = val_acc_metric.result()
        print("Validation acc: %.4f" % (float(val_acc),))

        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚ã‚ã‚Šã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã™
        train_acc_metric.reset_states()
        val_acc_metric.reset_states()

        # â­: `wandb.log`ã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã—ã¾ã™
        wandb.log({'epochs': epoch,
                   'loss': np.mean(train_loss),
                   'acc': float(train_acc), 
                   'val_loss': np.mean(val_loss),
                   'val_acc': float(val_acc)})
```

# ğŸ‘Ÿ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ

## Step 3ï¸âƒ£: `wandb.init`ã‚’å‘¼ã³å‡ºã—ã¦Runã‚’é–‹å§‹

ã“ã‚Œã«ã‚ˆã‚Šã€å®Ÿé¨“ã‚’é–‹å§‹ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã€ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒæä¾›ã•ã‚Œã¾ã™ã€‚

[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã“ã¡ã‚‰ã‚’ãƒã‚§ãƒƒã‚¯ $\rightarrow$](https://docs.wandb.com/library/init)

```python
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§è¨­å®šå€¤ã‚’æŒ‡å®šã—ã¦wandbã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
# è¨­å®šå€¤ã‚’å¤‰æ›´ã—ã¦ã€wandbãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§çµæœã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚
config = {
              "learning_rate": 0.001,
              "epochs": 10,
              "batch_size": 64,
              "log_step": 200,
              "val_log_step": 50,
              "architecture": "CNN",
              "dataset": "CIFAR-10"
           }

run = wandb.init(project='my-tf-integration', config=config)
config = wandb.config

# ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
model = make_model()

# ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚
optimizer = keras.optimizers.SGD(learning_rate=config.learning_rate)
# æå¤±é–¢æ•°ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æº–å‚™ã—ã¾ã™ã€‚
train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

train(train_dataset,
      val_dataset, 
      model,
      optimizer,
      train_acc_metric,
      val_acc_metric,
      epochs=config.epochs, 
      log_step=config.log_step, 
      val_log_step=config.val_log_step)

run.finish()  # Jupyter/Colabã§ã¯ã€runã®çµ‚äº†ã‚’çŸ¥ã‚‰ã›ã¾ã™ï¼
```

# ğŸ‘€ çµæœã®å¯è¦–åŒ–

ãƒ©ã‚¤ãƒ–çµæœã‚’è¦‹ã‚‹ã«ã¯ã€ä¸Šè¨˜ã®[**runãƒšãƒ¼ã‚¸**](https://docs.wandb.ai/ref/app/pages/run-page)ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

# ğŸ§¹ Sweep 101

Weights & Biases Sweepsã‚’ä½¿ç”¨ã—ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®æœ€é©åŒ–ã‚’è‡ªå‹•åŒ–ã—ã€å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ç©ºé–“ã‚’æ¢ç´¢ã—ã¾ã™ã€‚

## [W&B Sweepsã‚’ä½¿ç”¨ã—ãŸTensorFlowã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–ã‚’ãƒã‚§ãƒƒã‚¯ $\rightarrow$](http://wandb.me/tf-sweeps-colab)

### W&B Sweepsã‚’ä½¿ç”¨ã™ã‚‹åˆ©ç‚¹

* **ã‚¯ã‚¤ãƒƒã‚¯ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã§W&B Sweepsã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚
* **é€æ˜æ€§**: ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã™ã¹ã¦å¼•ç”¨ã—ã€[ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹](https://github.com/wandb/client/tree/master/wandb/sweeps)ã§ã™ã€‚
* **å¼·åŠ›**: Sweepsã¯å®Œå…¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ã§æ§‹æˆå¯èƒ½ã§ã™ã€‚æ•°åå°ã®ãƒã‚·ãƒ³ã§Sweepã‚’é–‹å§‹ã™ã‚‹ã“ã¨ã‚‚ã€ãƒ©ãƒƒãƒ—ãƒˆãƒƒãƒ—ã§é–‹å§‹ã™ã‚‹ã®ã¨åŒã˜ãã‚‰ã„ç°¡å˜ã§ã™ã€‚

<img src="https://i.imgur.com/6eWHZhg.png" alt="Sweep Result" />

# ğŸ¨ ä¾‹ã®ã‚®ãƒ£ãƒ©ãƒªãƒ¼

W&Bã‚’ä½¿ã£ã¦ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãŠã‚ˆã³å¯è¦–åŒ–ã—ãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¾‹ã‚’ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã§ã”è¦§ãã ã•ã„ [Fully Connected â†’](https://wandb.me/fc)

# ğŸ“ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
1. **Projects**: è¤‡æ•°ã®Runã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãƒ­ã‚°ã—ã¦æ¯”è¼ƒã—ã¾ã™ã€‚`wandb.init(project="project-name")`
2. **Groups**: è¤‡æ•°ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚„äº¤å·®æ¤œè¨¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã”ã¨ã«ã€å„ãƒ—ãƒ­ã‚»ã‚¹ã‚’Runã¨ã—ã¦ãƒ­ã‚°ã—ã€ãã‚Œã‚‰ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¾ã™ã€‚`wandb.init(group='experiment-1')`
3. **Tags**: ç¾åœ¨ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚„ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã«ã‚¿ã‚°ã‚’è¿½åŠ ã—ã¾ã™ã€‚
4. **Notes**: ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ¡ãƒ¢ã‚’å…¥åŠ›ã—ã¦ã€Runé–“ã®å¤‰æ›´ã‚’è¿½è·¡ã—ã¾ã™ã€‚
5. **Reports**: é€²æ—çŠ¶æ³ã«ã¤ã„ã¦åŒåƒšã¨å…±æœ‰ã™ã‚‹ãŸã‚ã«ç°¡å˜ãªãƒ¡ãƒ¢ã‚’å–ã‚Šã€MLãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

## ğŸ¤“ ä¸Šç´šè¨­å®š
1. [ç’°å¢ƒå¤‰æ•°](https://docs.wandb.com/library/environment-variables): ç’°å¢ƒå¤‰æ•°ã«APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ã€ç®¡ç†ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
2. [ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): `dryrun` ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€å¾Œã§çµæœã‚’åŒæœŸã—ã¾ã™ã€‚
3. [ã‚ªãƒ³ãƒ—ãƒ¬](https://docs.wandb.com/self-hosted): ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚¯ãƒ©ã‚¦ãƒ‰ã‚„ã‚¨ã‚¢ã‚®ãƒ£ãƒƒãƒ—ã•ã‚ŒãŸã‚µãƒ¼ãƒãƒ¼ã«W&Bã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã¾ã™ã€‚å­¦è¡“æ©Ÿé–¢ã‹ã‚‰ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒãƒ¼ãƒ ã¾ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
4. [Artifacts](http://wandb.me/artifacts-colab): ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã•ã‚Œã‚‹ã‚ˆã†ãªåŠ¹ç‡çš„ãªæ–¹æ³•ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«é–‹ç™ºãƒ•ãƒ­ãƒ¼ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’è‡ªå‹•çš„ã«å–å¾—ã—ã¾ã™ã€‚