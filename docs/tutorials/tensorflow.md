


# TensorFlow

[**Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§è©¦ã—ã¦ã¿ã‚‹ â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/tensorflow/Simple_TensorFlow_Integration.ipynb)

Weights & Biasesã‚’ä½¿ç”¨ã—ã¦ã€æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚

<div><img /></div>

<img src="http://wandb.me/mini-diagram" width="650" alt="Weights & Biases" />

<div><img /></div>

## ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§æ‰±ã†å†…å®¹

* Weights & Biases ã‚’ TensorFlow pipeline ã«ç°¡å˜ã«çµ±åˆã—ã¦å®Ÿé¨“ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã€‚
* `keras.metrics` ã‚’ä½¿ã£ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹æ–¹æ³•ã€‚
* ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã§ `wandb.log` ã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹æ–¹æ³•ã€‚

## ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªW&Bãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¯ã“ã®ã‚ˆã†ã«è¦‹ãˆã¾ã™:

![dashboard](/images/tutorials/tensorflow/dashboard.png)

**æ³¨æ„**: _Step_ ã§å§‹ã¾ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€W&Bã‚’æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã«çµ±åˆã™ã‚‹ãŸã‚ã«å¿…è¦ãªã‚‚ã®ã ã‘ã§ã™ã€‚ãã‚Œä»¥å¤–ã¯æ¨™æº–çš„ãªMNISTã®ä¾‹ã§ã™ã€‚

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.datasets import cifar10

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

# ğŸš€ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€ãƒ­ã‚°ã‚¤ãƒ³

## Step 0ï¸âƒ£: W&Bã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```python
%%capture
!pip install wandb
```

## Step 1ï¸âƒ£: W&Bã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãƒ­ã‚°ã‚¤ãƒ³

```python
import wandb
from wandb.keras import WandbCallback

wandb.login()
```

> ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒˆ: ã“ã‚ŒãŒåˆã‚ã¦ã®W&Bã®ä½¿ç”¨ã€ã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãªã„å ´åˆã¯ã€`wandb.login()` å®Ÿè¡Œå¾Œã«è¡¨ç¤ºã•ã‚Œã‚‹ãƒªãƒ³ã‚¯ã§ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—/ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—ã¯ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã§ç°¡å˜ã§ã™ã€‚

# ğŸ‘©â€ğŸ³ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™

```python
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™
BATCH_SIZE = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

# tf.dataã‚’ä½¿ç”¨ã—ã¦å…¥åŠ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)

val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
val_dataset = val_dataset.batch(BATCH_SIZE)
```

# ğŸ§  ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å®šç¾©

```python
def make_model():
    inputs = keras.Input(shape=(784,), name="digits")
    x1 = keras.layers.Dense(64, activation="relu")(inputs)
    x2 = keras.layers.Dense(64, activation="relu")(x1)
    outputs = keras.layers.Dense(10, name="predictions")(x2)

    return keras.Model(inputs=inputs, outputs=outputs)
```

```python
def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)

    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))

    train_acc_metric.update_state(y, logits)

    return loss_value
```

```python
def test_step(x, y, model, loss_fn, val_acc_metric):
    val_logits = model(x, training=False)
    loss_value = loss_fn(y, val_logits)
    val_acc_metric.update_state(y, val_logits)

    return loss_value
```

## Step 2ï¸âƒ£: `wandb.log`ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã«è¿½åŠ 

```python
def train(train_dataset, val_dataset,  model, optimizer,
          train_acc_metric, val_acc_metric,
          epochs=10,  log_step=200, val_log_step=50):
  
    for epoch in range(epochs):
        print("\nStart of epoch %d" % (epoch,))

        train_loss = []   
        val_loss = []

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒƒãƒã”ã¨ã«åå¾©å‡¦ç†
        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
            loss_value = train_step(x_batch_train, y_batch_train, 
                                    model, optimizer, 
                                    loss_fn, train_acc_metric)
            train_loss.append(float(loss_value))

        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚äº†æ™‚ã«æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ
        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):
            val_loss_value = test_step(x_batch_val, y_batch_val, 
                                       model, loss_fn, 
                                       val_acc_metric)
            val_loss.append(float(val_loss_value))
            
        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚äº†æ™‚ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º
        train_acc = train_acc_metric.result()
        print("Training acc over epoch: %.4f" % (float(train_acc),))

        val_acc = val_acc_metric.result()
        print("Validation acc: %.4f" % (float(val_acc),))

        # å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚äº†æ™‚ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
        train_acc_metric.reset_states()
        val_acc_metric.reset_states()

        # â­: wandb.logã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        wandb.log({'epochs': epoch,
                   'loss': np.mean(train_loss),
                   'acc': float(train_acc), 
                   'val_loss': np.mean(val_loss),
                   'val_acc':float(val_acc)})
```

# ğŸ‘Ÿ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ

## Step 3ï¸âƒ£: `wandb.init`ã‚’å‘¼ã³å‡ºã—ã¦runã‚’é–‹å§‹

ã“ã‚Œã«ã‚ˆã‚Šã€å®Ÿé¨“ã‚’é–‹å§‹ã—ã¦ã„ã‚‹ã“ã¨ãŒé€šçŸ¥ã•ã‚Œã€ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒæä¾›ã•ã‚Œã¾ã™ã€‚

[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã“ã¡ã‚‰ $\rightarrow$](https://docs.wandb.com/library/init)

```python
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§è¨­å®šå€¤ã‚’æŒ‡å®šã—ã¦wandbã‚’åˆæœŸåŒ–
# è¨­å®šå€¤ã‚’å¤‰æ›´ã—ã¦ã€wandbãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„
config = {
              "learning_rate": 0.001,
              "epochs": 10,
              "batch_size": 64,
              "log_step": 200,
              "val_log_step": 50,
              "architecture": "CNN",
              "dataset": "CIFAR-10"
           }

run = wandb.init(project='my-tf-integration', config=config)
config = wandb.config

# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
model = make_model()

# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
optimizer = keras.optimizers.SGD(learning_rate=config.learning_rate)
# æå¤±é–¢æ•°ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æº–å‚™
train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

train(train_dataset,
      val_dataset, 
      model,
      optimizer,
      train_acc_metric,
      val_acc_metric,
      epochs=config.epochs, 
      log_step=config.log_step, 
      val_log_step=config.val_log_step)

run.finish()  # Jupyter/Colabã§å®Ÿè¡Œã‚’çµ‚äº†ã—ãŸã“ã¨ã‚’é€šçŸ¥
```

# ğŸ‘€ çµæœã‚’å¯è¦–åŒ–

ä¸Šã®[**runãƒšãƒ¼ã‚¸**](https://docs.wandb.ai/ref/app/pages/run-page)ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€ãƒ©ã‚¤ãƒ–çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

# ğŸ§¹ Sweep 101

Weights & Biases Sweepsã‚’ä½¿ç”¨ã—ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®æœ€é©åŒ–ã‚’è‡ªå‹•åŒ–ã—ã€å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ç©ºé–“ã‚’æ¢ç´¢ã—ã¾ã—ã‚‡ã†ã€‚

## [W&B Sweepsã‚’ä½¿ç”¨ã—ãŸTensorFlowã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–ã®è©³ç´°ã¯ã“ã¡ã‚‰ â†’](http://wandb.me/tf-sweeps-colab)

### W&B Sweepsã‚’ä½¿ç”¨ã™ã‚‹åˆ©ç‚¹

* **ç°¡å˜ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã§W&B sweepsã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚
* **é€æ˜æ€§**: ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã™ã¹ã¦å¼•ç”¨ã—ã€[ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹](https://github.com/wandb/client/tree/master/wandb/sweeps)ã§ã™ã€‚
* **å¼·åŠ›**: Sweepsã¯å®Œå…¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãŠã‚ˆã³æ§‹æˆå¯èƒ½ã§ã™ã€‚æ•°åå°ã®ãƒã‚·ãƒ³ã§ã‚¹ã‚¤ãƒ¼ãƒ—ã‚’é–‹å§‹ã™ã‚‹ã®ã‚‚ã€ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã§é–‹å§‹ã™ã‚‹ã®ã‚‚åŒã˜ãã‚‰ã„ç°¡å˜ã§ã™ã€‚

<img src="https://i.imgur.com/6eWHZhg.png" alt="Sweep Result" />

# ğŸ¨ Example Gallery

W&Bã§ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãŠã‚ˆã³å¯è¦–åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¾‹ã‚’ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã§ã”è¦§ãã ã•ã„ã€‚[Fully Connected â†’](https://wandb.me/fc)

# ğŸ“ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

1. **Projects**: è¤‡æ•°ã®runsã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãƒ­ã‚°ã—ã¦æ¯”è¼ƒã€‚`wandb.init(project="project-name")`
2. **Groups**: è¤‡æ•°ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚„äº¤å·®æ¤œè¨¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®å ´åˆã€ãã‚Œãã‚Œã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’runsã¨ã—ã¦ãƒ­ã‚°ã—ã€ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã€‚`wandb.init(group='experiment-1')`
3. **Tags**: ç¾åœ¨ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚„ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ã‚¿ã‚°ã‚’è¿½åŠ ã€‚
4. **Notes**: ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ¡ãƒ¢ã‚’è¨˜å…¥ã—ã¦ã€runsé–“ã®å¤‰æ›´ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã€‚
5. **Reports**: é€²æ—ã«é–¢ã™ã‚‹ãƒ¡ãƒ¢ã‚’åŒåƒšã¨å…±æœ‰ã—ã€MLãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¨ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½œæˆã€‚

## ğŸ¤“ é«˜åº¦ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
1. [ç’°å¢ƒå¤‰æ•°](https://docs.wandb.com/library/environment-variables): ç’°å¢ƒå¤‰æ•°ã«APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ã€ç®¡ç†ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã€‚
2. [ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€å¾Œã§çµæœã‚’åŒæœŸã™ã‚‹ãŸã‚ã« `dryrun` ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã€‚
3. [ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹](https://docs.wandb.com/self-hosted): ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚¯ãƒ©ã‚¦ãƒ‰ã‚„ã‚¨ã‚¢ã‚®ãƒ£ãƒƒãƒ—ã•ã‚ŒãŸã‚µãƒ¼ãƒãƒ¼ã«W&Bã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‚å­¦è¡“æ©Ÿé–¢ã‹ã‚‰ä¼æ¥­ãƒãƒ¼ãƒ ã¾ã§ã€ã™ã¹ã¦ã®äººã«ãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’æä¾›ã€‚
4. [Artifacts](http://wandb.me/artifacts-colab): ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒˆãƒ©ãƒƒã‚¯ãŠã‚ˆã³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã—ã€é–‹ç™ºãƒ•ãƒ­ãƒ¼ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’è‡ªå‹•çš„ã«å–å¾—ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«åæ˜ ã€‚