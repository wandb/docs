# Track models and datasets

[**Try in a Colab Notebook here â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W&B_Artifacts.ipynb)

ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€W&B Artifacts ã‚’ä½¿ã£ã¦MLã®å®Ÿé¨“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

### ä¸€ç·’ã«é€²ã‚ã¾ã—ã‚‡ã†ï¼ [ãƒ“ãƒ‡ã‚ªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](http://tiny.cc/wb-artifacts-video)

### ğŸ¤” Artifactsã¨ã¯ä½•ã‹ã€ãã—ã¦ãªãœé‡è¦ãªã®ã‹ï¼Ÿ

ã€Œartifactã€ã¯ã€ã‚®ãƒªã‚·ãƒ£ã®[ã‚¢ãƒ³ãƒ•ã‚©ãƒ© ğŸº](https://en.wikipedia.org/wiki/Amphora)ã®ã‚ˆã†ãªä½œã‚‰ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã€ãƒ—ãƒ­ã‚»ã‚¹ã®å‡ºåŠ›ç‰©ã§ã™ã€‚
MLã§ã¯ã€æœ€ã‚‚é‡è¦ãªartifactã¯ _datasets_ ã¨ _models_ ã§ã™ã€‚

ãã—ã¦ã€[ã‚³ãƒ­ãƒŠãƒ‰ã®åå­—æ¶](https://indianajones.fandom.com/wiki/Cross_of_Coronado)ã®ã‚ˆã†ã«ã€ã“ã‚Œã‚‰ã®é‡è¦ãªartifactã¯åšç‰©é¤¨ã«ä¿å­˜ã•ã‚Œã‚‹ã¹ãã§ã™ï¼
ã¤ã¾ã‚Šã€ã“ã‚Œã‚‰ã¯ã‚«ã‚¿ãƒ­ã‚°åŒ–ã•ã‚Œã€æ•´ç†ã•ã‚Œã‚‹ã¹ãã§ã™ã€‚
ã‚ãªãŸã‚„ãƒãƒ¼ãƒ ã€ãã—ã¦åºƒç¯„ãªMLã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãŒãã‚Œã‚‰ã‹ã‚‰å­¦ã¹ã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã§ã™ã€‚
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ãªã„è€…ã¯ãã‚Œã‚’ç¹°ã‚Šè¿”ã™é‹å‘½ã«ã‚ã‚‹ã®ã§ã™ã€‚

Artifacts APIã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€`Artifact`ã‚’W&Bã®`Run`ã®å‡ºåŠ›ã¨ã—ã¦ãƒ­ã‚°ã«è¨˜éŒ²ã—ãŸã‚Šã€`Run`ã®å…¥åŠ›ã¨ã—ã¦`Artifact`ã‚’ä½¿ç”¨ã—ãŸã‚Šã§ãã¾ã™ã€‚ã“ã®ãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ ã®ã‚ˆã†ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°runãŒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å…¥åŠ›ã¨ã—ã¦å–ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

![](https://gblobscdn.gitbook.com/assets%2F-Lqya5RvLedGEWPhtkjU%2F-M94QAXA-oJmE6q07_iT%2F-M94QJCXLeePzH1p_fW1%2Fsimple%20artifact%20diagram%202.png?alt=media&token=94bc438a-bd3b-414d-a4e4-aa4f6f359f21)

1ã¤ã®runãŒä»–ã®runã®å‡ºåŠ›ã‚’å…¥åŠ›ã¨ã—ã¦ä½¿ç”¨ã§ãã‚‹ãŸã‚ã€Artifactsã¨Runsã¯ä¸€ç·’ã«æœ‰å‘ã‚°ãƒ©ãƒ•ï¼ˆå®Ÿéš›ã«ã¯äºŒéƒ¨[ãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ ](https://en.wikipedia.org/wiki/Directed_acyclic_graph)ï¼‰ã‚’å½¢æˆã—ã¾ã™ã€‚ãƒãƒ¼ãƒ‰ã¯`Artifact`ã¨`Run`ã®ã‚‚ã®ã§ã‚ã‚Šã€ãã‚Œãã‚Œæ¶ˆè²»ã‚„ç”Ÿæˆã•ã‚Œã‚‹`Artifact`ã«å¯¾ã—ã¦RunsãŒçŸ¢å°ã§ã¤ãªãŒã£ã¦ã„ã¾ã™ã€‚

# 0ï¸âƒ£ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

Artifactsã¯ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³`0.9.2`ã‹ã‚‰Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä¸€éƒ¨ã§ã™ã€‚

ä»–ã®å¤šãã®ML Pythonã‚¹ã‚¿ãƒƒã‚¯ã¨åŒæ§˜ã«ã€`pip`çµŒç”±ã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚

```python
# wandb ãƒãƒ¼ã‚¸ãƒ§ãƒ³0.9.2ä»¥é™ã«å¯¾å¿œ
!pip install wandb -qqq
!apt install tree
```

```python
import os
import wandb
```

# 1ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ã‚°ã™ã‚‹

ã¾ãšã€ã„ãã¤ã‹ã®Artifactsã‚’å®šç¾©ã—ã¾ã—ã‚‡ã†ã€‚

ã“ã®ä¾‹ã¯PyTorchã®
["Basic MNIST Example"](https://github.com/pytorch/examples/tree/master/mnist)
ã«åŸºã¥ã„ã¦ã„ã¾ã™ãŒã€[TensorFlow](http://wandb.me/artifacts-colab)ã‚„ä»–ã®ä»»æ„ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ã¾ãŸã¯ç´”ç²‹ãªPythonã§ã‚‚ç°¡å˜ã«å®Ÿè¡Œã§ãã¾ã™ã€‚

ã¾ãšã¯ã€`Dataset`ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã—ã¾ã™ï¼š
- `train`ã‚»ãƒƒãƒˆã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸ã¶ãŸã‚ã€
- `validation`ã‚»ãƒƒãƒˆã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸ã¶ãŸã‚ã€
- `test`ã‚»ãƒƒãƒˆã¯æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚

ä»¥ä¸‹ã®ã‚»ãƒ«ã§ã¯ã€ã“ã‚Œã‚‰3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å®šç¾©ã—ã¾ã™ã€‚

```python
import random 

import torch
import torchvision
from torch.utils.data import TensorDataset
from tqdm.auto import tqdm

# æ±ºå®šçš„ãªæŒ¯ã‚‹èˆã„ã‚’ä¿è¨¼
torch.backends.cudnn.deterministic = True
random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)

# ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
num_classes = 10
input_shape = (1, 28, 28)

# é…ã„ãƒŸãƒ©ãƒ¼ã‚’ãƒ‰ãƒ­ãƒƒãƒ—
torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors
                                      if not mirror.startswith("http://yann.lecun.com")]

def load(train_size=50_000):
    """
    # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
    """

    # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«åˆ†å‰²
    train = torchvision.datasets.MNIST("./", train=True, download=True)
    test = torchvision.datasets.MNIST("./", train=False, download=True)
    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)

    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã«æ¤œè¨¼ã‚»ãƒƒãƒˆã‚’åˆ†å‰²
    x_train, x_val = x_train[:train_size], x_train[train_size:]
    y_train, y_val = y_train[:train_size], y_train[train_size:]

    training_set = TensorDataset(x_train, y_train)
    validation_set = TensorDataset(x_val, y_val)
    test_set = TensorDataset(x_test, y_test)

    datasets = [training_set, validation_set, test_set]

    return datasets
```

ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ã®å‘¨ã‚Šã«ãƒ‡ãƒ¼ã‚¿ã‚’Artifactã¨ã—ã¦ãƒ­ã‚°ã™ã‚‹ã‚³ãƒ¼ãƒ‰ãŒãƒ©ãƒƒãƒ—ã•ã‚Œã¦ã„ã¾ã™ã€‚
ã“ã®å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚’`load`ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã¨ã€ãƒ‡ãƒ¼ã‚¿ã‚’`load_and_log`ã™ã‚‹ã‚³ãƒ¼ãƒ‰ãŒåˆ†é›¢ã•ã‚Œã¦ã„ã¾ã™ã€‚

ã“ã‚Œã¯è‰¯ã„å®Ÿè·µã§ã™ï¼

ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Artifactã¨ã—ã¦ãƒ­ã‚°ã™ã‚‹ã«ã¯ã€
1. `wandb.init`ã§`Run`ã‚’ä½œæˆ (L4)
2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®`Artifact`ã‚’ä½œæˆ (L10)
3. é–¢é€£ã™ã‚‹`file`ã‚’ä¿å­˜ã—ã¦ãƒ­ã‚° (L20, L23)

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚»ãƒ«ã®ä¾‹ã‚’è¦‹ã¦ã€ãã®å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å±•é–‹ã—ã¦è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```python
def load_and_log():

    # ğŸš€ runã‚’ã‚¹ã‚¿ãƒ¼ãƒˆã—ã€ã‚¿ã‚¤ãƒ—ã¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ãƒ©ãƒ™ãƒ«ä»˜ã‘ã™ã‚‹
    with wandb.init(project="artifacts-example", job_type="load-data") as run:
        
        datasets = load()  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚ã®åˆ†é›¢ã‚³ãƒ¼ãƒ‰
        names = ["training", "validation", "test"]

        # ğŸº Artifactã‚’ä½œæˆ
        raw_data = wandb.Artifact(
            "mnist-raw", type="dataset",
            description="ç”Ÿã®MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆã«åˆ†å‰²",
            metadata={"source": "torchvision.datasets.MNIST",
                      "sizes": [len(dataset) for dataset in datasets]})

        for name, data in zip(names, datasets):
            # ğŸ£ ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã«æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã€ãã®å†…å®¹ã«ä½•ã‹ã‚’æ›¸ãè¾¼ã¿ã¾ã™ã€‚
            with raw_data.new_file(name + ".pt", mode="wb") as file:
                x, y = data.tensors
                torch.save((x, y), file)

        # âœï¸ ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’W&Bã«ä¿å­˜ã—ã¾ã™ã€‚
        run.log_artifact(raw_data)

load_and_log()
```

### ğŸš€ `wandb.init`

Artifactã‚’ç”Ÿæˆã™ã‚‹`Run`ã‚’ä½œæˆã™ã‚‹éš›ã€ã©ã®`project`ã«å±ã™ã‚‹ã‹ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«å¿œã˜ã¦ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯`car-that-drives-itself`ã®ã‚ˆã†ã«å¤§ãã„ã‚‚ã®ã‹ã‚‰`iterative-architecture-experiment-117`ã®ã‚ˆã†ã«å°ã•ã„ã‚‚ã®ã¾ã§æ§˜ã€…ã§ã™ã€‚

> **Rule of ğŸ‘**: ã™ã¹ã¦ã®`Run`ãŒå…±æœ‰ã™ã‚‹`Artifact`ã‚’1ã¤ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã«ä¿æŒã§ãã‚‹å ´åˆã¯ã€ãã‚Œã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«ä¿ã¡ã¾ã—ã‚‡ã†ã€‚ãŸã ã—ã€å¿ƒé…ã—ãªã„ã§ãã ã•ã„â”€â”€`Artifact`ã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–“ã§ç§»å‹•å¯èƒ½ã§ã™ï¼

æ§˜ã€…ãªã‚¸ãƒ§ãƒ–ã‚’ç®¡ç†ã™ã‚‹ãŸã‚ã«ã€`Run`ã‚’ä½œæˆã™ã‚‹éš›ã«`job_type`ã‚’æä¾›ã™ã‚‹ã®ãŒä¾¿åˆ©ã§ã™ã€‚
ã“ã‚Œã«ã‚ˆã‚ŠArtifactã®ã‚°ãƒ©ãƒ•ãŒæ•´ç†ã•ã‚Œã¾ã™ã€‚

> **Rule of ğŸ‘**: `job_type`ã¯èª¬æ˜çš„ã§ã‚ã‚Šã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å˜ä¸€ã‚¹ãƒ†ãƒƒãƒ—ã«å¯¾å¿œã™ã‚‹ã‚ˆã†ã«ã—ã¾ã—ã‚‡ã†ã€‚ã“ã“ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®`load`ã¨ãƒ‡ãƒ¼ã‚¿ã®`preprocess`ã‚’åˆ†ã‘ã¦ã„ã¾ã™ã€‚

### ğŸº `wandb.Artifact`

ä½•ã‹ã‚’`Artifact`ã¨ã—ã¦ãƒ­ã‚°ã™ã‚‹ã«ã¯ã€æœ€åˆã«`Artifact`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ã™ã¹ã¦ã®`Artifact`ã«ã¯`name`ãŒã‚ã‚Šã¾ã™â”€â”€ã“ã‚ŒãŒæœ€åˆã®å¼•æ•°ã§è¨­å®šã•ã‚Œã¾ã™ã€‚

> **Rule of ğŸ‘**: `name`ã¯èª¬æ˜çš„ã§ã€è¦šãˆã‚„ã™ãå…¥åŠ›ã—ã‚„ã™ã„ã‚‚ã®ã«ã—ã¾ã—ã‚‡ã†â”€â”€ç§ãŸã¡ã¯ãƒã‚¤ãƒ•ãƒ³ã§åŒºåˆ‡ã‚‰ã‚ŒãŸåå‰ã‚’ã‚³ãƒ¼ãƒ‰å†…ã®å¤‰æ•°åã«å¯¾å¿œã•ã›ã‚‹ã®ãŒå¥½ãã§ã™ã€‚

ã¾ãŸã€`type`ã‚‚ã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€`Run`ã®`job_type`ã¨åŒæ§˜ã«`Run`ã¨`Artifact`ã®ã‚°ãƒ©ãƒ•ã‚’æ•´ç†ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

> **Rule of ğŸ‘**: `type`ã¯ã‚·ãƒ³ãƒ—ãƒ«ã§ã‚ã‚‹ã¹ãã§ã™ï¼š`dataset`ã‚„`model`ã®ã‚ˆã†ãªã‚‚ã®ã€‚

ã•ã‚‰ã«ã€è¾æ›¸å½¢å¼ã®`description`ã¨`metadata`ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
`metadata`ã¯JSONã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

> **Rule of ğŸ‘**: `metadata`ã¯å¯èƒ½ãªé™ã‚Šè©³ç´°ã«ã—ã¾ã™ã€‚

### ğŸ£ `artifact.new_file`ã¨âœï¸ `run.log_artifact`

`Artifact`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ãŸã‚‰ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ãã®é€šã‚Šã€_ãƒ•ã‚¡ã‚¤ãƒ«_ ã® _s_ ãŒã‚ã‚Šã¾ã™ã€‚
`Artifact`ã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚ˆã†ã«æ§‹é€ åŒ–ã•ã‚Œã¦ãŠã‚Šã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒã¡ã¾ã™ã€‚

> **Rule of ğŸ‘**: ã§ãã‚‹é™ã‚Šã€`Artifact`ã®å†…å®¹ã‚’è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã«ã‚ˆã‚Šã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹éš›ã«ä¾¿åˆ©ã§ã™ï¼

`new_file`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãã¨åŒæ™‚ã«`Artifact`ã«æ·»ä»˜ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
ä»¥ä¸‹ã§ã¯ã€ã“ã‚Œã‚‰ã®2ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’åˆ†ã‘ã‚‹`add_file`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã—ãŸã‚‰ã€`log_artifact`ã‚’ä½¿ç”¨ã—ã¦[wandb.ai](https://wandb.ai)ã«ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

å‡ºåŠ›ã«ã¯ã„ãã¤ã‹ã®URLãŒè¡¨ç¤ºã•ã‚Œã¾ã™ãŒã€ãã®ä¸­ã«ã¯Runãƒšãƒ¼ã‚¸ã®URLã‚‚å«ã¾ã‚Œã¾ã™ã€‚
ãã“ã§ã¯ã€`Run`ã®çµæœã‚’ç¢ºèªã§ãã€ãƒ­ã‚°ã•ã‚ŒãŸ`Artifact`ã‚‚è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

ä»¥ä¸‹ã¯ãã®ä»–ã®Runãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚ˆã‚Šæ´»ç”¨ã™ã‚‹ä¾‹ã§ã™ã€‚

# 2ï¸âƒ£ ãƒ­ã‚°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆArtifactã‚’ä½¿ç”¨ã™ã‚‹

W&Bã®`Artifact`ã¯ã€åšç‰©é¤¨ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã¯é•ã„ã€å˜ã«ä¿å­˜ã•ã‚Œã‚‹ã ã‘ã§ãªãã€_ä½¿ç”¨_ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚

ãã‚ŒãŒã©ã®ã‚ˆã†ãªã‚‚ã®ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

ä»¥ä¸‹ã®ã‚»ãƒ«ã§ã¯ã€ç”Ÿã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–ã‚Šè¾¼ã¿ã€ãã‚Œã‚’`preprocess`edãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦å‡ºåŠ›ã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®šç¾©ã—ã¾ã™ã€‚
ã“ã‚Œã¯`normalize`ã•ã‚Œã€æ­£ã—ãæ•´å½¢ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚

ã“ã“ã§ã‚‚ã€`wandb`ã¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã€é‡è¦ãªã‚³ãƒ¼ãƒ‰ã§ã‚ã‚‹`preprocess`ã‚’åˆ†é›¢ã—ã¦ã„ã¾ã™ã€‚

```python
def preprocess(dataset, normalize=True, expand_dims=True):
    """
    ## ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹
    """
    x, y = dataset.tensors

    if normalize:
        # ç”»åƒã‚’[0, 1]ã®ç¯„å›²ã«ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹
        x = x.type(torch.float32) / 255

    if expand_dims:
        # ç”»åƒãŒ(1, 28, 28)ã®å½¢çŠ¶ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹
        x = torch.unsqueeze(x, 1)
    
    return TensorDataset(x, y)
```

æ¬¡ã«ã€`preprocess`ã‚¹ãƒ†ãƒƒãƒ—ã‚’`wandb.Artifact`ã®ãƒ­ã‚°ã‚’ä½¿ç”¨ã—ã¦å™¨ç”¨ã«æ‰±ã†ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚

ä»¥ä¸‹ã®ä¾‹ã§ã¯ã€`Artifact`ã‚’`use`ã™ã‚‹ã“ã¨ï¼ˆã“ã‚Œã¯æ–°ã—ã„ï¼‰ã€ãã—ã¦ãã‚Œã‚’ãƒ­ã‚°ã™ã‚‹ã“ã¨ï¼ˆã“ã‚Œã¯å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨åŒã˜ï¼‰ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚
`Artifact`ã¯`Run`ã®å…¥åŠ›ã¨å‡ºåŠ›ã®ä¸¡æ–¹ã§ã™ï¼

æ–°ã—ã„`job_type`ã§ã‚ã‚‹`preprocess-data`ã‚’ä½¿ç”¨ã—ã¦ã€ã“ã‚ŒãŒå‰ã®`load`ã¨ã¯ç•°ãªã‚‹ç¨®é¡ã®ã‚¸ãƒ§ãƒ–ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¢ºã«ã—ã¾ã™ã€‚

```python
def preprocess_and_log(steps):

    with wandb.init(project="artifacts-example", job_type="preprocess-data") as run:

        processed_data = wandb.Artifact(
            "mnist-preprocess", type="dataset",
            description="å‰å‡¦ç†æ¸ˆã¿ã®MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            metadata=steps)
         
        # âœ”ï¸ ä½¿ç”¨ã™ã‚‹artifactã‚’å®£è¨€
        raw_data_artifact = run.use_artifact('mnist-raw:latest')

        # ğŸ“¥ å¿…è¦ã«å¿œã˜ã¦artifactã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        raw_dataset = raw_data_artifact.download()
        
        for split in ["training", "validation", "test"]:
            raw_split = read(raw_dataset, split)
            processed_dataset = preprocess(raw_split, **steps)

            with processed_data.new_file(split + ".pt", mode="wb") as file:
                x, y = processed_dataset.tensors
                torch.save((x, y), file)

        run.log_artifact(processed_data)


def read(data_dir, split):
    filename = split + ".pt"
    x, y = torch.load(os.path.join(data_dir, filename))

    return TensorDataset(x, y)
```

ã“ã“ã§æ°—ã¥ãã¹ãç‚¹ã¯ã€å‰å‡¦ç†ã®`steps`ãŒ`preprocessed_data`ã®`metadata`ã¨ã—ã¦ä¿å­˜ã•ã‚Œã‚‹ã“ã¨ã§ã™ã€‚

å®Ÿé¨“ã‚’å†ç¾å¯èƒ½ã«ã—ãŸã„å ´åˆã€ãŸãã•ã‚“ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹ã“ã¨ã¯è‰¯ã„è€ƒãˆã§ã™ï¼

ã¾ãŸã€ç§ãŸã¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã€Œ`large artifact`ã€ã§ã‚ã£ã¦ã‚‚ã€`download`ã‚¹ãƒ†ãƒƒãƒ—ã¯1ç§’æœªæº€ã§å®Œäº†ã—ã¾ã™ã€‚

è©³ç´°ã¯ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å±•é–‹ã—ã¦ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```python
steps = {"normalize": True,
         "expand_dims": True}

preprocess_and_log(steps)
```

### âœ”ï¸ `run.use_artifact`

ã“ã‚Œã‚‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã‚·ãƒ³ãƒ—ãƒ«ã§ã™ã€‚æ¶ˆè²»è€…ã¯å˜ã«`Artifact`ã®`name`ã‚’çŸ¥ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€ã¡ã‚‡ã£ã¨ã ã‘ä»–ã®æƒ…å ±ã‚‚ã€‚

ãã®ã€Œã¡ã‚‡ã£ã¨ã ã‘ä»–ã®æƒ…å ±ã€ã¨ã¯ã€æ±‚ã‚ã‚‹ç‰¹å®šã®`Artifact`ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®`alias`ã§ã™ã€‚

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€æœ€å¾Œã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã¯`latest`ãŒã‚¿ã‚°ä»˜ã‘ã•ã‚Œã¾ã™ã€‚
ãã‚Œä»¥å¤–ã®å ´åˆã€`v0`ã‚„`v1`ãªã©ã®å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’é¸æŠã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã—ã€
`best`ã‚„`jit-script`ãªã©è‡ªåˆ†ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
ã¡ã‚‡ã†ã©[Docker Hub](https://hub.docker.com/)ã®ã‚¿ã‚°ã®ã‚ˆã†ã«ã€
ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¯åå‰ã‹ã‚‰`:`ã§åˆ†é›¢ã•ã‚Œã¦ã„ã¾ã™ã€‚

> **Rule of ğŸ‘**: ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¯çŸ­ãã‚·ãƒ³ãƒ—ãƒ«ã«ä¿ã¡ã¾ã—ã‚‡ã†ã€‚
ã‚«ã‚¹ã‚¿ãƒ ã‚¨ã‚¤ãƒªã‚¢ã‚¹`latest`ã‚„`best`ã‚’ä½¿ç”¨ã—ã¦ã€ç‰¹å®šã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’æº€ãŸã™Artifactã‚’é¸ã³ã¾ã—ã‚‡ã†ã€‚

### ğŸ“¥ `artifact.download`

`download`å‘¼ã³å‡ºã—ãŒå¿ƒé…ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
åˆ¥ã®ã‚³ãƒ”ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€ãƒ¡ãƒ¢ãƒªã®è² æ‹…ãŒå€å¢—ã™ã‚‹ã®ã§ã¯ï¼Ÿ

å¿ƒé…ã—ãªã„ã§ãã ã•ã„ã€‚å®Ÿéš›ã«ä½•ã‹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‰ã«ã€
é©åˆ‡ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒãƒ­ãƒ¼ã‚«ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚
ã“ã‚Œã«ã¯[ãƒˆãƒ¬ãƒ³ãƒˆ](https://en.wikipedia.org/wiki/Torrent_file)ã‚„[`git`ã«ã‚ˆã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†](https://blog.thoughtram.io/git/2014/11/18/the-anatomy-of-a-git-commit.html)ã®æŠ€è¡“ãŒä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚

ArtifactãŒä½œæˆãƒ»ãƒ­ã‚°ã•ã‚Œã‚‹ã¨ã€ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¸­ã«`artifacts`ã¨ã„ã†ãƒ•ã‚©ãƒ«ãƒ€ãŒã§ãã€
ãã‚Œãã‚Œã®Artifactç”¨ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒåŸ‹ã‚å°½ãã•ã‚Œã¾ã™ã€‚
ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãã®å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ï¼š

```python
!tree artifacts
```

### ğŸŒ [wandb.ai](https://wandb.ai)ã®Artifactsãƒšãƒ¼ã‚¸

Artifactã‚’ãƒ­ã‚°ã—ã¦ä½¿ç”¨ã—ãŸã‚‰ã€Runãƒšãƒ¼ã‚¸ã®Artifactsã‚¿ãƒ–ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ã‚‡ã†ã€‚

`wandb`ã®å‡ºåŠ›ã«è¡¨ç¤ºã•ã‚Œã‚‹Runãƒšãƒ¼ã‚¸URLã«ç§»å‹•ã—ã€
å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€ŒArtifactsã€ã‚¿ãƒ–ã‚’é¸æŠã—ã¾ã™
ï¼ˆã“ã‚Œã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¤ã‚³ãƒ³ã®ã‚‚ã®ã§ã€ãƒ›ãƒƒã‚±ãƒ¼ã®ãƒ‘ãƒƒã‚¯ãŒ3ã¤ç©ã¿é‡ãªã£ãŸã‚ˆã†ã«è¦‹ãˆã¾ã™ï¼‰ã€‚

"Input Artifacts"ãƒ†ãƒ¼ãƒ–ãƒ«ã‚„"Output Artifacts"ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã€
ã‚¿ãƒ–ï¼ˆ"Overview"ã€"Metadata"ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€Artifactã«é–¢ã™ã‚‹ã™ã¹ã¦ã®ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

ç‰¹ã«ã€ŒGraph Viewã€ãŒå¥½ãã§ã™ã€‚
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€`Artifact`ã®ã‚¿ã‚¤ãƒ—ã¨`Run`ã®`job_type`ãŒ2ç¨®é¡ã®ãƒãƒ¼ãƒ‰ã§ç¤ºã•ã‚Œã€
çŸ¢å°ã¯æ¶ˆè²»ã¨ç”Ÿæˆã‚’è¡¨ã—ã¾ã™ã€‚

# 3ï¸âƒ£ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ã‚°ã™ã‚‹

ã“ã‚Œã§`Artifact` APIã®å‹•ä½œãŒã‚ã‹ã‚Šã¾ã—ãŸãŒã€ã“ã®ä¾‹ã‚’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®çµ‚ã‚ã‚Šã¾ã§ç¶šã‘ã¦ã€
`Artifact`ãŒMLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ã©ã®ã‚ˆã†ã«æ”¹å–„ã§ãã‚‹ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

ã“ã®æœ€åˆã®ã‚»ãƒ«ã¯ã€PyTorchã§DNN `model`ã‚’æ§‹ç¯‰ã—ã¾ã™â”€â”€ã”ãã‚·ãƒ³ãƒ—ãƒ«ãªConvNetã§ã™ã€‚

ã¾ãšã€ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã—ã¾ã›ã‚“ã€‚
ã“ã‚Œã«ã‚ˆã‚Šã€ä»–ã®éƒ¨åˆ†ã‚’å›ºå®šã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ç¹°ã‚Šè¿”ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
from math import floor

import torch.nn as nn

class ConvNet(nn.Module):
    def __init__(self, hidden_layer_sizes=[32, 64],
                  kernel_sizes=[3],
                  activation="ReLU",
                  pool_sizes=[2],
                  dropout=0.5,
                  num_classes=num_classes,
                  input_shape=input_shape):
      
        super(ConvNet, self).__init__()

        self.layer1 = nn.Sequential(
              nn.Conv2d(in_channels=input_shape[0], out_channels=hidden_layer_sizes[0], kernel_size=kernel_sizes[0]),
              getattr(nn, activation)(),
              nn.MaxPool2d(kernel_size=pool_sizes[0])
        )
        self.layer2 = nn.Sequential(
              nn.Conv2d(in_channels=hidden_layer_sizes[0], out_channels=hidden_layer_sizes[-1], kernel_size=kernel_sizes[-1]),
              getattr(nn, activation)(),
              nn.MaxPool2d(kernel_size=pool_sizes[-1])
        )
        self.layer3 = nn.Sequential(
              nn.Flatten(),
              nn.Dropout(dropout)
        )

        fc_input_dims = floor((input_shape[1] - kernel_sizes[0] + 1) / pool_sizes[0]) # layer 1 output size
        fc_input_dims = floor((fc_input_dims - kernel_sizes[-1] + 1) / pool_sizes[-1]) # layer 2 output size
        fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-1] # layer 3 output size

        self.fc = nn.Linear(fc_input_dims, num_classes)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.fc(x)
        return x
```

ã“ã“ã§ã¯ã€W&Bã‚’ä½¿ç”¨ã—ã¦runã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã—ã€
[`wandb.config`](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-config/Configs_in_W%26B.ipynb)
ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ã—ã¦ã™ã¹ã¦ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¦ã„ã¾ã™ã€‚

ãã®`config`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®`dict`ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯éå¸¸ã«ä¾¿åˆ©ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã®ã§ã€å¿…ãšå«ã‚ã¦ãã ã•ã„ï¼

```python
def build_model_and_log(config):
    with wandb.init(project="artifacts-example", job_type="initialize", config=config) as run:
        config = wandb.config
        
        model = ConvNet(**config)

        model_artifact = wandb.Artifact(
            "convnet", type="model",
            description="ã‚·ãƒ³ãƒ—ãƒ«ãªAlexNetã‚¹ã‚¿ã‚¤ãƒ«ã®CNN",
            metadata=dict(config))

        torch.save(model.state_dict(), "initialized_model.pth")
        # â• Artifactã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã™ã‚‹åˆ¥ã®æ–¹æ³•
        model_artifact.add_file("initialized_model.pth")

        wandb.save("initialized_model.pth")

        run.log_artifact(model_artifact)

model_config = {"hidden_layer_sizes": [32, 64],
                "kernel_sizes": [3],
                "activation": "ReLU",
                "pool_sizes": [2],
                "dropout": 0.5,
                "num_classes": 10}

build_model_and_log(model_config)
```

### â• `artifact.add_file`

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ã‚°ä¾‹ã®ã‚ˆã†ã«ã€`new_file`ã‚’ä¸€åº¦ã«æ›¸ãè¾¼ã¿ã€`Artifact`ã«è¿½åŠ ã™ã‚‹ä»£ã‚ã‚Šã«ã€
ãƒ•ã‚¡ã‚¤ãƒ«ã‚’1ã‚¹ãƒ†ãƒƒãƒ—ã§æ›¸ãï¼ˆã“ã“ã§ã¯ã€`torch.save`ï¼‰ã“ã¨ãŒã§ãã€ãã®å¾Œã§`Artifact`ã«è¿½åŠ ã§ãã¾ã™ã€‚

> **Rule of ğŸ‘**: é‡è¤‡ã‚’é˜²ããŸã‚ã«ã€å¯èƒ½ã§ã‚ã‚Œã°`new_file`ã‚’ä½¿ç”¨ã—ã¾ã—ã‚‡ã†ã€‚

# 4ï¸âƒ£ ãƒ­ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«Artifactã‚’ä½¿ç”¨ã™ã‚‹

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦`use_artifact`ã‚’å‘¼ã³å‡ºã›ã‚‹ã‚ˆã†ã«ã€
`initialized_model`ã«å¯¾ã—ã¦ã‚‚åŒæ§˜ã«å‘¼ã³å‡ºã—ã¦ã€åˆ¥ã®`Run`ã§ä½¿ç”¨ã§ãã¾ã™ã€‚

ä»Šå›ã¯ãƒ¢ãƒ‡ãƒ«ã‚’`train`ã—ã¾ã—ã‚‡ã†ã€‚

è©³ç´°ã¯ã€ç§ãŸã¡ã®Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
[W&Bã¨PyTorchã‚’ä½¿ã£ãŸã‚¤ãƒ³ã‚¹ãƒ„ãƒ«ãƒ¡ãƒ³ãƒˆ](http://wandb.me/pytorch-colab)ã‚’ã”è¦§ãã ã•ã„ã€‚

```python
import torch.nn.functional as F

def train(model, train_loader, valid_loader, config):
    optimizer = getattr(torch.optim, config.optimizer)(model.parameters())
    model.train()
    example_ct = 0
    for epoch in range(config.epochs):
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = F.cross_entropy(output, target)
            loss.backward()
            optimizer.step()

            example_ct += len(data)

            if batch_idx % config.batch_log_interval == 0:
                print('Train Epoch: {} [{}/{} ({:.0%})]\tLoss: {:.6f}'.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                    batch_idx / len(train_loader), loss.item()))
                
                train_log(loss, example_ct, epoch)

        # å„ã‚¨ãƒãƒƒã‚¯ã§æ¤œè¨¼ã‚»ãƒƒãƒˆä¸Šã®ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡
        loss, accuracy = test(model, valid_loader)  
        test_log(loss, accuracy, example_ct, epoch)

    
def test(model, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.cross_entropy(output, target, reduction='sum')  # ãƒãƒƒãƒæå¤±ã‚’åˆè¨ˆ
            pred = output.argmax(dim=1, keepdim=True)  # æœ€å¤§å¯¾æ•°ç¢ºç‡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            correct += pred.eq(target.view_as(pred)).sum()

    test_loss /= len(test_loader.dataset)

    accuracy = 100. * correct / len(test_loader.dataset)
    
    return test_loss, accuracy


def train_log(loss, example_ct, epoch):
    loss = float(loss)

    # ãƒã‚¸ãƒƒã‚¯ãŒèµ·ãã‚‹å ´æ‰€
    wandb.log({"epoch": epoch, "train/loss": loss}, step=example_ct)
    print(f"Loss after " + str(example_ct).zfill(5) + f" examples: {loss:.3f}")
    

def test_log(loss, accuracy, example_ct, epoch):
    loss = float(loss)
    accuracy = float(accuracy)

    # ãƒã‚¸ãƒƒã‚¯ãŒèµ·ãã‚‹å ´æ‰€
    wandb.log({"epoch": epoch, "validation/loss": loss, "validation/accuracy": accuracy}, step=example_ct)
    print(f"Loss/accuracy after " + str(example_ct).zfill(5) + f" examples: {loss:.3f}/{accuracy:.3f}")
```

ä»Šå›ã¯ã€Artifactã‚’ç”Ÿæˆã™ã‚‹2ã¤ã®åˆ¥ã€…ã®`Run`ã‚’è¡Œã„ã¾ã™ã€‚

ã¾ãšã€æœ€åˆã®runãŒãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã¨ã€
æ¬¡ã®runãŒ`trained-model`Artifactã‚’ä½¿ç”¨ã—ã¦ã€
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚

ã¾ãŸã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒæœ€ã‚‚ã†ã¾ãã„ã‹ãªã‹ã£ãŸ32ä¾‹â”€â”€`categorical_crossentropy`ãŒæœ€ã‚‚é«˜ã„ä¾‹â”€â”€ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

ã“ã‚Œã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŠã‚ˆã³ãƒ¢ãƒ‡ãƒ«ã®å•é¡Œã‚’è¨ºæ–­ã™ã‚‹ãŸã‚ã®è‰¯ã„æ–¹æ³•ã§ã™ï¼

```python
def evaluate(model, test_loader):
    """
    ## è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡
    """

    loss, accuracy = test(model, test_loader)
    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)

    return loss, accuracy, highest_losses, hardest_examples, true_labels, predictions

def get_hardest_k_examples(model, testing_set, k=32):
    model.eval()

    loader = DataLoader(testing_set, 1, shuffle=False)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®å„é …ç›®ã®æå¤±ã¨äºˆæ¸¬ã‚’å–å¾—
    losses = None
    predictions = None
    with torch.no_grad():
        for data, target in loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = F.cross_entropy(output, target)
            pred = output.argmax(dim=1, keepdim=True)
            
            if losses is None:
                losses = loss.view((1, 1))
                predictions = pred
            else:
                losses = torch.cat((losses, loss.view((1, 1))), 0)
                predictions = torch.cat((predictions, pred), 0)

    argsort_loss = torch.argsort(losses, dim=0)

    highest_k_losses = losses[argsort_loss[-k:]]
    hardest_k_examples = testing_set[argsort_loss[-k:]][0]
    true_labels = testing_set[argsort_loss[-k:]][1]
    predicted_labels = predictions[argsort_loss[-k:]]

    return highest_k_losses, hardest_k_examples, true_labels, predicted_labels
```

ã“ã‚Œã‚‰ã®ãƒ­ã‚°é–¢æ•°ã¯ã€æ–°ã—ã„`Artifact`æ©Ÿèƒ½ã‚’è¿½åŠ ã™ã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
å˜ã«ä½¿ç”¨ã—ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€Artifactã‚’ãƒ­ã‚°ã—ã¾ã™ã€‚

```python
from torch.utils.data import DataLoader

def train_and_log(config):

    with wandb.init(project="artifacts-example", job_type="train", config=config) as run:
        config = wandb.config

        data = run.use_artifact('mnist-preprocess:latest')
        data_dir = data.download()

        training_dataset =  read(data_dir, "training")
        validation_dataset = read(data_dir, "validation")

        train_loader = DataLoader(training_dataset, batch_size=config.batch_size)
        validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size)
        
        model_artifact = run.use_artifact("convnet:latest")
        model_dir = model_artifact.download()
        model_path = os.path.join(model_dir, "initialized_model.pth")
        model_config = model_artifact.metadata
        config.update(model_config)

        model = ConvNet(**model_config)
        model.load_state_dict(torch.load(model_path))
        model = model.to(device)
 
        train(model, train_loader, validation_loader, config)

        model_artifact = wandb.Artifact(
            "trained-model", type="model",
            description="ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿NNãƒ¢ãƒ‡ãƒ«",
            metadata=dict(model_config))

        torch.save(model.state_dict(), "trained_model.pth")
        model_artifact.add_file("trained_model.pth")
        wandb.save("trained_model.pth")

        run.log_artifact(model_artifact)

    return model

    
def evaluate_and_log(config=None):
    
    with wandb.init(project="artifacts-example", job_type="report", config=config) as run:
        data = run.use_artifact('mnist-preprocess:latest')
        data_dir = data.download()
        testing_set = read(data_dir, "test")

        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=128, shuffle=False)

        model_artifact = run.use_artifact("trained-model:latest")
        model_dir = model_artifact.download()
        model_path = os.path.join(model_dir, "trained_model.pth")
        model_config = model_artifact.metadata

        model = ConvNet(**model_config)
        model.load_state_dict(torch.load(model_path))
        model.to(device)

        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)

        run.summary.update({"loss": loss, "accuracy": accuracy})

        wandb.log({"high-loss-examples":
            [wandb.Image(hard_example, caption=str(int(pred)) + "," +  str(int(label)))
             for hard_example, pred, label in zip(hardest_examples, preds, true_labels)]})
```


```python
train_config = {"batch_size": 128,
                "epochs": 5,
                "batch_log_interval": 25,
                "optimizer": "Adam"}

model = train_and_log(train_config)
evaluate_and_log()
```

### ğŸ” ã‚°ãƒ©ãƒ•ãƒ“ãƒ¥ãƒ¼

`Artifact`ã®`type`ã‚’å¤‰æ›´ã—ã¾ã—ãŸï¼š
ã“ã‚Œã‚‰ã®`Run`sã¯`dataset`ã§ã¯ãªã`model`ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚
ã‚°ãƒ©ãƒ•ãƒ“ãƒ¥ãƒ¼ã®Artifactsãƒšãƒ¼ã‚¸ã§ã¯ã€
`model`ã‚’ç”Ÿæˆã™ã‚‹`Run`sã¯`dataset`ã‚’ç”Ÿæˆã™ã‚‹`Run`sã¨ã¯åŒºåˆ¥ã•ã‚Œã¾ã™ã€‚

ãœã²ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„ï¼ä»¥å‰ã¨åŒã˜ã‚ˆã†ã«ã€Runãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€
å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€ŒArtifactsã€ã‚¿ãƒ–ã‚’é¸ã³ã€
`Artifact`ã‚’é¸æŠã—ã¦ã€ŒGraph Viewã€ã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

### ğŸ’£ ã‚°ãƒ©ãƒ•ã®å±•é–‹

ã€ŒExplodeã€ã¨ãƒ©ãƒ™ãƒ«ä»˜ã‘ã•ã‚ŒãŸãƒœã‚¿ãƒ³ãŒã‚ã‚‹ã®ã«æ°—ã¥ã„ãŸã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ãã‚Œã‚’ã‚¯ãƒªãƒƒã‚¯ã—ãªã„ã§ãã ã•ã„ã€ãã‚Œã¯ç§ã®æœºã®ä¸‹ã«å°ã•ãªçˆ†å¼¾ã‚’ã‚»ãƒƒãƒˆã—ã¾ã™â€¦ã¨ã„ã†ã®ã¯å†—è«‡ã§ã™ã€‚ãã®ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€ã‚°ãƒ©ãƒ•ãŒã‚ˆã‚Šå„ªã—ãå±•é–‹ã•ã‚Œã¾ã™ï¼š
`Artifact`ã‚„`Run`sã¯ã€å€‹ã€…ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ãƒ¬ãƒ™ãƒ«ã§åˆ†é›¢ã•ã‚Œã¾ã™ã€ã‚¿ã‚¤ãƒ—ã§ã¯ãªãã€ä¾‹ãˆã°ã€
ãƒãƒ¼ãƒ‰ã¯`dataset`ã‚„`load-data`ã§ã¯ãªãã€`dataset:mnist-raw:v1`ã‚„`load-data:sunny-smoke-1`ãªã©ã§ã™ã€‚

ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ­ã‚°ã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã©ã™ã¹ã¦ãŒæ‰‹ã®å±Šãç¯„å›²ã«ã‚ã‚Šã€
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å…¨è²Œã‚’å®Œå…¨ã«æŠŠæ¡ã§ãã¾ã™â”€â”€ã‚ãªãŸãŒç§ãŸã¡ã«ãƒ­ã‚°ã‚’é¸ã‚“ã ã‚‚ã®ã ã‘ãŒé™ç•Œã§ã™ã€‚

# æ¬¡ã¯ä½•ã‚’ã™ã‚‹ï¼Ÿ

