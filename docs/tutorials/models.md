
# Register models

[**Try in a Colab Notebook here â†’**](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-model-registry/Model_Registry_E2E.ipynb)

ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¯ã€çµ„ç¹”å…¨ä½“ã§ç¾åœ¨ä½œæ¥­ä¸­ã®ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¹ã‚¯ã¨é–¢é€£ã™ã‚‹Artifactsã‚’é›†ã‚ã¦æ•´ç†ã™ã‚‹ãŸã‚ã®ä¸­å¤®ã®å ´æ‰€ã§ã™ã€‚
- ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†
- ãƒªãƒƒãƒãªãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’æ–‡æ›¸åŒ–
- ä½¿ç”¨ä¸­/ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆä¸­ã®ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®å±¥æ­´ã‚’ç¶­æŒ
- ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ãªå¼•ç¶™ãã¨ã‚¹ãƒ†ãƒ¼ã‚¸ç®¡ç†ã®ä¿ƒé€²
- å„ç¨®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¹ã‚¯ã®ã‚¿ã‚°ä»˜ã‘ã¨æ•´ç†
- ãƒ¢ãƒ‡ãƒ«ã®é€²æ—æ™‚ã«è‡ªå‹•é€šçŸ¥ã‚’è¨­å®š

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€ç°¡å˜ãªç”»åƒåˆ†é¡ã‚¿ã‚¹ã‚¯ã®ãƒ¢ãƒ‡ãƒ«é–‹ç™ºãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚’è¿½è·¡ã™ã‚‹æ–¹æ³•ã‚’ã”ç´¹ä»‹ã—ã¾ã™ã€‚

### ğŸ› ï¸ `wandb` ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
!pip install -q wandb onnx pytorch-lightning
```

## W&B ã«ãƒ­ã‚°ã‚¤ãƒ³
- `wandb login` ã¾ãŸã¯ `wandb.login()` ã‚’ä½¿ç”¨ã—ã¦æ˜ç¤ºçš„ã«ãƒ­ã‚°ã‚¤ãƒ³ã§ãã¾ã™ï¼ˆä»¥ä¸‹å‚ç…§ï¼‰
- ã‚ã‚‹ã„ã¯ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚W&Bãƒ­ã‚®ãƒ³ã‚°ã®æŒ¯ã‚‹èˆã„ã‚’å¤‰æ›´ã™ã‚‹ãŸã‚ã«è¨­å®šã§ãã‚‹ç’°å¢ƒå¤‰æ•°ã¯å¤šå²ã«ã‚ãŸã‚Šã€æœ€ã‚‚é‡è¦ãªã‚‚ã®ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:
    - `WANDB_API_KEY` - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ã€ŒSettingsã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§è¦‹ã¤ã‹ã‚Šã¾ã™
    - `WANDB_BASE_URL` - W&Bã‚µãƒ¼ãƒãƒ¼ã®URLã§ã™
- APIãƒˆãƒ¼ã‚¯ãƒ³ã¯W&Bã‚¢ãƒ—ãƒªã®ã€ŒProfileã€ -> ã€ŒSettingsã€ã§è¦‹ã¤ã‹ã‚Šã¾ã™

![api_token](https://drive.google.com/uc?export=view&id=1Xn7hnn0rfPu_EW0A_-32oCXqDmpA0-kx)

```notebook
!wandb login
```

:::note
[W&B Server](..//guides/hosting/intro.md)ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆï¼ˆ**Dedicated Cloud**ã¾ãŸã¯**Self-managed**ã®ã©ã¡ã‚‰ã‹ï¼‰ã«æ¥ç¶šã™ã‚‹éš›ã«ã¯ã€--reloginãŠã‚ˆã³--hostã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä¾‹ãˆã°:

```notebook
!wandb login --relogin --host=http://your-shared-local-host.com
```

å¿…è¦ã«å¿œã˜ã¦ã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆç®¡ç†è€…ã«ãƒ›ã‚¹ãƒˆåã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
:::

## Log Data and Model Checkpoints as Artifacts
W&B Artifactsã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€è©•ä¾¡çµæœãªã©ã®ä»»æ„ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã‚’è¿½è·¡ã—ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ãŒå¯èƒ½ã§ã™ã€‚ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’ä½œæˆã™ã‚‹éš›ã«ã¯åå‰ã¨ã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®šã—ã¾ã™ã€‚ã“ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¯å®Ÿé¨“çš„ãªSoRï¼ˆsystem of recordï¼‰ã¨æ°¸ä¹…ã«ãƒªãƒ³ã‚¯ã•ã‚Œã¾ã™ã€‚åŸºç¤ãƒ‡ãƒ¼ã‚¿ãŒå¤‰æ›´ã•ã‚Œã€ãã®ãƒ‡ãƒ¼ã‚¿è³‡ç”£ã‚’å†åº¦ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹ã¨ã€W&BãŒè‡ªå‹•çš„ã«å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã—ã€æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½œæˆã—ã¾ã™ã€‚W&B Artifactsã¯ã€å…±æœ‰ã®éæ§‹é€ åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®ä¸Šã®è»½é‡ãªæŠ½è±¡ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨ã—ã¦è€ƒãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

### ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®æ§‹é€ 

`Artifact`ã‚¯ãƒ©ã‚¹ã¯ã€W&Bã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªå†…ã®ã‚¨ãƒ³ãƒˆãƒªã«å¯¾å¿œã—ã¾ã™ã€‚ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã¾ã™:
* åå‰
* ã‚¿ã‚¤ãƒ—
* ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
* èª¬æ˜
* ãƒ•ã‚¡ã‚¤ãƒ«ã€ãŠã‚ˆã³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€ã¾ãŸã¯å‚ç…§

ä½¿ç”¨ä¾‹:
```python
run = wandb.init(project="my-project")
artifact = wandb.Artifact(name="my_artifact", type="data")
artifact.add_file("/path/to/my/file.txt")
run.log_artifact(artifact)
run.finish()
```

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€ã¾ãšãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãã‚Œã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã§ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦ãƒ­ã‚°ã«è¨˜éŒ²ã—ã¾ã™ã€‚

```python
# @title Enter your W&B project and entity

# FORM VARIABLES
PROJECT_NAME = "model-registry-tutorial"  # @param {type:"string"}
ENTITY = None  # @param {type:"string"}

# SIZEã‚’"TINY"ã€"SMALL"ã€"MEDIUM"ã€ã¾ãŸã¯"LARGE"ã«è¨­å®šã—ã¦ã€ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã„ãšã‚Œã‹ã‚’é¸æŠ
# TINYãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: 100æšã®ç”»åƒã€30MB
# SMALLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: 1000æšã®ç”»åƒã€312MB
# MEDIUMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: 5000æšã®ç”»åƒã€1.5GB
# LARGEãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: 12,000æšã®ç”»åƒã€3.6GB

SIZE = "TINY"

if SIZE == "TINY":
    src_url = "https://storage.googleapis.com/wandb_datasets/nature_100.zip"
    src_zip = "nature_100.zip"
    DATA_SRC = "nature_100"
    IMAGES_PER_LABEL = 10
    BALANCED_SPLITS = {"train": 8, "val": 1, "test": 1}
elif SIZE == "SMALL":
    src_url = "https://storage.googleapis.com/wandb_datasets/nature_1K.zip"
    src_zip = "nature_1K.zip"
    DATA_SRC = "nature_1K"
    IMAGES_PER_LABEL = 100
    BALANCED_SPLITS = {"train": 80, "val": 10, "test": 10}
elif SIZE == "MEDIUM":
    src_url = "https://storage.googleapis.com/wandb_datasets/nature_12K.zip"
    src_zip = "nature_12K.zip"
    DATA_SRC = "inaturalist_12K/train"  #ï¼ˆæŠ€è¡“çš„ã«ã¯10,000æšã®ç”»åƒã®ã¿ã®ã‚µãƒ–ã‚»ãƒƒãƒˆï¼‰
    IMAGES_PER_LABEL = 500
    BALANCED_SPLITS = {"train": 400, "val": 50, "test": 50}
elif SIZE == "LARGE":
    src_url = "https://storage.googleapis.com/wandb_datasets/nature_12K.zip"
    src_zip = "nature_12K.zip"
    DATA_SRC = "inaturalist_12K/train"  #ï¼ˆæŠ€è¡“çš„ã«ã¯10,000æšã®ç”»åƒã®ã¿ã®ã‚µãƒ–ã‚»ãƒƒãƒˆï¼‰
    IMAGES_PER_LABEL = 1000
    BALANCED_SPLITS = {"train": 800, "val": 100, "test": 100}
```

```notebook
%%capture
!curl -SL $src_url > $src_zip
!unzip $src_zip
```

```python
import wandb
import pandas as pd
import os

with wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type="log_datasets") as run:
    img_paths = []
    for root, dirs, files in os.walk("nature_100", topdown=False):
        for name in files:
            img_path = os.path.join(root, name)
            label = img_path.split("/")[1]
            img_paths.append([img_path, label])

    index_df = pd.DataFrame(columns=["image_path", "label"], data=img_paths)
    index_df.to_csv("index.csv", index=False)

    train_art = wandb.Artifact(
        name="Nature_100",
        type="raw_images",
        description="nature image dataset with 10 classes, 10 images per class",
    )
    train_art.add_dir("nature_100")

    # å„ç”»åƒã®ãƒ©ãƒ™ãƒ«ã‚’ç¤ºã™csvã‚‚è¿½åŠ 
    train_art.add_file("index.csv")
    wandb.log_artifact(train_art)
```

### ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆåã¨ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿è³‡ç”£ã‚’ç°¡å˜ã«å¼•ãç¶™ããŠã‚ˆã³æŠ½è±¡åŒ–
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚„ãƒ¢ãƒ‡ãƒ«ã®`name:alias`çµ„ã¿åˆã‚ã›ã‚’å‚ç…§ã™ã‚‹ã ã‘ã§ã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚ˆã‚Šè‰¯ãæ¨™æº–åŒ–ã§ãã¾ã™ 
- ä¾‹ã¨ã—ã¦ã€PyTorch `Dataset`ã‚„`DataModule`ã‚’æ§‹ç¯‰ã—ã€å¼•æ•°ã¨ã—ã¦W&B Artifactåã¨ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’ä½¿ç”¨ã—ã¦é©åˆ‡ã«ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€ã“ã‚Œã‚’ä½¿ç”¨ã™ã‚‹W&B runsã€ãŠã‚ˆã³ä¸Šä¸‹æµã®å…¨ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®ãƒªãƒãƒ¼ã‚¸ã‚’ä»Šã™ãç¢ºèªã§ãã¾ã™ï¼

![api_token](https://drive.google.com/uc?export=view&id=1fEEddXMkabgcgusja0g8zMz8whlP2Y5P)

```python
from torchvision import transforms
import pytorch_lightning as pl
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from skimage import io, transform
from torchvision import transforms, utils, models
import math

class NatureDataset(Dataset):
    def __init__(
        self,
        wandb_run,
        artifact_name_alias="Nature_100:latest",
        local_target_dir="Nature_100:latest",
        transform=None,
    ):
        self.local_target_dir = local_target_dir
        self.transform = transform

        # ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«å–å¾—ã—ã¦ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰
        art = wandb_run.use_artifact(artifact_name_alias)
        path_at = art.download(root=self.local_target_dir)

        self.ref_df = pd.read_csv(os.path.join(self.local_target_dir, "index.csv"))
        self.class_names = self.ref_df.iloc[:, 1].unique().tolist()
        self.idx_to_class = {k: v for k, v in enumerate(self.class_names)}
        self.class_to_idx = {v: k for k, v in enumerate(self.class_names)}

    def __len__(self):
        return len(self.ref_df)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        img_path = self.ref_df.iloc[idx, 0]

        image = io.imread(img_path)
        label = self.ref_df.iloc[idx, 1]
        label = torch.tensor(self.class_to_idx[label], dtype=torch.long)

        if self.transform:
            image = self.transform(image)

        return image, label

class NatureDatasetModule(pl.LightningDataModule):
    def __init__(
        self,
        wandb_run,
        artifact_name_alias: str = "Nature_100:latest",
        local_target_dir: str = "Nature_100:latest",
        batch_size: int = 16,
        input_size: int = 224,
        seed: int = 42,
    ):
        super().__init__()
        self.wandb_run = wandb_run
        self.artifact_name_alias = artifact_name_alias
        self.local_target_dir = local_target_dir
        self.batch_size = batch_size
        self.input_size = input_size
        self.seed = seed

    def setup(self, stage=None):
        self.nature_dataset = NatureDataset(
            wandb_run=self.wandb_run,
            artifact_name_alias=self.artifact_name_alias,
            local_target_dir=self.local_target_dir,
            transform=transforms.Compose(
                [
                    transforms.ToTensor(),
                    transforms.CenterCrop(self.input_size),
                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
                ]
            ),
        )

        nature_length = len(self.nature_dataset)
        train_size = math.floor(0.8 * nature_length)
        val_size = math.floor(0.2 * nature_length)
        self.nature_train, self.nature_val = random_split(
            self.nature_dataset,
            [train_size, val_size],
            generator=torch.Generator().manual_seed(self.seed),
        )
        return self

    def train_dataloader(self):
        return DataLoader(self.nature_train, batch_size=self.batch_size)

    def val_dataloader(self):
        return DataLoader(self.nature_val, batch_size=self.batch_size)

    def predict_dataloader(self):
        pass

    def teardown(self, stage: str):
        pass
```

## Model Training

### ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã¨æ¤œè¨¼é–¢æ•°ã®ä½œæˆ

```python
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
import onnx

def set_parameter_requires_grad(model, feature_extracting):
    if feature_extracting:
        for param in model.parameters():
            param.requires_grad = False

def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):
    # ã“ã®ifã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆã§è¨­å®šã•ã‚Œã‚‹å¤‰æ•°ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®å¤‰æ•°ã¯ãã‚Œãã‚Œã®ãƒ¢ãƒ‡ãƒ«ã«ç‰¹æœ‰ã§ã™ã€‚
    model_ft = None
    input_size = 0

    if model_name == "resnet":
        """Resnet18"""
        model_ft = models.resnet18(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.fc.in_features
        model_ft.fc = torch.nn.Linear(num_ftrs, num_classes)
        input_size = 224

    elif model_name == "alexnet":
        """Alexnet"""
        model_ft = models.alexnet(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.classifier[6].in_features
        model_ft.classifier[6] = torch.nn.Linear(num_ftrs, num_classes)
        input_size = 224

    elif model_name == "vgg":
        """VGG11_bn"""
        model_ft = models.vgg11_bn(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.classifier[6].in_features
        model_ft.classifier[6] = torch.nn.Linear(num_ftrs, num_classes)
        input_size = 224

    elif model_name == "squeezenet":
        """Squeezenet"""
        model_ft = models.squeezenet1_0(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        model_ft.classifier[1] = torch.nn.Conv2d(
            512, num_classes, kernel_size=(1, 1), stride=(1, 1)
        )
        model_ft.num_classes = num_classes
        input_size = 224

    elif model_name == "densenet":
        """Densenet"""
        model_ft = models.densenet121(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.classifier.in_features
        model_ft.classifier = torch.nn.Linear(num_ftrs, num_classes)
        input_size = 224

    else:
        print("Invalid model name, exiting...")
        exit()

    return model_ft, input_size

class NaturePyTorchModule(torch.nn.Module):
    def __init__(self, model_name, num_classes=10, feature_extract=True, lr=0.01):
        """ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã™ã‚‹ãŸã‚ã®ãƒ¡ã‚½ãƒƒãƒ‰"""
        super().__init__()

        self.model_name = model_name
        self.num_classes = num_classes
        self.feature_extract = feature_extract
        self.lr = lr
        self.model, self.input_size = initialize_model(
            model_name=self.model_name,
            num_classes=self.num_classes,
            feature_extract=True,
        )

    def forward(self, x):
        """æ¨è«–ç”¨ã®å…¥åŠ› -> å‡ºåŠ›ãƒ¡ã‚½ãƒƒãƒ‰"""
        x = self.model(x)

        return x

def evaluate_model(model, eval_data, idx_to_class, class_names, epoch_ndx):
    device = torch.device("cpu")
    model.eval()
    test_loss = 0
    correct = 0
    preds = []
    actual = []

    val_table = wandb.Table(columns=["pred", "actual", "image"])

    with torch.no_grad():
        for data, target in eval_data:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(
                output, target, reduction="sum"
            ).item()  # ãƒãƒƒãƒæå¤±ã®åˆè¨ˆ
            pred = output.argmax(
                dim=1, keepdim=True
            )  # æœ€å¤§å¯¾æ•°ç¢ºç‡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            preds += list(pred.flatten().tolist())
            actual += target.numpy().tolist()
            correct += pred.eq(target.view_as(pred)).sum().item()

            for idx, img in enumerate(data):
                img = img.numpy().transpose(1, 2, 0)
                pred_class = idx_to_class[pred.numpy()[idx][0]]
                target_class = idx_to_class[target.numpy()[idx]]
                val_table.add_data(pred_class, target_class, wandb.Image(img))

    test_loss /= len(eval_data.dataset)
    accuracy = 100.0 * correct / len(eval_data.dataset)
    conf_mat = wandb.plot.confusion_matrix(
        y_true=actual, preds=preds, class_names=class_names
    )
    return test_loss, accuracy, preds, val_table, conf_mat
```

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ«ãƒ¼ãƒ—ã®è¿½è·¡
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã¯ã€å®šæœŸçš„ã«ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹ã®ãŒãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã§ã™ã€‚ã“ã†ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒä¸­æ–­ã•ã‚ŒãŸã‚Šã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãŸå ´åˆã§ã‚‚ã€é€”ä¸­ã‹ã‚‰å†é–‹ã§ãã¾ã™ã€‚Artifactã‚’ãƒ­ã‚°ã™ã‚‹ã“ã¨ã§ã€ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’W&Bã§è¿½è·¡ã—ã€ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®å½¢å¼ã‚„ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ãªã©ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»˜åŠ ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€èª°ã‹ãŒãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’åˆ©ç”¨ã™ã‚‹éš›ã«ã€ã©ã®ã‚ˆã†ã«ä½¿ç”¨ã™ã‚Œã°ã‚ˆã„ã‹ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚Artifactã¨ã—ã¦ä»»æ„ã®å½¢å¼ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ã‚°ã™ã‚‹éš›ã«ã¯ã€Artifactã®`type`ã‚’`model`ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚

```python
run = wandb.init(
    project=PROJECT_NAME,
    entity=ENTITY,
    job_type="training",
    config={
        "model_type": "squeezenet",
        "lr": 1.0,
        "gamma": 0.75,
        "batch_size": 16,
        "epochs": 5,
    },
)

model = NaturePyTorchModule(wandb.config["model_type"])
wandb.watch(model)

wandb.config["input_size"] = 224

nature_module = NatureDatasetModule(
    wandb_run=run,
    artifact_name_alias="Nature_100:latest",
    local_target_dir="Nature_100:latest",
    batch_size=wandb.config["batch_size"],
    input_size=wandb.config["input_size"],
)
nature_module.setup()

# ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
learning_rate = wandb.config["lr"]
gamma = wandb.config["gamma"]
epochs = wandb.config["epochs"]

device = torch.device("cpu")
optimizer = optim.Adadelta(model.parameters(), lr=wandb.config["lr"])
scheduler = StepLR(optimizer, step_size=1, gamma=wandb.config["gamma"])

best_loss = float("inf")
best_model = None

for epoch_ndx in range(epochs):
    model.train()
    for batch_ndx, batch in enumerate(nature_module.train_dataloader()):
        data, target = batch[0].to("cpu"), batch[1].to("cpu")
        optimizer.zero_grad()
        preds = model(data)
        loss = F.nll_loss(preds, target)
        loss.backward()
        optimizer.step()
        scheduler.step()

        ### ã‚ãªãŸã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã—ã¾ã—ã‚‡ã† ###
        wandb.log(
            {
                "train/epoch_ndx": epoch_ndx,
                "train/batch_ndx": batch_ndx,
                "train/train_loss": loss,
                "train/learning_rate": optimizer.param_groups[0]["lr"],
            }
        )

    ### å„ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®è©•ä¾¡ ###
    model.eval()
    test_loss, accuracy, preds, val_table, conf_mat = evaluate_model(
        model,
        nature_module.val_dataloader(),
        nature_module.nature_dataset.idx_to_class,
        nature_module.nature_dataset.class_names,
        epoch_ndx,
    )

    is_best = test_loss < best_loss

    wandb.log(
        {
            "eval/test_loss": test_loss,
            "eval/accuracy": accuracy,
            "eval/conf_mat": conf_mat,
            "eval/val_table": val_table,
        }
    )

    ### ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦ä¿å­˜ ###
    x = torch.randn(1, 3, 224, 224, requires_grad=True)
    torch.onnx.export(
        model,  # å®Ÿè¡Œã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«
        x,  # ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ï¼ˆã¾ãŸã¯è¤‡æ•°å…¥åŠ›ã®å ´åˆã¯ã‚¿ãƒ—ãƒ«ï¼‰
        "model.onnx",  # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹å ´æ‰€ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ©ã‚¤ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰
        export_params=True,  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡é‡ã‚’ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        opset_version=10,  # ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ONNXãƒãƒ¼ã‚¸ãƒ§ãƒ³
        do_constant_folding=True,  # æœ€é©åŒ–ã®ãŸã‚ã®å®šæ•°æŠ˜ã‚ŠãŸãŸã¿ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹
        input_names=["input"],  # ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›å
        output_names=["output"],  # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å
        dynamic_axes={
            "input": {0: "batch_size"},  # å¯å¤‰é•·ã®è»¸
            "output": {0: "batch_size"},
        },
    )

    art = wandb.Artifact(
        f"nature-{wandb.run.id}",
        type="model",
        metadata={
            "format": "onnx",
            "num_classes": len(nature_module.nature_dataset.class_names),
            "model_type": wandb.config["model_type"],
            "model_input_size": wandb.config["input_size"],
            "index_to_class": nature_module.nature_dataset.idx_to_class,
        },
    )

    art.add_file("model.onnx")

    ### é•·æœŸã«ã‚ãŸã£ã¦æœ€è‰¯ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’è¿½è·¡ã™ã‚‹ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’è¿½åŠ  ###
    wandb.log_artifact(art, aliases=["best", "latest"] if is_best else None)
    if is_best:
        best_model = art
```

### 1ã¤ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸‹ã§ã‚ãªãŸã®ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç®¡ç†ã—ã¾ã™ã€‚

![api_token](https://drive.google.com/uc?export=view&id=1z7nXRgqHTPYjfR1SoP-CkezyxklbAZlM)

### æ³¨è¨˜: W&Bã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã¨ã®åŒæœŸ
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ä½•ã‚‰ã‹ã®ç†ç”±ã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é€šä¿¡ãŒåˆ‡æ–­ã•ã‚ŒãŸå ´åˆã§ã‚‚ã€`wandb sync`ã‚’ä½¿ç”¨ã—ã¦é€²æ—ã‚’å¸¸ã«åŒæœŸã§ãã¾ã™ã€‚

W&B SDKã¯ã™ã¹ã¦ã®ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª`wandb`ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€`wandb sync`ã‚’å‘¼ã³å‡ºã™ã¨ã€ãƒ­ãƒ¼ã‚«ãƒ«ã®çŠ¶æ…‹ãŒã‚¦ã‚§ãƒ–ã‚¢ãƒ—ãƒªã¨åŒæœŸã•ã‚Œã¾ã™ã€‚

## ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
å®Ÿé¨“ä¸­ã«è¤‡æ•°ã®runã§å¤šæ•°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ã‚°ã—ãŸå¾Œã¯ã€æ¬¡ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¸ï¼ˆä¾‹ï¼šãƒ†ã‚¹ãƒˆã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆï¼‰ã«æœ€è‰¯ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å¼•ãç¶™ãæ™‚ã§ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¯ã€å€‹ã€…ã®W&Bãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸Šä½ã«ä½ç½®ã™ã‚‹ä¸­å¤®ãƒšãƒ¼ã‚¸ã§ã™ã€‚ã“ã“ã«ã¯**Registered Models**ãŒä¿å­˜ã•ã‚Œã€å€‹ã€…ã®W&Bãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å­˜åœ¨ã™ã‚‹ä¾¡å€¤ã®ã‚ã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¸ã®ã€Œãƒªãƒ³ã‚¯ã€ã‚’æ ¼ç´ã—ã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¯ã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¹ã‚¯ã®æœ€è‰¯ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®é›†ä¸­ç®¡ç†ã®å ´ã‚’æä¾›ã—ã¾ã™ã€‚ã‚ãªãŸãŒãƒ­ã‚°ã™ã‚‹ä»»æ„ã®`model`ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¯ã€Registered Modelã«ã€Œãƒªãƒ³ã‚¯ã€ã§ãã¾ã™ã€‚

### **Registered Models**ã‚’ä½œæˆã—ã€UIã‹ã‚‰ãƒªãƒ³ã‚¯ã™ã‚‹

#### 1. ãƒãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€`Model Registry`ã‚’é¸æŠã—ã¦ã€ãƒãƒ¼ãƒ ã®ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™ã€‚

![model registry](https://drive.google.com/uc?export=view&id=1ZtJwBsFWPTm4Sg5w8vHhRpvDSeQPwsKw)

#### 2. æ–°ã—ã„Registered Modelã‚’ä½œæˆã—ã¾ã™ã€‚

![model registry](https://drive.google.com/uc?export=view&id=1RuayTZHNE0LJCxt1t0l6-2zjwiV4aDXe)

#### 3. ãƒ¢ãƒ‡ãƒ«ã®ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚¿ãƒ–ã«ç§»å‹•ã—ã¾ã™ã€‚

![model registry](https://drive.google.com/uc?export=view&id=1LfTLrRNpBBPaUb_RmBIE7fWFMG0h3e0E)

#### 4. ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ã€ŒLink to Registryã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚

### **API**ã‚’é€šã˜ã¦Registered Modelsã‚’ä½œæˆã—ãƒªãƒ³ã‚¯ã™ã‚‹
`wandb.run.link_artifact`ã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŠã‚ˆã³**Registered Model**ã®åå‰ã€ã•ã‚‰ã«ä»˜åŠ ã—ãŸã„ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’æ¸¡ã—ã¦ã€[APIçµŒç”±ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªãƒ³ã‚¯](https://docs.wandb.ai/guides/models)ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚W&Bã§ã¯**Registered Models**ã¯ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆãƒãƒ¼ãƒ ï¼‰ã«ã‚¹ã‚³ãƒ¼ãƒ—ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãƒãƒ¼ãƒ ã®ãƒ¡ãƒ³ãƒãƒ¼ã®ã¿ãŒãã“ã«ã‚ã‚‹**Registered Models**ã‚’è¡¨ç¤ºãŠã‚ˆã³ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚APIã§Registered Modelã®åå‰ã‚’æŒ‡å®šã™ã‚‹å ´åˆã¯ã€`<entity>/model-registry/<registered-model-name>`ã¨ã—ã¾ã™ã€‚Registered ModelãŒå­˜åœ¨ã—ãªã„å ´åˆã€è‡ªå‹•çš„ã«ä½œæˆã•ã‚Œã¾ã™ã€‚

```python
if ENTITY:
    wandb.run.link_artifact(
        best_model,
        f"{ENTITY}/model-registry/Model Registry Tutorial",
        aliases=["staging"],
    )
else:
    print("Must indicate entity where Registered Model will exist")
wandb.finish()
```

### ã€Œãƒªãƒ³ã‚¯ã€ã¨ã¯ï¼Ÿ
ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ãƒªãƒ³ã‚¯ã™ã‚‹ã¨ã€ãã®Registered Modelã®æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒä½œæˆã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ãã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã«å­˜åœ¨ã™ã‚‹ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸ã®ãƒã‚¤ãƒ³ã‚¿ã«ã™ãã¾ã›ã‚“ã€‚W&BãŒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨Registered Modelã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚’åˆ†ã‘ã‚‹ç†ç”±ã¯ã“ã“ã«ã‚ã‚Šã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ãƒªãƒ³ã‚¯ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€ãã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’Registered Modelã‚¿ã‚¹ã‚¯ã®ä¸‹ã§ã€Œãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã€ã™ã‚‹ã“ã¨ã¨åŒç­‰ã§ã™ã€‚

é€šå¸¸ã€ç ”ç©¶é–‹ç™ºã‚„å®Ÿé¨“ä¸­ã«ç ”ç©¶è€…ã¯100ä»¥ä¸Šã€ã•ã‚‰ã«ã¯1000ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ãŒã€ãã®ã†ã¡å®Ÿéš›ã«ã€Œæ—¥ã®ç›®ã‚’è¦‹ã‚‹ã€ã‚‚ã®ã¯ä¸€ã¤ã‹äºŒã¤ã ã‘ã§ã™ã€‚ã“ã‚Œã‚‰ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’åˆ¥ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã•ã‚ŒãŸãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ãƒªãƒ³ã‚¯ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€ãƒ¢ãƒ‡ãƒ«é–‹ç™ºå´ã¨ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ/æ¶ˆè²»å´ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ˜ç¢ºã«åŒºåˆ¥ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®ä¸–ç•Œå…±é€šã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³/ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¯ã€ç ”ç©¶é–‹ç™ºä¸­ã«ç”Ÿæˆã•ã‚Œã‚‹ã™ã¹ã¦ã®å®Ÿé¨“ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‹ã‚‰æ±šæŸ“ã•ã‚Œãªã„ã‚ˆã†ã«ã™ã¹ãã§ã‚ã‚Šã€ã—ãŸãŒã£ã¦ã€Registered Modelã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¯æ–°ã—ã„ã€Œãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã•ã‚ŒãŸã€ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦å¢—åŠ ã—ã¾ã™ã€‚

## ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã®é›†ä¸­ãƒãƒ–ã‚’ä½œæˆ
- Registered Modelã«ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã€ã‚¿ã‚°ã€Slacké€šçŸ¥ã‚’è¿½åŠ 
- ãƒ¢ãƒ‡ãƒ«ãŒç•°ãªã‚‹ãƒ•ã‚§ãƒ¼ã‚ºã‚’ç§»è¡Œã™ã‚‹éš›ã«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’å¤‰æ›´
- ãƒ¢ãƒ‡ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨å›å¸°ãƒ¬ãƒãƒ¼ãƒˆã®ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚’ãƒ¬ãƒãƒ¼ãƒˆã«åŸ‹ã‚è¾¼ã¿ã¾ã™ã€‚ä¾‹ãˆã°ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼š[example](https://api.wandb.ai/links/wandb-smle/r82bj9at)
![model registry](https://drive.google.com/uc?export=view&id=1lKPgaw-Ak4WK_91aBMcLvUMJL6pDQpgO)

### ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ãŒãƒªãƒ³ã‚¯ã•ã‚Œã‚‹ã¨ãã«Slacké€šçŸ¥ã‚’è¨­å®š

![model registry](https://drive.google.com/uc?export=view&id=1RsWCa6maJYD5y34gQ0nwWiKSWUCqcjT9)

## Registered Modelã®åˆ©ç”¨
APIã‚’ä»‹ã—ã¦å¯¾å¿œã™ã‚‹`name:alias`ã‚’å‚ç…§ã™ã‚‹ã“ã¨ã§ã€ä»»æ„ã®Registered Modelã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ç ”ç©¶è€…ã€CI/CDãƒ—ãƒ­ã‚»ã‚¹ã®ã„ãšã‚Œã§ã‚ã£ã¦ã‚‚ã€ãƒ†ã‚¹ãƒˆã‚’é€šéã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã«ç§»è¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®é›†ä¸­ãƒãƒ–ã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚

```notebook
%%wandb -h 600

run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type='inference')
artifact = run.use_artifact(f'{ENTITY}/model-registry/Model Registry Tutorial:staging', type='model')
artifact_dir = artifact.download()
wandb.finish()
```

# æ¬¡ã¯ä½•ï¼Ÿ
æ¬¡ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’åå¾©å‡¦ç†ã—ã€W&B Promptsã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒãƒƒã‚°ã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ï¼š

## ğŸ‘‰ [Iterate on LLMs](prompts)